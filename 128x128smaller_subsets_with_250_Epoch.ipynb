{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Bte-J-FN6Gfj-P4RoJMsy9yvt6PJX7cX","timestamp":1701178673402}],"gpuType":"T4","authorship_tag":"ABX9TyMl/U4525TzSYsvW4/9PChk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlZIzh7_h_bN","executionInfo":{"status":"ok","timestamp":1701180547385,"user_tz":-60,"elapsed":22249,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}},"outputId":"a2b4d6bd-21a3-46d6-fcc7-57190fd6a013"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","from torch.nn.functional import relu\n","import torch.nn.functional as F\n","\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import numpy as np\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, image_paths, label_paths):\n","        self.image_paths = image_paths\n","        self.label_paths = label_paths\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        # Load the image\n","        image = Image.open(self.image_paths[idx])\n","        np_image = np.array(image, dtype=np.float32)\n","\n","        # Normalize the image\n","        normalized_image = np_image / 65535.0  # For 16-bit images\n","\n","        # Load and process the label data\n","        label_image = Image.open(self.label_paths[idx])\n","        label_array = np.array(label_image, dtype=np.float32)\n","\n","        grayscale_to_class_mapping = {0: 0, 128: 1, 255: 2} # a set that maps gray-levels to a class\n","\n","        # Map grayscale values to class labels\n","        mapped_labels = np.copy(label_array)\n","        for grayscale_value, class_id in grayscale_to_class_mapping.items():\n","            mapped_labels[label_array == grayscale_value] = class_id\n","\n","        # Convert to PyTorch tensors\n","        image_tensor = torch.from_numpy(normalized_image).unsqueeze(0) # unsqueeze to enable channel dimension, was gone due to being a grayscale image\n","        label_tensor = torch.from_numpy(mapped_labels)\n","\n","        return image_tensor, label_tensor\n"],"metadata":{"id":"m95UJeDziH6e","executionInfo":{"status":"ok","timestamp":1701180553058,"user_tz":-60,"elapsed":5682,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["### Label images ###\n","# white class - 255 nickel\n","# gray class - 128 ysz\n","# black class - 0 pores\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_class):\n","        super().__init__()\n","\n","        # Define a helper function for creating a block\n","        def conv_block(in_channels, out_channels):\n","            return nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels),\n","                nn.ReLU(),\n","                nn.Dropout(p=0.1)\n","            )\n","\n","        # Encoder\n","        self.e11 = conv_block(1, 64)\n","        self.e12 = conv_block(64, 64)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.e21 = conv_block(64, 128)\n","        self.e22 = conv_block(128, 128)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.e31 = conv_block(128, 256)\n","        self.e32 = conv_block(256, 256)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.e41 = conv_block(256, 512)\n","        self.e42 = conv_block(512, 512)\n","        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.e51 = conv_block(512, 1024)\n","        self.e52 = conv_block(1024, 1024)\n","\n","        # Decoder\n","        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n","        self.d11 = conv_block(1024, 512)\n","        self.d12 = conv_block(512, 512)\n","\n","        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","        self.d21 = conv_block(512, 256)\n","        self.d22 = conv_block(256, 256)\n","\n","        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.d31 = conv_block(256, 128)\n","        self.d32 = conv_block(128, 128)\n","\n","        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.d41 = conv_block(128, 64)\n","        self.d42 = conv_block(64, 64)\n","\n","        # Output layer\n","        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n","\n","    def forward(self, x):\n","        # Encoder\n","        xe11 = self.e11(x)\n","        xe12 = self.e12(xe11)\n","        xp1 = self.pool1(xe12)\n","\n","        xe21 = self.e21(xp1)\n","        xe22 = self.e22(xe21)\n","        xp2 = self.pool2(xe22)\n","\n","        xe31 = self.e31(xp2)\n","        xe32 = self.e32(xe31)\n","        xp3 = self.pool3(xe32)\n","\n","        xe41 = self.e41(xp3)\n","        xe42 = self.e42(xe41)\n","        xp4 = self.pool4(xe42)\n","\n","        xe51 = self.e51(xp4)\n","        xe52 = self.e52(xe51)\n","\n","        # Decoder\n","        xu1 = self.upconv1(xe52)\n","        xu11 = torch.cat([xu1, xe42], dim=1)\n","        xd11 = self.d11(xu11)\n","        xd12 = self.d12(xd11)\n","\n","        xu2 = self.upconv2(xd12)\n","        xu22 = torch.cat([xu2, xe32], dim=1)\n","        xd21 = self.d21(xu22)\n","        xd22 = self.d22(xd21)\n","\n","        xu3 = self.upconv3(xd22)\n","        xu33 = torch.cat([xu3, xe22], dim=1)\n","        xd31 = self.d31(xu33)\n","        xd32 = self.d32(xd31)\n","\n","        xu4 = self.upconv4(xd32)\n","        xu44 = torch.cat([xu4, xe12], dim=1)\n","        xd41 = self.d41(xu44)\n","        xd42 = self.d42(xd41)\n","\n","        # Output layer\n","        out = self.outconv(xd42)\n","\n","        return out"],"metadata":{"id":"7Bc52XUliJtl","executionInfo":{"status":"ok","timestamp":1701180553059,"user_tz":-60,"elapsed":14,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, random_split\n","from torch import optim\n","import torch\n","import os\n","import matplotlib.pyplot as plt\n","\n","def dice_coefficient(predicted, target, num_classes):\n","    dice_scores = []  # To store dice coefficient for each class\n","\n","    # Convert predictions and targets to one-hot encoded form\n","    predicted_one_hot = F.one_hot(predicted, num_classes).permute(0, 3, 1, 2).float()\n","    target_one_hot = F.one_hot(target, num_classes).permute(0, 3, 1, 2).float()\n","\n","    # Calculate Dice coefficient for each class\n","    for class_index in range(num_classes):\n","        intersection = (predicted_one_hot[:, class_index, :, :] * target_one_hot[:, class_index, :, :]).sum()\n","        union = predicted_one_hot[:, class_index, :, :].sum() + target_one_hot[:, class_index, :, :].sum()\n","        dice_score = (2 * intersection + 1e-6) / (union + 1e-6)  # Adding a small epsilon to avoid division by zero\n","        dice_scores.append(dice_score)\n","\n","    # Average Dice score across all classes\n","    avg_dice_score = sum(dice_scores) / len(dice_scores)\n","    return avg_dice_score.item()  # Return the value as a Python scalar\n","\n","def get_image_paths(data_dir, label_dir):\n","    data_paths = [os.path.join(data_dir, img) for img in sorted(os.listdir(data_dir))]\n","    label_paths = [os.path.join(label_dir, lbl) for lbl in sorted(os.listdir(label_dir))]\n","    return data_paths, label_paths\n","\n","def create_subsets(dataset, subset_sizes):\n","    subsets = {}\n","    for size in subset_sizes:\n","        if size == len(dataset):\n","            subsets[size] = dataset  # Use the full dataset\n","        else:\n","            subset, _ = random_split(dataset, [size, len(dataset) - size])\n","            subsets[size] = subset\n","    return subsets"],"metadata":{"id":"tjYoI2YS8WpH","executionInfo":{"status":"ok","timestamp":1701180553059,"user_tz":-60,"elapsed":12,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Define your dataset paths\n","data_dir = '/content/gdrive/MyDrive/training_dataset/data_crop/'\n","label_dir = '/content/gdrive/MyDrive/training_dataset/label_crop/'\n","\n","# Get image paths and create the full dataset\n","image_paths, label_paths = get_image_paths(data_dir, label_dir)\n","dataset = CustomDataset(image_paths=image_paths, label_paths=label_paths)\n","\n","# Define subset sizes including the full dataset size\n","subset_sizes = [50, 125, 250, len(dataset)]  # Add the full dataset size\n","\n","# Create subsets\n","dataset_subsets = create_subsets(dataset, subset_sizes)\n","\n","# Device setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using {device}\")\n","\n","# Training configurations\n","learning_rate = 0.001\n","num_epochs = 250  # Adjust as needed\n","\n","# Loop over subsets and train the model\n","for size, subset in dataset_subsets.items():\n","    print(f\"\\nTraining on subset size: {size}\")\n","\n","    # Split the subset into training, validation, and test datasets\n","    train_size = int(0.70 * len(subset))\n","    val_size = int(0.15 * len(subset))\n","    test_size = len(subset) - train_size - val_size\n","    train_dataset, val_dataset, test_dataset = random_split(subset, [train_size, val_size, test_size])\n","\n","    # DataLoader setup\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","    # Model, loss function, and optimizer setup\n","    model = UNet(n_class=3).to(device)\n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for batch_idx, (images, labels) in enumerate(train_loader):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            labels = labels.squeeze(1).long()\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            if batch_idx % 10 == 0:\n","                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item()}\")\n","\n","        # Validation phase\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = 0\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                labels = labels.squeeze(1).long()\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","            val_loss /= len(val_loader)\n","            print(f\"Validation Loss after Epoch {epoch+1}: {val_loss}\")\n","\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        test_loss = 0\n","        correct = 0\n","        total = 0\n","        dice_scores = []\n","\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            probabilities = F.softmax(outputs, dim=1)\n","            _, predicted = torch.max(probabilities, 1)\n","            labels = labels.squeeze(1).long()\n","\n","            loss = criterion(outputs, labels)\n","            test_loss += loss.item()\n","            total += labels.numel()\n","            correct += (predicted == labels).sum().item()\n","\n","            dice_score = dice_coefficient(predicted, labels, num_classes=3)\n","            dice_scores.append(dice_score)\n","\n","        test_loss /= len(test_loader)\n","        test_accuracy = 100 * correct / total\n","        average_dice_score = sum(dice_scores) / len(dice_scores)\n","\n","        print(f\"Subset size {size} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Average Dice Score: {average_dice_score:.4f}\")\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zbb85T-1iL01","executionInfo":{"status":"ok","timestamp":1701183321241,"user_tz":-60,"elapsed":2763224,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}},"outputId":"e84b5e6c-1f18-4438-99c0-c52c217b4a95"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda\n","\n","Training on subset size: 50\n","Epoch 1/250, Batch 1/2, Loss: 1.2206496000289917\n","Validation Loss after Epoch 1: 1.108677625656128\n","Epoch 2/250, Batch 1/2, Loss: 0.43256068229675293\n","Validation Loss after Epoch 2: 1.088362455368042\n","Epoch 3/250, Batch 1/2, Loss: 0.3106340765953064\n","Validation Loss after Epoch 3: 1.0665152072906494\n","Epoch 4/250, Batch 1/2, Loss: 0.2666759788990021\n","Validation Loss after Epoch 4: 1.0937740802764893\n","Epoch 5/250, Batch 1/2, Loss: 0.23554226756095886\n","Validation Loss after Epoch 5: 1.1008728742599487\n","Epoch 6/250, Batch 1/2, Loss: 0.22044607996940613\n","Validation Loss after Epoch 6: 1.3644424676895142\n","Epoch 7/250, Batch 1/2, Loss: 0.20196132361888885\n","Validation Loss after Epoch 7: 1.7136586904525757\n","Epoch 8/250, Batch 1/2, Loss: 0.18506842851638794\n","Validation Loss after Epoch 8: 0.9910423159599304\n","Epoch 9/250, Batch 1/2, Loss: 0.17292624711990356\n","Validation Loss after Epoch 9: 1.0913158655166626\n","Epoch 10/250, Batch 1/2, Loss: 0.15431155264377594\n","Validation Loss after Epoch 10: 1.0616413354873657\n","Epoch 11/250, Batch 1/2, Loss: 0.14845795929431915\n","Validation Loss after Epoch 11: 1.0028393268585205\n","Epoch 12/250, Batch 1/2, Loss: 0.13870544731616974\n","Validation Loss after Epoch 12: 0.8880325555801392\n","Epoch 13/250, Batch 1/2, Loss: 0.13060878217220306\n","Validation Loss after Epoch 13: 0.836251437664032\n","Epoch 14/250, Batch 1/2, Loss: 0.11743062734603882\n","Validation Loss after Epoch 14: 0.7374330163002014\n","Epoch 15/250, Batch 1/2, Loss: 0.11446470767259598\n","Validation Loss after Epoch 15: 0.694696307182312\n","Epoch 16/250, Batch 1/2, Loss: 0.106193907558918\n","Validation Loss after Epoch 16: 0.6639760732650757\n","Epoch 17/250, Batch 1/2, Loss: 0.10805731266736984\n","Validation Loss after Epoch 17: 0.6851387619972229\n","Epoch 18/250, Batch 1/2, Loss: 0.1033247709274292\n","Validation Loss after Epoch 18: 0.7230305075645447\n","Epoch 19/250, Batch 1/2, Loss: 0.09905624389648438\n","Validation Loss after Epoch 19: 0.7497870326042175\n","Epoch 20/250, Batch 1/2, Loss: 0.10113575309515\n","Validation Loss after Epoch 20: 0.7563682198524475\n","Epoch 21/250, Batch 1/2, Loss: 0.09553487598896027\n","Validation Loss after Epoch 21: 0.7096644639968872\n","Epoch 22/250, Batch 1/2, Loss: 0.08836281299591064\n","Validation Loss after Epoch 22: 0.659672737121582\n","Epoch 23/250, Batch 1/2, Loss: 0.08165513724088669\n","Validation Loss after Epoch 23: 0.6139394044876099\n","Epoch 24/250, Batch 1/2, Loss: 0.07902076840400696\n","Validation Loss after Epoch 24: 0.5806335806846619\n","Epoch 25/250, Batch 1/2, Loss: 0.07673124223947525\n","Validation Loss after Epoch 25: 0.4918789863586426\n","Epoch 26/250, Batch 1/2, Loss: 0.0769457072019577\n","Validation Loss after Epoch 26: 0.4674665033817291\n","Epoch 27/250, Batch 1/2, Loss: 0.0724281445145607\n","Validation Loss after Epoch 27: 0.40712958574295044\n","Epoch 28/250, Batch 1/2, Loss: 0.07378910481929779\n","Validation Loss after Epoch 28: 0.32483962178230286\n","Epoch 29/250, Batch 1/2, Loss: 0.07205525785684586\n","Validation Loss after Epoch 29: 0.2195405215024948\n","Epoch 30/250, Batch 1/2, Loss: 0.06802625954151154\n","Validation Loss after Epoch 30: 0.16327457129955292\n","Epoch 31/250, Batch 1/2, Loss: 0.06673362106084824\n","Validation Loss after Epoch 31: 0.1659010797739029\n","Epoch 32/250, Batch 1/2, Loss: 0.06632301211357117\n","Validation Loss after Epoch 32: 0.18140313029289246\n","Epoch 33/250, Batch 1/2, Loss: 0.06473895907402039\n","Validation Loss after Epoch 33: 0.20837529003620148\n","Epoch 34/250, Batch 1/2, Loss: 0.0609540194272995\n","Validation Loss after Epoch 34: 0.2193680852651596\n","Epoch 35/250, Batch 1/2, Loss: 0.06147773191332817\n","Validation Loss after Epoch 35: 0.21221618354320526\n","Epoch 36/250, Batch 1/2, Loss: 0.05904175713658333\n","Validation Loss after Epoch 36: 0.1892366111278534\n","Epoch 37/250, Batch 1/2, Loss: 0.06004762649536133\n","Validation Loss after Epoch 37: 0.1668154001235962\n","Epoch 38/250, Batch 1/2, Loss: 0.055391471832990646\n","Validation Loss after Epoch 38: 0.1545991152524948\n","Epoch 39/250, Batch 1/2, Loss: 0.05361957475543022\n","Validation Loss after Epoch 39: 0.12393160909414291\n","Epoch 40/250, Batch 1/2, Loss: 0.0549464076757431\n","Validation Loss after Epoch 40: 0.07808788120746613\n","Epoch 41/250, Batch 1/2, Loss: 0.054492004215717316\n","Validation Loss after Epoch 41: 0.06272105127573013\n","Epoch 42/250, Batch 1/2, Loss: 0.052173200994729996\n","Validation Loss after Epoch 42: 0.05813898518681526\n","Epoch 43/250, Batch 1/2, Loss: 0.053856320679187775\n","Validation Loss after Epoch 43: 0.0561603307723999\n","Epoch 44/250, Batch 1/2, Loss: 0.05077655240893364\n","Validation Loss after Epoch 44: 0.06048974022269249\n","Epoch 45/250, Batch 1/2, Loss: 0.05178554728627205\n","Validation Loss after Epoch 45: 0.07363415509462357\n","Epoch 46/250, Batch 1/2, Loss: 0.05137431249022484\n","Validation Loss after Epoch 46: 0.08046456426382065\n","Epoch 47/250, Batch 1/2, Loss: 0.050468023866415024\n","Validation Loss after Epoch 47: 0.06321682780981064\n","Epoch 48/250, Batch 1/2, Loss: 0.048305694013834\n","Validation Loss after Epoch 48: 0.05467205122113228\n","Epoch 49/250, Batch 1/2, Loss: 0.04579084366559982\n","Validation Loss after Epoch 49: 0.048482198268175125\n","Epoch 50/250, Batch 1/2, Loss: 0.04598416015505791\n","Validation Loss after Epoch 50: 0.0460372194647789\n","Epoch 51/250, Batch 1/2, Loss: 0.048785027116537094\n","Validation Loss after Epoch 51: 0.04511032626032829\n","Epoch 52/250, Batch 1/2, Loss: 0.048442937433719635\n","Validation Loss after Epoch 52: 0.04231082275509834\n","Epoch 53/250, Batch 1/2, Loss: 0.04681131988763809\n","Validation Loss after Epoch 53: 0.04507800564169884\n","Epoch 54/250, Batch 1/2, Loss: 0.04929649084806442\n","Validation Loss after Epoch 54: 0.04875045642256737\n","Epoch 55/250, Batch 1/2, Loss: 0.047385074198246\n","Validation Loss after Epoch 55: 0.04695216938853264\n","Epoch 56/250, Batch 1/2, Loss: 0.04622981697320938\n","Validation Loss after Epoch 56: 0.049207061529159546\n","Epoch 57/250, Batch 1/2, Loss: 0.051192980259656906\n","Validation Loss after Epoch 57: 0.07955460995435715\n","Epoch 58/250, Batch 1/2, Loss: 0.05180835351347923\n","Validation Loss after Epoch 58: 0.11268734186887741\n","Epoch 59/250, Batch 1/2, Loss: 0.05228375643491745\n","Validation Loss after Epoch 59: 0.06489507853984833\n","Epoch 60/250, Batch 1/2, Loss: 0.0455327145755291\n","Validation Loss after Epoch 60: 0.04342338442802429\n","Epoch 61/250, Batch 1/2, Loss: 0.04299406707286835\n","Validation Loss after Epoch 61: 0.0639529600739479\n","Epoch 62/250, Batch 1/2, Loss: 0.04849924147129059\n","Validation Loss after Epoch 62: 0.07431899756193161\n","Epoch 63/250, Batch 1/2, Loss: 0.04494500532746315\n","Validation Loss after Epoch 63: 0.06390637904405594\n","Epoch 64/250, Batch 1/2, Loss: 0.044304002076387405\n","Validation Loss after Epoch 64: 0.05339250713586807\n","Epoch 65/250, Batch 1/2, Loss: 0.04205857589840889\n","Validation Loss after Epoch 65: 0.04844004288315773\n","Epoch 66/250, Batch 1/2, Loss: 0.04125675559043884\n","Validation Loss after Epoch 66: 0.05159724876284599\n","Epoch 67/250, Batch 1/2, Loss: 0.040738724172115326\n","Validation Loss after Epoch 67: 0.061263781040906906\n","Epoch 68/250, Batch 1/2, Loss: 0.04057258740067482\n","Validation Loss after Epoch 68: 0.056722961366176605\n","Epoch 69/250, Batch 1/2, Loss: 0.041953179985284805\n","Validation Loss after Epoch 69: 0.043034736067056656\n","Epoch 70/250, Batch 1/2, Loss: 0.04153125733137131\n","Validation Loss after Epoch 70: 0.04097699746489525\n","Epoch 71/250, Batch 1/2, Loss: 0.04117710515856743\n","Validation Loss after Epoch 71: 0.05258043482899666\n","Epoch 72/250, Batch 1/2, Loss: 0.04117046669125557\n","Validation Loss after Epoch 72: 0.054378677159547806\n","Epoch 73/250, Batch 1/2, Loss: 0.040616173297166824\n","Validation Loss after Epoch 73: 0.04210427775979042\n","Epoch 74/250, Batch 1/2, Loss: 0.03857800364494324\n","Validation Loss after Epoch 74: 0.04268417879939079\n","Epoch 75/250, Batch 1/2, Loss: 0.04429473355412483\n","Validation Loss after Epoch 75: 0.04156038910150528\n","Epoch 76/250, Batch 1/2, Loss: 0.043346114456653595\n","Validation Loss after Epoch 76: 0.037872232496738434\n","Epoch 77/250, Batch 1/2, Loss: 0.03898569196462631\n","Validation Loss after Epoch 77: 0.04077677056193352\n","Epoch 78/250, Batch 1/2, Loss: 0.041410330682992935\n","Validation Loss after Epoch 78: 0.055275388062000275\n","Epoch 79/250, Batch 1/2, Loss: 0.04945193603634834\n","Validation Loss after Epoch 79: 0.05480461195111275\n","Epoch 80/250, Batch 1/2, Loss: 0.04759961739182472\n","Validation Loss after Epoch 80: 0.047448474913835526\n","Epoch 81/250, Batch 1/2, Loss: 0.04421866312623024\n","Validation Loss after Epoch 81: 0.041948527097702026\n","Epoch 82/250, Batch 1/2, Loss: 0.03973390907049179\n","Validation Loss after Epoch 82: 0.03709429129958153\n","Epoch 83/250, Batch 1/2, Loss: 0.03633855655789375\n","Validation Loss after Epoch 83: 0.043715834617614746\n","Epoch 84/250, Batch 1/2, Loss: 0.03588058426976204\n","Validation Loss after Epoch 84: 0.06173430755734444\n","Epoch 85/250, Batch 1/2, Loss: 0.039331551641225815\n","Validation Loss after Epoch 85: 0.07246404886245728\n","Epoch 86/250, Batch 1/2, Loss: 0.03879166021943092\n","Validation Loss after Epoch 86: 0.05801931768655777\n","Epoch 87/250, Batch 1/2, Loss: 0.037045884877443314\n","Validation Loss after Epoch 87: 0.04068617895245552\n","Epoch 88/250, Batch 1/2, Loss: 0.03581586852669716\n","Validation Loss after Epoch 88: 0.03763854503631592\n","Epoch 89/250, Batch 1/2, Loss: 0.03492423892021179\n","Validation Loss after Epoch 89: 0.03683837130665779\n","Epoch 90/250, Batch 1/2, Loss: 0.0346880778670311\n","Validation Loss after Epoch 90: 0.03703080490231514\n","Epoch 91/250, Batch 1/2, Loss: 0.034218452870845795\n","Validation Loss after Epoch 91: 0.0387747623026371\n","Epoch 92/250, Batch 1/2, Loss: 0.03475947305560112\n","Validation Loss after Epoch 92: 0.0404813252389431\n","Epoch 93/250, Batch 1/2, Loss: 0.03379248455166817\n","Validation Loss after Epoch 93: 0.04112165421247482\n","Epoch 94/250, Batch 1/2, Loss: 0.032829008996486664\n","Validation Loss after Epoch 94: 0.03773250803351402\n","Epoch 95/250, Batch 1/2, Loss: 0.03250990808010101\n","Validation Loss after Epoch 95: 0.04047142714262009\n","Epoch 96/250, Batch 1/2, Loss: 0.03272804617881775\n","Validation Loss after Epoch 96: 0.05058775097131729\n","Epoch 97/250, Batch 1/2, Loss: 0.03842264413833618\n","Validation Loss after Epoch 97: 0.04557311162352562\n","Epoch 98/250, Batch 1/2, Loss: 0.04030086472630501\n","Validation Loss after Epoch 98: 0.03892377018928528\n","Epoch 99/250, Batch 1/2, Loss: 0.03611717373132706\n","Validation Loss after Epoch 99: 0.03880247846245766\n","Epoch 100/250, Batch 1/2, Loss: 0.03852137550711632\n","Validation Loss after Epoch 100: 0.03883277624845505\n","Epoch 101/250, Batch 1/2, Loss: 0.036344386637210846\n","Validation Loss after Epoch 101: 0.036592576652765274\n","Epoch 102/250, Batch 1/2, Loss: 0.037550654262304306\n","Validation Loss after Epoch 102: 0.038241080939769745\n","Epoch 103/250, Batch 1/2, Loss: 0.03815130889415741\n","Validation Loss after Epoch 103: 0.037295784801244736\n","Epoch 104/250, Batch 1/2, Loss: 0.037775080651044846\n","Validation Loss after Epoch 104: 0.0348786786198616\n","Epoch 105/250, Batch 1/2, Loss: 0.03619726002216339\n","Validation Loss after Epoch 105: 0.03671620413661003\n","Epoch 106/250, Batch 1/2, Loss: 0.039185333997011185\n","Validation Loss after Epoch 106: 0.035606276243925095\n","Epoch 107/250, Batch 1/2, Loss: 0.035648755729198456\n","Validation Loss after Epoch 107: 0.035957153886556625\n","Epoch 108/250, Batch 1/2, Loss: 0.03436389937996864\n","Validation Loss after Epoch 108: 0.03966841474175453\n","Epoch 109/250, Batch 1/2, Loss: 0.032611068338155746\n","Validation Loss after Epoch 109: 0.0429878905415535\n","Epoch 110/250, Batch 1/2, Loss: 0.03274455666542053\n","Validation Loss after Epoch 110: 0.04417084529995918\n","Epoch 111/250, Batch 1/2, Loss: 0.03319256007671356\n","Validation Loss after Epoch 111: 0.044232286512851715\n","Epoch 112/250, Batch 1/2, Loss: 0.03271917253732681\n","Validation Loss after Epoch 112: 0.038721971213817596\n","Epoch 113/250, Batch 1/2, Loss: 0.03153509646654129\n","Validation Loss after Epoch 113: 0.041190918534994125\n","Epoch 114/250, Batch 1/2, Loss: 0.04054989665746689\n","Validation Loss after Epoch 114: 0.039338547736406326\n","Epoch 115/250, Batch 1/2, Loss: 0.04241558164358139\n","Validation Loss after Epoch 115: 0.03410065919160843\n","Epoch 116/250, Batch 1/2, Loss: 0.033050719648599625\n","Validation Loss after Epoch 116: 0.04071018472313881\n","Epoch 117/250, Batch 1/2, Loss: 0.038701292127370834\n","Validation Loss after Epoch 117: 0.04672873392701149\n","Epoch 118/250, Batch 1/2, Loss: 0.04270226135849953\n","Validation Loss after Epoch 118: 0.041971053928136826\n","Epoch 119/250, Batch 1/2, Loss: 0.03385061398148537\n","Validation Loss after Epoch 119: 0.036743272095918655\n","Epoch 120/250, Batch 1/2, Loss: 0.03433149307966232\n","Validation Loss after Epoch 120: 0.043292783200740814\n","Epoch 121/250, Batch 1/2, Loss: 0.034162018448114395\n","Validation Loss after Epoch 121: 0.047160517424345016\n","Epoch 122/250, Batch 1/2, Loss: 0.03279415890574455\n","Validation Loss after Epoch 122: 0.0472993366420269\n","Epoch 123/250, Batch 1/2, Loss: 0.03375660628080368\n","Validation Loss after Epoch 123: 0.04333954304456711\n","Epoch 124/250, Batch 1/2, Loss: 0.034128181636333466\n","Validation Loss after Epoch 124: 0.03949233517050743\n","Epoch 125/250, Batch 1/2, Loss: 0.03362089395523071\n","Validation Loss after Epoch 125: 0.03740484267473221\n","Epoch 126/250, Batch 1/2, Loss: 0.03392191231250763\n","Validation Loss after Epoch 126: 0.036418795585632324\n","Epoch 127/250, Batch 1/2, Loss: 0.03162924572825432\n","Validation Loss after Epoch 127: 0.03316884860396385\n","Epoch 128/250, Batch 1/2, Loss: 0.03442460298538208\n","Validation Loss after Epoch 128: 0.03639787808060646\n","Epoch 129/250, Batch 1/2, Loss: 0.03290285915136337\n","Validation Loss after Epoch 129: 0.04895534738898277\n","Epoch 130/250, Batch 1/2, Loss: 0.031962715089321136\n","Validation Loss after Epoch 130: 0.04500194266438484\n","Epoch 131/250, Batch 1/2, Loss: 0.03943393751978874\n","Validation Loss after Epoch 131: 0.04143606871366501\n","Epoch 132/250, Batch 1/2, Loss: 0.041415099054574966\n","Validation Loss after Epoch 132: 0.05849117785692215\n","Epoch 133/250, Batch 1/2, Loss: 0.03770436719059944\n","Validation Loss after Epoch 133: 0.05662676319479942\n","Epoch 134/250, Batch 1/2, Loss: 0.03338561952114105\n","Validation Loss after Epoch 134: 0.040828853845596313\n","Epoch 135/250, Batch 1/2, Loss: 0.034028708934783936\n","Validation Loss after Epoch 135: 0.03437306359410286\n","Epoch 136/250, Batch 1/2, Loss: 0.03574059158563614\n","Validation Loss after Epoch 136: 0.03589906170964241\n","Epoch 137/250, Batch 1/2, Loss: 0.031563881784677505\n","Validation Loss after Epoch 137: 0.036039113998413086\n","Epoch 138/250, Batch 1/2, Loss: 0.03060727007687092\n","Validation Loss after Epoch 138: 0.03752857446670532\n","Epoch 139/250, Batch 1/2, Loss: 0.035065386444330215\n","Validation Loss after Epoch 139: 0.03853916749358177\n","Epoch 140/250, Batch 1/2, Loss: 0.03926985710859299\n","Validation Loss after Epoch 140: 0.037404898554086685\n","Epoch 141/250, Batch 1/2, Loss: 0.03538824990391731\n","Validation Loss after Epoch 141: 0.04564161226153374\n","Epoch 142/250, Batch 1/2, Loss: 0.03335892781615257\n","Validation Loss after Epoch 142: 0.05625665932893753\n","Epoch 143/250, Batch 1/2, Loss: 0.03682807832956314\n","Validation Loss after Epoch 143: 0.0463176853954792\n","Epoch 144/250, Batch 1/2, Loss: 0.044561680406332016\n","Validation Loss after Epoch 144: 0.03428272157907486\n","Epoch 145/250, Batch 1/2, Loss: 0.04061314836144447\n","Validation Loss after Epoch 145: 0.042129140347242355\n","Epoch 146/250, Batch 1/2, Loss: 0.0355493389070034\n","Validation Loss after Epoch 146: 0.05372126027941704\n","Epoch 147/250, Batch 1/2, Loss: 0.03415928781032562\n","Validation Loss after Epoch 147: 0.05317790061235428\n","Epoch 148/250, Batch 1/2, Loss: 0.03377082571387291\n","Validation Loss after Epoch 148: 0.04232504963874817\n","Epoch 149/250, Batch 1/2, Loss: 0.03578449413180351\n","Validation Loss after Epoch 149: 0.041876889765262604\n","Epoch 150/250, Batch 1/2, Loss: 0.03996366634964943\n","Validation Loss after Epoch 150: 0.041991379112005234\n","Epoch 151/250, Batch 1/2, Loss: 0.04331269860267639\n","Validation Loss after Epoch 151: 0.04118121415376663\n","Epoch 152/250, Batch 1/2, Loss: 0.03551405295729637\n","Validation Loss after Epoch 152: 0.04884788766503334\n","Epoch 153/250, Batch 1/2, Loss: 0.03270716965198517\n","Validation Loss after Epoch 153: 0.058079201728105545\n","Epoch 154/250, Batch 1/2, Loss: 0.031879082322120667\n","Validation Loss after Epoch 154: 0.05658307671546936\n","Epoch 155/250, Batch 1/2, Loss: 0.03313523530960083\n","Validation Loss after Epoch 155: 0.049342282116413116\n","Epoch 156/250, Batch 1/2, Loss: 0.03607887029647827\n","Validation Loss after Epoch 156: 0.049610111862421036\n","Epoch 157/250, Batch 1/2, Loss: 0.035966966301202774\n","Validation Loss after Epoch 157: 0.0540018305182457\n","Epoch 158/250, Batch 1/2, Loss: 0.034115321934223175\n","Validation Loss after Epoch 158: 0.056106097996234894\n","Epoch 159/250, Batch 1/2, Loss: 0.03375323861837387\n","Validation Loss after Epoch 159: 0.042135272175073624\n","Epoch 160/250, Batch 1/2, Loss: 0.030575377866625786\n","Validation Loss after Epoch 160: 0.036304205656051636\n","Epoch 161/250, Batch 1/2, Loss: 0.030422842130064964\n","Validation Loss after Epoch 161: 0.03419521450996399\n","Epoch 162/250, Batch 1/2, Loss: 0.030527211725711823\n","Validation Loss after Epoch 162: 0.036545101553201675\n","Epoch 163/250, Batch 1/2, Loss: 0.029432304203510284\n","Validation Loss after Epoch 163: 0.03888385742902756\n","Epoch 164/250, Batch 1/2, Loss: 0.028609581291675568\n","Validation Loss after Epoch 164: 0.03739526495337486\n","Epoch 165/250, Batch 1/2, Loss: 0.03001454286277294\n","Validation Loss after Epoch 165: 0.03334173932671547\n","Epoch 166/250, Batch 1/2, Loss: 0.02968478761613369\n","Validation Loss after Epoch 166: 0.03271117061376572\n","Epoch 167/250, Batch 1/2, Loss: 0.029057079926133156\n","Validation Loss after Epoch 167: 0.03597770258784294\n","Epoch 168/250, Batch 1/2, Loss: 0.028889959678053856\n","Validation Loss after Epoch 168: 0.0415659062564373\n","Epoch 169/250, Batch 1/2, Loss: 0.029095269739627838\n","Validation Loss after Epoch 169: 0.040008701384067535\n","Epoch 170/250, Batch 1/2, Loss: 0.029847798869013786\n","Validation Loss after Epoch 170: 0.03399905562400818\n","Epoch 171/250, Batch 1/2, Loss: 0.028036683797836304\n","Validation Loss after Epoch 171: 0.03132782131433487\n","Epoch 172/250, Batch 1/2, Loss: 0.027955276891589165\n","Validation Loss after Epoch 172: 0.03105730190873146\n","Epoch 173/250, Batch 1/2, Loss: 0.029987841844558716\n","Validation Loss after Epoch 173: 0.03161595016717911\n","Epoch 174/250, Batch 1/2, Loss: 0.028813056647777557\n","Validation Loss after Epoch 174: 0.031867265701293945\n","Epoch 175/250, Batch 1/2, Loss: 0.027217457070946693\n","Validation Loss after Epoch 175: 0.03602784126996994\n","Epoch 176/250, Batch 1/2, Loss: 0.028698613867163658\n","Validation Loss after Epoch 176: 0.04461309686303139\n","Epoch 177/250, Batch 1/2, Loss: 0.031595077365636826\n","Validation Loss after Epoch 177: 0.03986334428191185\n","Epoch 178/250, Batch 1/2, Loss: 0.029323970898985863\n","Validation Loss after Epoch 178: 0.034183528274297714\n","Epoch 179/250, Batch 1/2, Loss: 0.0296388678252697\n","Validation Loss after Epoch 179: 0.032095957547426224\n","Epoch 180/250, Batch 1/2, Loss: 0.029231000691652298\n","Validation Loss after Epoch 180: 0.03168625757098198\n","Epoch 181/250, Batch 1/2, Loss: 0.03069162182509899\n","Validation Loss after Epoch 181: 0.03507543355226517\n","Epoch 182/250, Batch 1/2, Loss: 0.030203770846128464\n","Validation Loss after Epoch 182: 0.035849638283252716\n","Epoch 183/250, Batch 1/2, Loss: 0.030288200825452805\n","Validation Loss after Epoch 183: 0.03614286333322525\n","Epoch 184/250, Batch 1/2, Loss: 0.028574815019965172\n","Validation Loss after Epoch 184: 0.04606315493583679\n","Epoch 185/250, Batch 1/2, Loss: 0.027177784591913223\n","Validation Loss after Epoch 185: 0.05298004671931267\n","Epoch 186/250, Batch 1/2, Loss: 0.028259698301553726\n","Validation Loss after Epoch 186: 0.04054833576083183\n","Epoch 187/250, Batch 1/2, Loss: 0.029020138084888458\n","Validation Loss after Epoch 187: 0.033628907054662704\n","Epoch 188/250, Batch 1/2, Loss: 0.026451369747519493\n","Validation Loss after Epoch 188: 0.03759276494383812\n","Epoch 189/250, Batch 1/2, Loss: 0.027804501354694366\n","Validation Loss after Epoch 189: 0.04061521962285042\n","Epoch 190/250, Batch 1/2, Loss: 0.029968975111842155\n","Validation Loss after Epoch 190: 0.03673020377755165\n","Epoch 191/250, Batch 1/2, Loss: 0.03026404231786728\n","Validation Loss after Epoch 191: 0.03316078707575798\n","Epoch 192/250, Batch 1/2, Loss: 0.028235845267772675\n","Validation Loss after Epoch 192: 0.034300003200769424\n","Epoch 193/250, Batch 1/2, Loss: 0.02553737722337246\n","Validation Loss after Epoch 193: 0.035917650908231735\n","Epoch 194/250, Batch 1/2, Loss: 0.027164490893483162\n","Validation Loss after Epoch 194: 0.04025549069046974\n","Epoch 195/250, Batch 1/2, Loss: 0.026443947106599808\n","Validation Loss after Epoch 195: 0.04115861654281616\n","Epoch 196/250, Batch 1/2, Loss: 0.026532862335443497\n","Validation Loss after Epoch 196: 0.03368372097611427\n","Epoch 197/250, Batch 1/2, Loss: 0.025505220517516136\n","Validation Loss after Epoch 197: 0.030178118497133255\n","Epoch 198/250, Batch 1/2, Loss: 0.025693081319332123\n","Validation Loss after Epoch 198: 0.030227389186620712\n","Epoch 199/250, Batch 1/2, Loss: 0.02665785327553749\n","Validation Loss after Epoch 199: 0.030360644683241844\n","Epoch 200/250, Batch 1/2, Loss: 0.02719084732234478\n","Validation Loss after Epoch 200: 0.029650675132870674\n","Epoch 201/250, Batch 1/2, Loss: 0.02666383981704712\n","Validation Loss after Epoch 201: 0.0287344790995121\n","Epoch 202/250, Batch 1/2, Loss: 0.024878447875380516\n","Validation Loss after Epoch 202: 0.029069246724247932\n","Epoch 203/250, Batch 1/2, Loss: 0.025051182135939598\n","Validation Loss after Epoch 203: 0.02957688271999359\n","Epoch 204/250, Batch 1/2, Loss: 0.026086069643497467\n","Validation Loss after Epoch 204: 0.03156778961420059\n","Epoch 205/250, Batch 1/2, Loss: 0.026841573417186737\n","Validation Loss after Epoch 205: 0.0353940986096859\n","Epoch 206/250, Batch 1/2, Loss: 0.02703033573925495\n","Validation Loss after Epoch 206: 0.036172907799482346\n","Epoch 207/250, Batch 1/2, Loss: 0.027160538360476494\n","Validation Loss after Epoch 207: 0.032776594161987305\n","Epoch 208/250, Batch 1/2, Loss: 0.025093846023082733\n","Validation Loss after Epoch 208: 0.031656570732593536\n","Epoch 209/250, Batch 1/2, Loss: 0.02534249983727932\n","Validation Loss after Epoch 209: 0.032242823392152786\n","Epoch 210/250, Batch 1/2, Loss: 0.027153458446264267\n","Validation Loss after Epoch 210: 0.033106729388237\n","Epoch 211/250, Batch 1/2, Loss: 0.028239233419299126\n","Validation Loss after Epoch 211: 0.033486511558294296\n","Epoch 212/250, Batch 1/2, Loss: 0.02761947177350521\n","Validation Loss after Epoch 212: 0.03515377640724182\n","Epoch 213/250, Batch 1/2, Loss: 0.02741621993482113\n","Validation Loss after Epoch 213: 0.03714936226606369\n","Epoch 214/250, Batch 1/2, Loss: 0.02548152394592762\n","Validation Loss after Epoch 214: 0.040669478476047516\n","Epoch 215/250, Batch 1/2, Loss: 0.024739859625697136\n","Validation Loss after Epoch 215: 0.033427026122808456\n","Epoch 216/250, Batch 1/2, Loss: 0.02376621961593628\n","Validation Loss after Epoch 216: 0.029451744630932808\n","Epoch 217/250, Batch 1/2, Loss: 0.02433755248785019\n","Validation Loss after Epoch 217: 0.0296334158629179\n","Epoch 218/250, Batch 1/2, Loss: 0.024980371817946434\n","Validation Loss after Epoch 218: 0.03197556361556053\n","Epoch 219/250, Batch 1/2, Loss: 0.023130327463150024\n","Validation Loss after Epoch 219: 0.03261902928352356\n","Epoch 220/250, Batch 1/2, Loss: 0.024939412251114845\n","Validation Loss after Epoch 220: 0.03457475081086159\n","Epoch 221/250, Batch 1/2, Loss: 0.024354785680770874\n","Validation Loss after Epoch 221: 0.03750988468527794\n","Epoch 222/250, Batch 1/2, Loss: 0.022994233295321465\n","Validation Loss after Epoch 222: 0.03987029567360878\n","Epoch 223/250, Batch 1/2, Loss: 0.02483191154897213\n","Validation Loss after Epoch 223: 0.035425715148448944\n","Epoch 224/250, Batch 1/2, Loss: 0.022737570106983185\n","Validation Loss after Epoch 224: 0.0379306934773922\n","Epoch 225/250, Batch 1/2, Loss: 0.0278310626745224\n","Validation Loss after Epoch 225: 0.035756487399339676\n","Epoch 226/250, Batch 1/2, Loss: 0.02700975351035595\n","Validation Loss after Epoch 226: 0.03141096979379654\n","Epoch 227/250, Batch 1/2, Loss: 0.025000212714076042\n","Validation Loss after Epoch 227: 0.033705223351716995\n","Epoch 228/250, Batch 1/2, Loss: 0.02661791630089283\n","Validation Loss after Epoch 228: 0.04430511221289635\n","Epoch 229/250, Batch 1/2, Loss: 0.03478474169969559\n","Validation Loss after Epoch 229: 0.03501176834106445\n","Epoch 230/250, Batch 1/2, Loss: 0.030627276748418808\n","Validation Loss after Epoch 230: 0.03168461099267006\n","Epoch 231/250, Batch 1/2, Loss: 0.02437179535627365\n","Validation Loss after Epoch 231: 0.033521465957164764\n","Epoch 232/250, Batch 1/2, Loss: 0.02972227893769741\n","Validation Loss after Epoch 232: 0.03420094773173332\n","Epoch 233/250, Batch 1/2, Loss: 0.028295189142227173\n","Validation Loss after Epoch 233: 0.03252521529793739\n","Epoch 234/250, Batch 1/2, Loss: 0.026079362258315086\n","Validation Loss after Epoch 234: 0.03093932755291462\n","Epoch 235/250, Batch 1/2, Loss: 0.025487814098596573\n","Validation Loss after Epoch 235: 0.029848476871848106\n","Epoch 236/250, Batch 1/2, Loss: 0.02632141299545765\n","Validation Loss after Epoch 236: 0.03002418950200081\n","Epoch 237/250, Batch 1/2, Loss: 0.02471574954688549\n","Validation Loss after Epoch 237: 0.03214769437909126\n","Epoch 238/250, Batch 1/2, Loss: 0.024978559464216232\n","Validation Loss after Epoch 238: 0.03268250450491905\n","Epoch 239/250, Batch 1/2, Loss: 0.023499425500631332\n","Validation Loss after Epoch 239: 0.03249424695968628\n","Epoch 240/250, Batch 1/2, Loss: 0.02320392057299614\n","Validation Loss after Epoch 240: 0.032419733703136444\n","Epoch 241/250, Batch 1/2, Loss: 0.023179292678833008\n","Validation Loss after Epoch 241: 0.03066168539226055\n","Epoch 242/250, Batch 1/2, Loss: 0.02349838986992836\n","Validation Loss after Epoch 242: 0.028967106714844704\n","Epoch 243/250, Batch 1/2, Loss: 0.023882944136857986\n","Validation Loss after Epoch 243: 0.028859641402959824\n","Epoch 244/250, Batch 1/2, Loss: 0.022481998428702354\n","Validation Loss after Epoch 244: 0.03024163842201233\n","Epoch 245/250, Batch 1/2, Loss: 0.0224221833050251\n","Validation Loss after Epoch 245: 0.030658064410090446\n","Epoch 246/250, Batch 1/2, Loss: 0.021735547110438347\n","Validation Loss after Epoch 246: 0.028540844097733498\n","Epoch 247/250, Batch 1/2, Loss: 0.025606198236346245\n","Validation Loss after Epoch 247: 0.029829906299710274\n","Epoch 248/250, Batch 1/2, Loss: 0.025202810764312744\n","Validation Loss after Epoch 248: 0.03073643147945404\n","Epoch 249/250, Batch 1/2, Loss: 0.025710655376315117\n","Validation Loss after Epoch 249: 0.033230144530534744\n","Epoch 250/250, Batch 1/2, Loss: 0.02652050368487835\n","Validation Loss after Epoch 250: 0.02950400300323963\n","Subset size 50 - Test Loss: 0.0457, Test Accuracy: 98.57%, Average Dice Score: 0.9848\n","\n","Training on subset size: 125\n","Epoch 1/250, Batch 1/3, Loss: 1.1358336210250854\n","Validation Loss after Epoch 1: 1.127689003944397\n","Epoch 2/250, Batch 1/3, Loss: 0.38580021262168884\n","Validation Loss after Epoch 2: 1.0821250677108765\n","Epoch 3/250, Batch 1/3, Loss: 0.2834252417087555\n","Validation Loss after Epoch 3: 1.120896816253662\n","Epoch 4/250, Batch 1/3, Loss: 0.23079471290111542\n","Validation Loss after Epoch 4: 1.1719586849212646\n","Epoch 5/250, Batch 1/3, Loss: 0.20221062004566193\n","Validation Loss after Epoch 5: 1.2425780296325684\n","Epoch 6/250, Batch 1/3, Loss: 0.1910126805305481\n","Validation Loss after Epoch 6: 1.290344476699829\n","Epoch 7/250, Batch 1/3, Loss: 0.16419781744480133\n","Validation Loss after Epoch 7: 1.3029828071594238\n","Epoch 8/250, Batch 1/3, Loss: 0.14458714425563812\n","Validation Loss after Epoch 8: 1.21198308467865\n","Epoch 9/250, Batch 1/3, Loss: 0.12868666648864746\n","Validation Loss after Epoch 9: 1.0402207374572754\n","Epoch 10/250, Batch 1/3, Loss: 0.11947057396173477\n","Validation Loss after Epoch 10: 0.9490612745285034\n","Epoch 11/250, Batch 1/3, Loss: 0.10831514745950699\n","Validation Loss after Epoch 11: 0.87996506690979\n","Epoch 12/250, Batch 1/3, Loss: 0.09676901251077652\n","Validation Loss after Epoch 12: 0.797180712223053\n","Epoch 13/250, Batch 1/3, Loss: 0.09184227883815765\n","Validation Loss after Epoch 13: 0.7623120546340942\n","Epoch 14/250, Batch 1/3, Loss: 0.08131877332925797\n","Validation Loss after Epoch 14: 0.7555394172668457\n","Epoch 15/250, Batch 1/3, Loss: 0.07600615918636322\n","Validation Loss after Epoch 15: 0.7433449029922485\n","Epoch 16/250, Batch 1/3, Loss: 0.0769236758351326\n","Validation Loss after Epoch 16: 0.7307308912277222\n","Epoch 17/250, Batch 1/3, Loss: 0.06805310398340225\n","Validation Loss after Epoch 17: 0.6386798024177551\n","Epoch 18/250, Batch 1/3, Loss: 0.06957980245351791\n","Validation Loss after Epoch 18: 0.505865752696991\n","Epoch 19/250, Batch 1/3, Loss: 0.05878119915723801\n","Validation Loss after Epoch 19: 0.3365754783153534\n","Epoch 20/250, Batch 1/3, Loss: 0.056287527084350586\n","Validation Loss after Epoch 20: 0.2518113851547241\n","Epoch 21/250, Batch 1/3, Loss: 0.061785779893398285\n","Validation Loss after Epoch 21: 0.14504379034042358\n","Epoch 22/250, Batch 1/3, Loss: 0.06532467156648636\n","Validation Loss after Epoch 22: 0.10538692772388458\n","Epoch 23/250, Batch 1/3, Loss: 0.05356147885322571\n","Validation Loss after Epoch 23: 0.10601019859313965\n","Epoch 24/250, Batch 1/3, Loss: 0.048997633159160614\n","Validation Loss after Epoch 24: 0.07524420320987701\n","Epoch 25/250, Batch 1/3, Loss: 0.04900377243757248\n","Validation Loss after Epoch 25: 0.0645122304558754\n","Epoch 26/250, Batch 1/3, Loss: 0.04480213299393654\n","Validation Loss after Epoch 26: 0.0720660388469696\n","Epoch 27/250, Batch 1/3, Loss: 0.04647327587008476\n","Validation Loss after Epoch 27: 0.06564867496490479\n","Epoch 28/250, Batch 1/3, Loss: 0.04898885637521744\n","Validation Loss after Epoch 28: 0.06545555591583252\n","Epoch 29/250, Batch 1/3, Loss: 0.043814197182655334\n","Validation Loss after Epoch 29: 0.06177157908678055\n","Epoch 30/250, Batch 1/3, Loss: 0.04721437022089958\n","Validation Loss after Epoch 30: 0.05166520178318024\n","Epoch 31/250, Batch 1/3, Loss: 0.04141801968216896\n","Validation Loss after Epoch 31: 0.06646918505430222\n","Epoch 32/250, Batch 1/3, Loss: 0.04492257162928581\n","Validation Loss after Epoch 32: 0.05611295998096466\n","Epoch 33/250, Batch 1/3, Loss: 0.03975081071257591\n","Validation Loss after Epoch 33: 0.04673086479306221\n","Epoch 34/250, Batch 1/3, Loss: 0.04336993768811226\n","Validation Loss after Epoch 34: 0.050507623702287674\n","Epoch 35/250, Batch 1/3, Loss: 0.04113078862428665\n","Validation Loss after Epoch 35: 0.045285504311323166\n","Epoch 36/250, Batch 1/3, Loss: 0.041750676929950714\n","Validation Loss after Epoch 36: 0.04425923153758049\n","Epoch 37/250, Batch 1/3, Loss: 0.036515068262815475\n","Validation Loss after Epoch 37: 0.045185305178165436\n","Epoch 38/250, Batch 1/3, Loss: 0.03928903117775917\n","Validation Loss after Epoch 38: 0.04271223768591881\n","Epoch 39/250, Batch 1/3, Loss: 0.03634488210082054\n","Validation Loss after Epoch 39: 0.040959130972623825\n","Epoch 40/250, Batch 1/3, Loss: 0.03509240224957466\n","Validation Loss after Epoch 40: 0.041632477194070816\n","Epoch 41/250, Batch 1/3, Loss: 0.03293713927268982\n","Validation Loss after Epoch 41: 0.04322297126054764\n","Epoch 42/250, Batch 1/3, Loss: 0.04325982555747032\n","Validation Loss after Epoch 42: 0.04119051620364189\n","Epoch 43/250, Batch 1/3, Loss: 0.034393519163131714\n","Validation Loss after Epoch 43: 0.044246673583984375\n","Epoch 44/250, Batch 1/3, Loss: 0.03178596496582031\n","Validation Loss after Epoch 44: 0.0426933728158474\n","Epoch 45/250, Batch 1/3, Loss: 0.030842363834381104\n","Validation Loss after Epoch 45: 0.03937472775578499\n","Epoch 46/250, Batch 1/3, Loss: 0.03385472297668457\n","Validation Loss after Epoch 46: 0.04082658886909485\n","Epoch 47/250, Batch 1/3, Loss: 0.036877408623695374\n","Validation Loss after Epoch 47: 0.04425496980547905\n","Epoch 48/250, Batch 1/3, Loss: 0.03160600736737251\n","Validation Loss after Epoch 48: 0.040202632546424866\n","Epoch 49/250, Batch 1/3, Loss: 0.036389242857694626\n","Validation Loss after Epoch 49: 0.03550387918949127\n","Epoch 50/250, Batch 1/3, Loss: 0.030193613842129707\n","Validation Loss after Epoch 50: 0.039063841104507446\n","Epoch 51/250, Batch 1/3, Loss: 0.03571169078350067\n","Validation Loss after Epoch 51: 0.04456975311040878\n","Epoch 52/250, Batch 1/3, Loss: 0.030899660661816597\n","Validation Loss after Epoch 52: 0.04550843685865402\n","Epoch 53/250, Batch 1/3, Loss: 0.029038695618510246\n","Validation Loss after Epoch 53: 0.03516854718327522\n","Epoch 54/250, Batch 1/3, Loss: 0.027952302247285843\n","Validation Loss after Epoch 54: 0.03377281129360199\n","Epoch 55/250, Batch 1/3, Loss: 0.0343366414308548\n","Validation Loss after Epoch 55: 0.04566778242588043\n","Epoch 56/250, Batch 1/3, Loss: 0.029695402830839157\n","Validation Loss after Epoch 56: 0.04994357377290726\n","Epoch 57/250, Batch 1/3, Loss: 0.027898326516151428\n","Validation Loss after Epoch 57: 0.03553430736064911\n","Epoch 58/250, Batch 1/3, Loss: 0.02786613628268242\n","Validation Loss after Epoch 58: 0.03407926484942436\n","Epoch 59/250, Batch 1/3, Loss: 0.030499273911118507\n","Validation Loss after Epoch 59: 0.0378689207136631\n","Epoch 60/250, Batch 1/3, Loss: 0.02888641506433487\n","Validation Loss after Epoch 60: 0.036375027149915695\n","Epoch 61/250, Batch 1/3, Loss: 0.027600087225437164\n","Validation Loss after Epoch 61: 0.035045113414525986\n","Epoch 62/250, Batch 1/3, Loss: 0.027837594971060753\n","Validation Loss after Epoch 62: 0.039685361087322235\n","Epoch 63/250, Batch 1/3, Loss: 0.02663445472717285\n","Validation Loss after Epoch 63: 0.038993582129478455\n","Epoch 64/250, Batch 1/3, Loss: 0.027780156582593918\n","Validation Loss after Epoch 64: 0.0437333770096302\n","Epoch 65/250, Batch 1/3, Loss: 0.02653731405735016\n","Validation Loss after Epoch 65: 0.03540359064936638\n","Epoch 66/250, Batch 1/3, Loss: 0.025022588670253754\n","Validation Loss after Epoch 66: 0.03761794790625572\n","Epoch 67/250, Batch 1/3, Loss: 0.024782724678516388\n","Validation Loss after Epoch 67: 0.03861245885491371\n","Epoch 68/250, Batch 1/3, Loss: 0.028042271733283997\n","Validation Loss after Epoch 68: 0.03471826761960983\n","Epoch 69/250, Batch 1/3, Loss: 0.02619652822613716\n","Validation Loss after Epoch 69: 0.04341490566730499\n","Epoch 70/250, Batch 1/3, Loss: 0.030406827107071877\n","Validation Loss after Epoch 70: 0.04841531068086624\n","Epoch 71/250, Batch 1/3, Loss: 0.029732223600149155\n","Validation Loss after Epoch 71: 0.04109685868024826\n","Epoch 72/250, Batch 1/3, Loss: 0.02341160550713539\n","Validation Loss after Epoch 72: 0.03963419795036316\n","Epoch 73/250, Batch 1/3, Loss: 0.023545723408460617\n","Validation Loss after Epoch 73: 0.04066312685608864\n","Epoch 74/250, Batch 1/3, Loss: 0.025019679218530655\n","Validation Loss after Epoch 74: 0.035198912024497986\n","Epoch 75/250, Batch 1/3, Loss: 0.025330448523163795\n","Validation Loss after Epoch 75: 0.038012076169252396\n","Epoch 76/250, Batch 1/3, Loss: 0.022232785820961\n","Validation Loss after Epoch 76: 0.04047270491719246\n","Epoch 77/250, Batch 1/3, Loss: 0.025220880284905434\n","Validation Loss after Epoch 77: 0.03739964962005615\n","Epoch 78/250, Batch 1/3, Loss: 0.024265902116894722\n","Validation Loss after Epoch 78: 0.04100770875811577\n","Epoch 79/250, Batch 1/3, Loss: 0.026446392759680748\n","Validation Loss after Epoch 79: 0.039222098886966705\n","Epoch 80/250, Batch 1/3, Loss: 0.02297772467136383\n","Validation Loss after Epoch 80: 0.033582571893930435\n","Epoch 81/250, Batch 1/3, Loss: 0.023448117077350616\n","Validation Loss after Epoch 81: 0.0323055163025856\n","Epoch 82/250, Batch 1/3, Loss: 0.022595098242163658\n","Validation Loss after Epoch 82: 0.0322633795440197\n","Epoch 83/250, Batch 1/3, Loss: 0.023938225582242012\n","Validation Loss after Epoch 83: 0.035873301327228546\n","Epoch 84/250, Batch 1/3, Loss: 0.02673713117837906\n","Validation Loss after Epoch 84: 0.03730200603604317\n","Epoch 85/250, Batch 1/3, Loss: 0.022970618680119514\n","Validation Loss after Epoch 85: 0.03332142531871796\n","Epoch 86/250, Batch 1/3, Loss: 0.028774850070476532\n","Validation Loss after Epoch 86: 0.03286537155508995\n","Epoch 87/250, Batch 1/3, Loss: 0.026245366781949997\n","Validation Loss after Epoch 87: 0.03031378984451294\n","Epoch 88/250, Batch 1/3, Loss: 0.023075247183442116\n","Validation Loss after Epoch 88: 0.03581131994724274\n","Epoch 89/250, Batch 1/3, Loss: 0.021085824817419052\n","Validation Loss after Epoch 89: 0.03425784781575203\n","Epoch 90/250, Batch 1/3, Loss: 0.021081391721963882\n","Validation Loss after Epoch 90: 0.04137543588876724\n","Epoch 91/250, Batch 1/3, Loss: 0.021353742107748985\n","Validation Loss after Epoch 91: 0.03985761106014252\n","Epoch 92/250, Batch 1/3, Loss: 0.021959546953439713\n","Validation Loss after Epoch 92: 0.03406834602355957\n","Epoch 93/250, Batch 1/3, Loss: 0.02237672731280327\n","Validation Loss after Epoch 93: 0.03489124774932861\n","Epoch 94/250, Batch 1/3, Loss: 0.020878849551081657\n","Validation Loss after Epoch 94: 0.033669982105493546\n","Epoch 95/250, Batch 1/3, Loss: 0.02102142758667469\n","Validation Loss after Epoch 95: 0.03639679774641991\n","Epoch 96/250, Batch 1/3, Loss: 0.02105714939534664\n","Validation Loss after Epoch 96: 0.03619733080267906\n","Epoch 97/250, Batch 1/3, Loss: 0.019119679927825928\n","Validation Loss after Epoch 97: 0.03404655307531357\n","Epoch 98/250, Batch 1/3, Loss: 0.019350260496139526\n","Validation Loss after Epoch 98: 0.02927938848733902\n","Epoch 99/250, Batch 1/3, Loss: 0.02469351701438427\n","Validation Loss after Epoch 99: 0.03483855724334717\n","Epoch 100/250, Batch 1/3, Loss: 0.02097109891474247\n","Validation Loss after Epoch 100: 0.03084402345120907\n","Epoch 101/250, Batch 1/3, Loss: 0.023551706224679947\n","Validation Loss after Epoch 101: 0.030374417081475258\n","Epoch 102/250, Batch 1/3, Loss: 0.021191615611314774\n","Validation Loss after Epoch 102: 0.038675758987665176\n","Epoch 103/250, Batch 1/3, Loss: 0.018984748050570488\n","Validation Loss after Epoch 103: 0.03407441824674606\n","Epoch 104/250, Batch 1/3, Loss: 0.019560353830456734\n","Validation Loss after Epoch 104: 0.03106984868645668\n","Epoch 105/250, Batch 1/3, Loss: 0.020641211420297623\n","Validation Loss after Epoch 105: 0.03215711563825607\n","Epoch 106/250, Batch 1/3, Loss: 0.01944616623222828\n","Validation Loss after Epoch 106: 0.03388502076268196\n","Epoch 107/250, Batch 1/3, Loss: 0.01978074200451374\n","Validation Loss after Epoch 107: 0.03490309417247772\n","Epoch 108/250, Batch 1/3, Loss: 0.022758852690458298\n","Validation Loss after Epoch 108: 0.03176768124103546\n","Epoch 109/250, Batch 1/3, Loss: 0.019852399826049805\n","Validation Loss after Epoch 109: 0.033103540539741516\n","Epoch 110/250, Batch 1/3, Loss: 0.019182775169610977\n","Validation Loss after Epoch 110: 0.03195936605334282\n","Epoch 111/250, Batch 1/3, Loss: 0.021394122391939163\n","Validation Loss after Epoch 111: 0.030820833519101143\n","Epoch 112/250, Batch 1/3, Loss: 0.01974036730825901\n","Validation Loss after Epoch 112: 0.03457942232489586\n","Epoch 113/250, Batch 1/3, Loss: 0.021969031542539597\n","Validation Loss after Epoch 113: 0.037511613219976425\n","Epoch 114/250, Batch 1/3, Loss: 0.01983572542667389\n","Validation Loss after Epoch 114: 0.035091567784547806\n","Epoch 115/250, Batch 1/3, Loss: 0.019092297181487083\n","Validation Loss after Epoch 115: 0.031765639781951904\n","Epoch 116/250, Batch 1/3, Loss: 0.0244913212954998\n","Validation Loss after Epoch 116: 0.0326700322329998\n","Epoch 117/250, Batch 1/3, Loss: 0.018124990165233612\n","Validation Loss after Epoch 117: 0.03171011060476303\n","Epoch 118/250, Batch 1/3, Loss: 0.016859546303749084\n","Validation Loss after Epoch 118: 0.031021393835544586\n","Epoch 119/250, Batch 1/3, Loss: 0.01833644136786461\n","Validation Loss after Epoch 119: 0.03108261711895466\n","Epoch 120/250, Batch 1/3, Loss: 0.017042366787791252\n","Validation Loss after Epoch 120: 0.03324206545948982\n","Epoch 121/250, Batch 1/3, Loss: 0.01771138422191143\n","Validation Loss after Epoch 121: 0.033732637763023376\n","Epoch 122/250, Batch 1/3, Loss: 0.017831826582551003\n","Validation Loss after Epoch 122: 0.036675795912742615\n","Epoch 123/250, Batch 1/3, Loss: 0.015670379623770714\n","Validation Loss after Epoch 123: 0.032131023705005646\n","Epoch 124/250, Batch 1/3, Loss: 0.01617112196981907\n","Validation Loss after Epoch 124: 0.03570236265659332\n","Epoch 125/250, Batch 1/3, Loss: 0.016512857750058174\n","Validation Loss after Epoch 125: 0.03727462515234947\n","Epoch 126/250, Batch 1/3, Loss: 0.01624075323343277\n","Validation Loss after Epoch 126: 0.032878030091524124\n","Epoch 127/250, Batch 1/3, Loss: 0.01666465401649475\n","Validation Loss after Epoch 127: 0.030485689640045166\n","Epoch 128/250, Batch 1/3, Loss: 0.01670951023697853\n","Validation Loss after Epoch 128: 0.03497840836644173\n","Epoch 129/250, Batch 1/3, Loss: 0.01770554482936859\n","Validation Loss after Epoch 129: 0.028293345123529434\n","Epoch 130/250, Batch 1/3, Loss: 0.019040850922465324\n","Validation Loss after Epoch 130: 0.036288872361183167\n","Epoch 131/250, Batch 1/3, Loss: 0.022564535960555077\n","Validation Loss after Epoch 131: 0.030729131773114204\n","Epoch 132/250, Batch 1/3, Loss: 0.01755780540406704\n","Validation Loss after Epoch 132: 0.037439268082380295\n","Epoch 133/250, Batch 1/3, Loss: 0.0169661957770586\n","Validation Loss after Epoch 133: 0.03895187750458717\n","Epoch 134/250, Batch 1/3, Loss: 0.020840033888816833\n","Validation Loss after Epoch 134: 0.03909321129322052\n","Epoch 135/250, Batch 1/3, Loss: 0.018085535615682602\n","Validation Loss after Epoch 135: 0.03234684839844704\n","Epoch 136/250, Batch 1/3, Loss: 0.017942389473319054\n","Validation Loss after Epoch 136: 0.03436665236949921\n","Epoch 137/250, Batch 1/3, Loss: 0.017982840538024902\n","Validation Loss after Epoch 137: 0.03197600319981575\n","Epoch 138/250, Batch 1/3, Loss: 0.016916422173380852\n","Validation Loss after Epoch 138: 0.028505252674221992\n","Epoch 139/250, Batch 1/3, Loss: 0.01574596017599106\n","Validation Loss after Epoch 139: 0.03254115208983421\n","Epoch 140/250, Batch 1/3, Loss: 0.018075020983815193\n","Validation Loss after Epoch 140: 0.0349004901945591\n","Epoch 141/250, Batch 1/3, Loss: 0.024050362408161163\n","Validation Loss after Epoch 141: 0.031535208225250244\n","Epoch 142/250, Batch 1/3, Loss: 0.018362585455179214\n","Validation Loss after Epoch 142: 0.03147319704294205\n","Epoch 143/250, Batch 1/3, Loss: 0.01643681898713112\n","Validation Loss after Epoch 143: 0.03069189190864563\n","Epoch 144/250, Batch 1/3, Loss: 0.01649535819888115\n","Validation Loss after Epoch 144: 0.029345111921429634\n","Epoch 145/250, Batch 1/3, Loss: 0.014883563853800297\n","Validation Loss after Epoch 145: 0.032350972294807434\n","Epoch 146/250, Batch 1/3, Loss: 0.01584438793361187\n","Validation Loss after Epoch 146: 0.038554057478904724\n","Epoch 147/250, Batch 1/3, Loss: 0.020946191623806953\n","Validation Loss after Epoch 147: 0.028971632942557335\n","Epoch 148/250, Batch 1/3, Loss: 0.017706336453557014\n","Validation Loss after Epoch 148: 0.03026163950562477\n","Epoch 149/250, Batch 1/3, Loss: 0.017645997926592827\n","Validation Loss after Epoch 149: 0.028087668120861053\n","Epoch 150/250, Batch 1/3, Loss: 0.016670571640133858\n","Validation Loss after Epoch 150: 0.03063124790787697\n","Epoch 151/250, Batch 1/3, Loss: 0.016820617020130157\n","Validation Loss after Epoch 151: 0.027603670954704285\n","Epoch 152/250, Batch 1/3, Loss: 0.014258197508752346\n","Validation Loss after Epoch 152: 0.028787152841687202\n","Epoch 153/250, Batch 1/3, Loss: 0.014203018508851528\n","Validation Loss after Epoch 153: 0.03057357296347618\n","Epoch 154/250, Batch 1/3, Loss: 0.014959922060370445\n","Validation Loss after Epoch 154: 0.0347486175596714\n","Epoch 155/250, Batch 1/3, Loss: 0.013506992720067501\n","Validation Loss after Epoch 155: 0.033707164227962494\n","Epoch 156/250, Batch 1/3, Loss: 0.014797653071582317\n","Validation Loss after Epoch 156: 0.034199684858322144\n","Epoch 157/250, Batch 1/3, Loss: 0.013597861863672733\n","Validation Loss after Epoch 157: 0.03172893822193146\n","Epoch 158/250, Batch 1/3, Loss: 0.012708346359431744\n","Validation Loss after Epoch 158: 0.03347340598702431\n","Epoch 159/250, Batch 1/3, Loss: 0.012596518732607365\n","Validation Loss after Epoch 159: 0.03125244006514549\n","Epoch 160/250, Batch 1/3, Loss: 0.012820599600672722\n","Validation Loss after Epoch 160: 0.0389232411980629\n","Epoch 161/250, Batch 1/3, Loss: 0.012462601065635681\n","Validation Loss after Epoch 161: 0.03514347970485687\n","Epoch 162/250, Batch 1/3, Loss: 0.013999374583363533\n","Validation Loss after Epoch 162: 0.029258601367473602\n","Epoch 163/250, Batch 1/3, Loss: 0.011831016279757023\n","Validation Loss after Epoch 163: 0.03060953877866268\n","Epoch 164/250, Batch 1/3, Loss: 0.013634922914206982\n","Validation Loss after Epoch 164: 0.032292984426021576\n","Epoch 165/250, Batch 1/3, Loss: 0.01414735522121191\n","Validation Loss after Epoch 165: 0.034492120146751404\n","Epoch 166/250, Batch 1/3, Loss: 0.012788555584847927\n","Validation Loss after Epoch 166: 0.030896387994289398\n","Epoch 167/250, Batch 1/3, Loss: 0.012557461857795715\n","Validation Loss after Epoch 167: 0.03188462555408478\n","Epoch 168/250, Batch 1/3, Loss: 0.011480523273348808\n","Validation Loss after Epoch 168: 0.02979908138513565\n","Epoch 169/250, Batch 1/3, Loss: 0.014100717380642891\n","Validation Loss after Epoch 169: 0.03292025625705719\n","Epoch 170/250, Batch 1/3, Loss: 0.012665769085288048\n","Validation Loss after Epoch 170: 0.03746000677347183\n","Epoch 171/250, Batch 1/3, Loss: 0.011308453045785427\n","Validation Loss after Epoch 171: 0.030802961438894272\n","Epoch 172/250, Batch 1/3, Loss: 0.011862012557685375\n","Validation Loss after Epoch 172: 0.032866232097148895\n","Epoch 173/250, Batch 1/3, Loss: 0.011782028712332249\n","Validation Loss after Epoch 173: 0.03174004331231117\n","Epoch 174/250, Batch 1/3, Loss: 0.012078174389898777\n","Validation Loss after Epoch 174: 0.03493841737508774\n","Epoch 175/250, Batch 1/3, Loss: 0.011346332728862762\n","Validation Loss after Epoch 175: 0.03511049970984459\n","Epoch 176/250, Batch 1/3, Loss: 0.011173873208463192\n","Validation Loss after Epoch 176: 0.033298738300800323\n","Epoch 177/250, Batch 1/3, Loss: 0.012468203902244568\n","Validation Loss after Epoch 177: 0.032112378627061844\n","Epoch 178/250, Batch 1/3, Loss: 0.010739556513726711\n","Validation Loss after Epoch 178: 0.0319247767329216\n","Epoch 179/250, Batch 1/3, Loss: 0.010907617397606373\n","Validation Loss after Epoch 179: 0.037580396980047226\n","Epoch 180/250, Batch 1/3, Loss: 0.01441868208348751\n","Validation Loss after Epoch 180: 0.032519012689590454\n","Epoch 181/250, Batch 1/3, Loss: 0.012337933294475079\n","Validation Loss after Epoch 181: 0.034843169152736664\n","Epoch 182/250, Batch 1/3, Loss: 0.010353888384997845\n","Validation Loss after Epoch 182: 0.03593778982758522\n","Epoch 183/250, Batch 1/3, Loss: 0.010509406216442585\n","Validation Loss after Epoch 183: 0.033673886209726334\n","Epoch 184/250, Batch 1/3, Loss: 0.010124677792191505\n","Validation Loss after Epoch 184: 0.03382071852684021\n","Epoch 185/250, Batch 1/3, Loss: 0.011474418453872204\n","Validation Loss after Epoch 185: 0.03320480138063431\n","Epoch 186/250, Batch 1/3, Loss: 0.011750651523470879\n","Validation Loss after Epoch 186: 0.031503815203905106\n","Epoch 187/250, Batch 1/3, Loss: 0.011671170592308044\n","Validation Loss after Epoch 187: 0.03228911757469177\n","Epoch 188/250, Batch 1/3, Loss: 0.010673883371055126\n","Validation Loss after Epoch 188: 0.038253024220466614\n","Epoch 189/250, Batch 1/3, Loss: 0.010235805064439774\n","Validation Loss after Epoch 189: 0.03194303810596466\n","Epoch 190/250, Batch 1/3, Loss: 0.01055187452584505\n","Validation Loss after Epoch 190: 0.037221506237983704\n","Epoch 191/250, Batch 1/3, Loss: 0.010532409884035587\n","Validation Loss after Epoch 191: 0.03790510445833206\n","Epoch 192/250, Batch 1/3, Loss: 0.009509137831628323\n","Validation Loss after Epoch 192: 0.03679734468460083\n","Epoch 193/250, Batch 1/3, Loss: 0.009778667241334915\n","Validation Loss after Epoch 193: 0.03475484997034073\n","Epoch 194/250, Batch 1/3, Loss: 0.009937047027051449\n","Validation Loss after Epoch 194: 0.03532712161540985\n","Epoch 195/250, Batch 1/3, Loss: 0.010322315618395805\n","Validation Loss after Epoch 195: 0.03922294080257416\n","Epoch 196/250, Batch 1/3, Loss: 0.010097771883010864\n","Validation Loss after Epoch 196: 0.03611554205417633\n","Epoch 197/250, Batch 1/3, Loss: 0.009552078321576118\n","Validation Loss after Epoch 197: 0.04238228127360344\n","Epoch 198/250, Batch 1/3, Loss: 0.011254999786615372\n","Validation Loss after Epoch 198: 0.032328370958566666\n","Epoch 199/250, Batch 1/3, Loss: 0.00979468785226345\n","Validation Loss after Epoch 199: 0.0371692031621933\n","Epoch 200/250, Batch 1/3, Loss: 0.013417236506938934\n","Validation Loss after Epoch 200: 0.04036073014140129\n","Epoch 201/250, Batch 1/3, Loss: 0.013016690500080585\n","Validation Loss after Epoch 201: 0.042986758053302765\n","Epoch 202/250, Batch 1/3, Loss: 0.014511761255562305\n","Validation Loss after Epoch 202: 0.03658132255077362\n","Epoch 203/250, Batch 1/3, Loss: 0.00924907810986042\n","Validation Loss after Epoch 203: 0.03681815043091774\n","Epoch 204/250, Batch 1/3, Loss: 0.009329866617918015\n","Validation Loss after Epoch 204: 0.036393214017152786\n","Epoch 205/250, Batch 1/3, Loss: 0.010386127047240734\n","Validation Loss after Epoch 205: 0.037390246987342834\n","Epoch 206/250, Batch 1/3, Loss: 0.009239478036761284\n","Validation Loss after Epoch 206: 0.03797483071684837\n","Epoch 207/250, Batch 1/3, Loss: 0.0099069494754076\n","Validation Loss after Epoch 207: 0.03475561365485191\n","Epoch 208/250, Batch 1/3, Loss: 0.009457129053771496\n","Validation Loss after Epoch 208: 0.03518657758831978\n","Epoch 209/250, Batch 1/3, Loss: 0.010219276882708073\n","Validation Loss after Epoch 209: 0.03357530012726784\n","Epoch 210/250, Batch 1/3, Loss: 0.008806460537016392\n","Validation Loss after Epoch 210: 0.03571413457393646\n","Epoch 211/250, Batch 1/3, Loss: 0.00848534144461155\n","Validation Loss after Epoch 211: 0.04246525466442108\n","Epoch 212/250, Batch 1/3, Loss: 0.009544973261654377\n","Validation Loss after Epoch 212: 0.0342218242585659\n","Epoch 213/250, Batch 1/3, Loss: 0.010374127887189388\n","Validation Loss after Epoch 213: 0.0439484566450119\n","Epoch 214/250, Batch 1/3, Loss: 0.00929251778870821\n","Validation Loss after Epoch 214: 0.03212757781147957\n","Epoch 215/250, Batch 1/3, Loss: 0.01000287476927042\n","Validation Loss after Epoch 215: 0.03664384037256241\n","Epoch 216/250, Batch 1/3, Loss: 0.007786610629409552\n","Validation Loss after Epoch 216: 0.0485713928937912\n","Epoch 217/250, Batch 1/3, Loss: 0.008492406457662582\n","Validation Loss after Epoch 217: 0.033467743545770645\n","Epoch 218/250, Batch 1/3, Loss: 0.009106293320655823\n","Validation Loss after Epoch 218: 0.04460412263870239\n","Epoch 219/250, Batch 1/3, Loss: 0.009892374277114868\n","Validation Loss after Epoch 219: 0.035841960459947586\n","Epoch 220/250, Batch 1/3, Loss: 0.009478526189923286\n","Validation Loss after Epoch 220: 0.03318948298692703\n","Epoch 221/250, Batch 1/3, Loss: 0.008344941772520542\n","Validation Loss after Epoch 221: 0.03384484723210335\n","Epoch 222/250, Batch 1/3, Loss: 0.008432657457888126\n","Validation Loss after Epoch 222: 0.034491755068302155\n","Epoch 223/250, Batch 1/3, Loss: 0.009485626593232155\n","Validation Loss after Epoch 223: 0.039905112236738205\n","Epoch 224/250, Batch 1/3, Loss: 0.007614608854055405\n","Validation Loss after Epoch 224: 0.03463255241513252\n","Epoch 225/250, Batch 1/3, Loss: 0.007288663182407618\n","Validation Loss after Epoch 225: 0.037923090159893036\n","Epoch 226/250, Batch 1/3, Loss: 0.007897671312093735\n","Validation Loss after Epoch 226: 0.04589023068547249\n","Epoch 227/250, Batch 1/3, Loss: 0.00771833211183548\n","Validation Loss after Epoch 227: 0.040187373757362366\n","Epoch 228/250, Batch 1/3, Loss: 0.006652636453509331\n","Validation Loss after Epoch 228: 0.03691621497273445\n","Epoch 229/250, Batch 1/3, Loss: 0.007301608100533485\n","Validation Loss after Epoch 229: 0.037291668355464935\n","Epoch 230/250, Batch 1/3, Loss: 0.008379480801522732\n","Validation Loss after Epoch 230: 0.03463862091302872\n","Epoch 231/250, Batch 1/3, Loss: 0.007847675122320652\n","Validation Loss after Epoch 231: 0.037480782717466354\n","Epoch 232/250, Batch 1/3, Loss: 0.009646674618124962\n","Validation Loss after Epoch 232: 0.03840908035635948\n","Epoch 233/250, Batch 1/3, Loss: 0.007557298988103867\n","Validation Loss after Epoch 233: 0.038017723709344864\n","Epoch 234/250, Batch 1/3, Loss: 0.008868920616805553\n","Validation Loss after Epoch 234: 0.037732526659965515\n","Epoch 235/250, Batch 1/3, Loss: 0.008410581387579441\n","Validation Loss after Epoch 235: 0.03488333150744438\n","Epoch 236/250, Batch 1/3, Loss: 0.0079783471301198\n","Validation Loss after Epoch 236: 0.032976455986499786\n","Epoch 237/250, Batch 1/3, Loss: 0.006991599686443806\n","Validation Loss after Epoch 237: 0.036669034510850906\n","Epoch 238/250, Batch 1/3, Loss: 0.007591957226395607\n","Validation Loss after Epoch 238: 0.032195109874010086\n","Epoch 239/250, Batch 1/3, Loss: 0.00730841513723135\n","Validation Loss after Epoch 239: 0.04193705692887306\n","Epoch 240/250, Batch 1/3, Loss: 0.008018597960472107\n","Validation Loss after Epoch 240: 0.034331101924180984\n","Epoch 241/250, Batch 1/3, Loss: 0.007170261815190315\n","Validation Loss after Epoch 241: 0.04550610110163689\n","Epoch 242/250, Batch 1/3, Loss: 0.006219933740794659\n","Validation Loss after Epoch 242: 0.03829143941402435\n","Epoch 243/250, Batch 1/3, Loss: 0.006072371266782284\n","Validation Loss after Epoch 243: 0.03729819506406784\n","Epoch 244/250, Batch 1/3, Loss: 0.005917430855333805\n","Validation Loss after Epoch 244: 0.03824450448155403\n","Epoch 245/250, Batch 1/3, Loss: 0.006346983835101128\n","Validation Loss after Epoch 245: 0.039058566093444824\n","Epoch 246/250, Batch 1/3, Loss: 0.006110619753599167\n","Validation Loss after Epoch 246: 0.04687787592411041\n","Epoch 247/250, Batch 1/3, Loss: 0.006374458782374859\n","Validation Loss after Epoch 247: 0.041439544409513474\n","Epoch 248/250, Batch 1/3, Loss: 0.007139298599213362\n","Validation Loss after Epoch 248: 0.04799795150756836\n","Epoch 249/250, Batch 1/3, Loss: 0.008499194867908955\n","Validation Loss after Epoch 249: 0.033985793590545654\n","Epoch 250/250, Batch 1/3, Loss: 0.007290273904800415\n","Validation Loss after Epoch 250: 0.04071857035160065\n","Subset size 125 - Test Loss: 0.0381, Test Accuracy: 98.81%, Average Dice Score: 0.9862\n","\n","Training on subset size: 250\n","Epoch 1/250, Batch 1/6, Loss: 1.1149605512619019\n","Validation Loss after Epoch 1: 1.0715465545654297\n","Epoch 2/250, Batch 1/6, Loss: 0.21520322561264038\n","Validation Loss after Epoch 2: 1.2681629061698914\n","Epoch 3/250, Batch 1/6, Loss: 0.15517881512641907\n","Validation Loss after Epoch 3: 1.3987568616867065\n","Epoch 4/250, Batch 1/6, Loss: 0.12159667909145355\n","Validation Loss after Epoch 4: 1.0726292133331299\n","Epoch 5/250, Batch 1/6, Loss: 0.10502980649471283\n","Validation Loss after Epoch 5: 1.0730738639831543\n","Epoch 6/250, Batch 1/6, Loss: 0.08447765558958054\n","Validation Loss after Epoch 6: 0.9071232676506042\n","Epoch 7/250, Batch 1/6, Loss: 0.07463525235652924\n","Validation Loss after Epoch 7: 0.8609697222709656\n","Epoch 8/250, Batch 1/6, Loss: 0.06462130695581436\n","Validation Loss after Epoch 8: 0.7464043200016022\n","Epoch 9/250, Batch 1/6, Loss: 0.06422410160303116\n","Validation Loss after Epoch 9: 0.5456942021846771\n","Epoch 10/250, Batch 1/6, Loss: 0.055510032922029495\n","Validation Loss after Epoch 10: 0.1123415119946003\n","Epoch 11/250, Batch 1/6, Loss: 0.057930368930101395\n","Validation Loss after Epoch 11: 0.10832565277814865\n","Epoch 12/250, Batch 1/6, Loss: 0.04754020273685455\n","Validation Loss after Epoch 12: 0.05077245086431503\n","Epoch 13/250, Batch 1/6, Loss: 0.04851273074746132\n","Validation Loss after Epoch 13: 0.05927312560379505\n","Epoch 14/250, Batch 1/6, Loss: 0.04630870744585991\n","Validation Loss after Epoch 14: 0.04195966199040413\n","Epoch 15/250, Batch 1/6, Loss: 0.049663230776786804\n","Validation Loss after Epoch 15: 0.041244346648454666\n","Epoch 16/250, Batch 1/6, Loss: 0.04462778940796852\n","Validation Loss after Epoch 16: 0.038932079449296\n","Epoch 17/250, Batch 1/6, Loss: 0.04118543863296509\n","Validation Loss after Epoch 17: 0.03879261948168278\n","Epoch 18/250, Batch 1/6, Loss: 0.03778946027159691\n","Validation Loss after Epoch 18: 0.03828272223472595\n","Epoch 19/250, Batch 1/6, Loss: 0.03663768246769905\n","Validation Loss after Epoch 19: 0.037288038060069084\n","Epoch 20/250, Batch 1/6, Loss: 0.03962089866399765\n","Validation Loss after Epoch 20: 0.03714416176080704\n","Epoch 21/250, Batch 1/6, Loss: 0.03548763692378998\n","Validation Loss after Epoch 21: 0.036043211817741394\n","Epoch 22/250, Batch 1/6, Loss: 0.041324812918901443\n","Validation Loss after Epoch 22: 0.03730643540620804\n","Epoch 23/250, Batch 1/6, Loss: 0.038589853793382645\n","Validation Loss after Epoch 23: 0.03605593182146549\n","Epoch 24/250, Batch 1/6, Loss: 0.031224988400936127\n","Validation Loss after Epoch 24: 0.03424743190407753\n","Epoch 25/250, Batch 1/6, Loss: 0.03309299051761627\n","Validation Loss after Epoch 25: 0.031272606924176216\n","Epoch 26/250, Batch 1/6, Loss: 0.03213737905025482\n","Validation Loss after Epoch 26: 0.03431563451886177\n","Epoch 27/250, Batch 1/6, Loss: 0.03266090899705887\n","Validation Loss after Epoch 27: 0.030440804548561573\n","Epoch 28/250, Batch 1/6, Loss: 0.03192498907446861\n","Validation Loss after Epoch 28: 0.03392942622303963\n","Epoch 29/250, Batch 1/6, Loss: 0.030354324728250504\n","Validation Loss after Epoch 29: 0.03235544729977846\n","Epoch 30/250, Batch 1/6, Loss: 0.030011704191565514\n","Validation Loss after Epoch 30: 0.03197356499731541\n","Epoch 31/250, Batch 1/6, Loss: 0.03070557489991188\n","Validation Loss after Epoch 31: 0.029728186316788197\n","Epoch 32/250, Batch 1/6, Loss: 0.030074575915932655\n","Validation Loss after Epoch 32: 0.027635464444756508\n","Epoch 33/250, Batch 1/6, Loss: 0.0331808403134346\n","Validation Loss after Epoch 33: 0.03384030796587467\n","Epoch 34/250, Batch 1/6, Loss: 0.030244771391153336\n","Validation Loss after Epoch 34: 0.02938876859843731\n","Epoch 35/250, Batch 1/6, Loss: 0.02996446006000042\n","Validation Loss after Epoch 35: 0.028973063454031944\n","Epoch 36/250, Batch 1/6, Loss: 0.032147470861673355\n","Validation Loss after Epoch 36: 0.028978269547224045\n","Epoch 37/250, Batch 1/6, Loss: 0.028135962784290314\n","Validation Loss after Epoch 37: 0.03198303561657667\n","Epoch 38/250, Batch 1/6, Loss: 0.028964435681700706\n","Validation Loss after Epoch 38: 0.026633252389729023\n","Epoch 39/250, Batch 1/6, Loss: 0.02823549322783947\n","Validation Loss after Epoch 39: 0.027043838053941727\n","Epoch 40/250, Batch 1/6, Loss: 0.0274079367518425\n","Validation Loss after Epoch 40: 0.02692477125674486\n","Epoch 41/250, Batch 1/6, Loss: 0.028478199616074562\n","Validation Loss after Epoch 41: 0.027278045192360878\n","Epoch 42/250, Batch 1/6, Loss: 0.025180302560329437\n","Validation Loss after Epoch 42: 0.032364629209041595\n","Epoch 43/250, Batch 1/6, Loss: 0.024993151426315308\n","Validation Loss after Epoch 43: 0.025622825138270855\n","Epoch 44/250, Batch 1/6, Loss: 0.02940140664577484\n","Validation Loss after Epoch 44: 0.02750941552221775\n","Epoch 45/250, Batch 1/6, Loss: 0.025498487055301666\n","Validation Loss after Epoch 45: 0.02683559339493513\n","Epoch 46/250, Batch 1/6, Loss: 0.027900002896785736\n","Validation Loss after Epoch 46: 0.025698532350361347\n","Epoch 47/250, Batch 1/6, Loss: 0.030423352494835854\n","Validation Loss after Epoch 47: 0.028302105143666267\n","Epoch 48/250, Batch 1/6, Loss: 0.0247140321880579\n","Validation Loss after Epoch 48: 0.025840695947408676\n","Epoch 49/250, Batch 1/6, Loss: 0.026553185656666756\n","Validation Loss after Epoch 49: 0.028309102170169353\n","Epoch 50/250, Batch 1/6, Loss: 0.031862061470746994\n","Validation Loss after Epoch 50: 0.030690201558172703\n","Epoch 51/250, Batch 1/6, Loss: 0.023741189390420914\n","Validation Loss after Epoch 51: 0.024735908024013042\n","Epoch 52/250, Batch 1/6, Loss: 0.02533418871462345\n","Validation Loss after Epoch 52: 0.030769025906920433\n","Epoch 53/250, Batch 1/6, Loss: 0.02974773198366165\n","Validation Loss after Epoch 53: 0.025803474709391594\n","Epoch 54/250, Batch 1/6, Loss: 0.024213377386331558\n","Validation Loss after Epoch 54: 0.024879897013306618\n","Epoch 55/250, Batch 1/6, Loss: 0.022958308458328247\n","Validation Loss after Epoch 55: 0.027784109115600586\n","Epoch 56/250, Batch 1/6, Loss: 0.028541991487145424\n","Validation Loss after Epoch 56: 0.025051585398614407\n","Epoch 57/250, Batch 1/6, Loss: 0.023054225370287895\n","Validation Loss after Epoch 57: 0.025915595702826977\n","Epoch 58/250, Batch 1/6, Loss: 0.02337946556508541\n","Validation Loss after Epoch 58: 0.026774168945848942\n","Epoch 59/250, Batch 1/6, Loss: 0.02462424337863922\n","Validation Loss after Epoch 59: 0.03322404809296131\n","Epoch 60/250, Batch 1/6, Loss: 0.028330737724900246\n","Validation Loss after Epoch 60: 0.023747441358864307\n","Epoch 61/250, Batch 1/6, Loss: 0.02467508055269718\n","Validation Loss after Epoch 61: 0.02397711854428053\n","Epoch 62/250, Batch 1/6, Loss: 0.027317943051457405\n","Validation Loss after Epoch 62: 0.028702208772301674\n","Epoch 63/250, Batch 1/6, Loss: 0.02568022720515728\n","Validation Loss after Epoch 63: 0.025383468717336655\n","Epoch 64/250, Batch 1/6, Loss: 0.026753678917884827\n","Validation Loss after Epoch 64: 0.03862439654767513\n","Epoch 65/250, Batch 1/6, Loss: 0.024002602323889732\n","Validation Loss after Epoch 65: 0.024476969614624977\n","Epoch 66/250, Batch 1/6, Loss: 0.023352909833192825\n","Validation Loss after Epoch 66: 0.024815255776047707\n","Epoch 67/250, Batch 1/6, Loss: 0.023942237719893456\n","Validation Loss after Epoch 67: 0.03554105572402477\n","Epoch 68/250, Batch 1/6, Loss: 0.02131679281592369\n","Validation Loss after Epoch 68: 0.026379195973277092\n","Epoch 69/250, Batch 1/6, Loss: 0.022801190614700317\n","Validation Loss after Epoch 69: 0.0238970834761858\n","Epoch 70/250, Batch 1/6, Loss: 0.023182345554232597\n","Validation Loss after Epoch 70: 0.034027352929115295\n","Epoch 71/250, Batch 1/6, Loss: 0.0258298609405756\n","Validation Loss after Epoch 71: 0.02329216618090868\n","Epoch 72/250, Batch 1/6, Loss: 0.02050192281603813\n","Validation Loss after Epoch 72: 0.025062518194317818\n","Epoch 73/250, Batch 1/6, Loss: 0.021105794236063957\n","Validation Loss after Epoch 73: 0.02337762899696827\n","Epoch 74/250, Batch 1/6, Loss: 0.019678564742207527\n","Validation Loss after Epoch 74: 0.022842085920274258\n","Epoch 75/250, Batch 1/6, Loss: 0.02179647795855999\n","Validation Loss after Epoch 75: 0.024194627068936825\n","Epoch 76/250, Batch 1/6, Loss: 0.020977137610316277\n","Validation Loss after Epoch 76: 0.023883781395852566\n","Epoch 77/250, Batch 1/6, Loss: 0.021855533123016357\n","Validation Loss after Epoch 77: 0.02454198617488146\n","Epoch 78/250, Batch 1/6, Loss: 0.02050049789249897\n","Validation Loss after Epoch 78: 0.023169984109699726\n","Epoch 79/250, Batch 1/6, Loss: 0.023064229637384415\n","Validation Loss after Epoch 79: 0.023947956040501595\n","Epoch 80/250, Batch 1/6, Loss: 0.020365390926599503\n","Validation Loss after Epoch 80: 0.026237791404128075\n","Epoch 81/250, Batch 1/6, Loss: 0.01774287410080433\n","Validation Loss after Epoch 81: 0.023050423711538315\n","Epoch 82/250, Batch 1/6, Loss: 0.021979833021759987\n","Validation Loss after Epoch 82: 0.023726414889097214\n","Epoch 83/250, Batch 1/6, Loss: 0.019994042813777924\n","Validation Loss after Epoch 83: 0.024458831176161766\n","Epoch 84/250, Batch 1/6, Loss: 0.02217145636677742\n","Validation Loss after Epoch 84: 0.024896299466490746\n","Epoch 85/250, Batch 1/6, Loss: 0.020737452432513237\n","Validation Loss after Epoch 85: 0.02396536897867918\n","Epoch 86/250, Batch 1/6, Loss: 0.02092892676591873\n","Validation Loss after Epoch 86: 0.027645871974527836\n","Epoch 87/250, Batch 1/6, Loss: 0.018316347151994705\n","Validation Loss after Epoch 87: 0.023564640432596207\n","Epoch 88/250, Batch 1/6, Loss: 0.01863611675798893\n","Validation Loss after Epoch 88: 0.02485109493136406\n","Epoch 89/250, Batch 1/6, Loss: 0.017702018842101097\n","Validation Loss after Epoch 89: 0.026457471773028374\n","Epoch 90/250, Batch 1/6, Loss: 0.02686798945069313\n","Validation Loss after Epoch 90: 0.02213280461728573\n","Epoch 91/250, Batch 1/6, Loss: 0.018472736701369286\n","Validation Loss after Epoch 91: 0.024336155503988266\n","Epoch 92/250, Batch 1/6, Loss: 0.021203024312853813\n","Validation Loss after Epoch 92: 0.029741160571575165\n","Epoch 93/250, Batch 1/6, Loss: 0.018203431740403175\n","Validation Loss after Epoch 93: 0.02243001013994217\n","Epoch 94/250, Batch 1/6, Loss: 0.019000457599759102\n","Validation Loss after Epoch 94: 0.024022171273827553\n","Epoch 95/250, Batch 1/6, Loss: 0.020118115469813347\n","Validation Loss after Epoch 95: 0.022051668725907803\n","Epoch 96/250, Batch 1/6, Loss: 0.01733267679810524\n","Validation Loss after Epoch 96: 0.025216990150511265\n","Epoch 97/250, Batch 1/6, Loss: 0.01633026823401451\n","Validation Loss after Epoch 97: 0.022529616951942444\n","Epoch 98/250, Batch 1/6, Loss: 0.021148400381207466\n","Validation Loss after Epoch 98: 0.04993472993373871\n","Epoch 99/250, Batch 1/6, Loss: 0.022900152951478958\n","Validation Loss after Epoch 99: 0.022492436692118645\n","Epoch 100/250, Batch 1/6, Loss: 0.019848063588142395\n","Validation Loss after Epoch 100: 0.030955706723034382\n","Epoch 101/250, Batch 1/6, Loss: 0.021961191669106483\n","Validation Loss after Epoch 101: 0.038633573800325394\n","Epoch 102/250, Batch 1/6, Loss: 0.02046308107674122\n","Validation Loss after Epoch 102: 0.023005178198218346\n","Epoch 103/250, Batch 1/6, Loss: 0.020891718566417694\n","Validation Loss after Epoch 103: 0.022358360700309277\n","Epoch 104/250, Batch 1/6, Loss: 0.018361596390604973\n","Validation Loss after Epoch 104: 0.028895012103021145\n","Epoch 105/250, Batch 1/6, Loss: 0.018937017768621445\n","Validation Loss after Epoch 105: 0.02508058398962021\n","Epoch 106/250, Batch 1/6, Loss: 0.01891375705599785\n","Validation Loss after Epoch 106: 0.029039692133665085\n","Epoch 107/250, Batch 1/6, Loss: 0.01928659901022911\n","Validation Loss after Epoch 107: 0.02519901469349861\n","Epoch 108/250, Batch 1/6, Loss: 0.016729842871427536\n","Validation Loss after Epoch 108: 0.02262485958635807\n","Epoch 109/250, Batch 1/6, Loss: 0.017596639692783356\n","Validation Loss after Epoch 109: 0.023486807011067867\n","Epoch 110/250, Batch 1/6, Loss: 0.01773371361196041\n","Validation Loss after Epoch 110: 0.022680597379803658\n","Epoch 111/250, Batch 1/6, Loss: 0.019527610391378403\n","Validation Loss after Epoch 111: 0.021406140178442\n","Epoch 112/250, Batch 1/6, Loss: 0.0160776749253273\n","Validation Loss after Epoch 112: 0.02096298150718212\n","Epoch 113/250, Batch 1/6, Loss: 0.015984872356057167\n","Validation Loss after Epoch 113: 0.028373681008815765\n","Epoch 114/250, Batch 1/6, Loss: 0.01793200895190239\n","Validation Loss after Epoch 114: 0.02290105726569891\n","Epoch 115/250, Batch 1/6, Loss: 0.015977220609784126\n","Validation Loss after Epoch 115: 0.024530533701181412\n","Epoch 116/250, Batch 1/6, Loss: 0.014862553216516972\n","Validation Loss after Epoch 116: 0.02231660485267639\n","Epoch 117/250, Batch 1/6, Loss: 0.01547822542488575\n","Validation Loss after Epoch 117: 0.02371897269040346\n","Epoch 118/250, Batch 1/6, Loss: 0.019574979320168495\n","Validation Loss after Epoch 118: 0.022407691925764084\n","Epoch 119/250, Batch 1/6, Loss: 0.015248297713696957\n","Validation Loss after Epoch 119: 0.02474942896515131\n","Epoch 120/250, Batch 1/6, Loss: 0.020099347457289696\n","Validation Loss after Epoch 120: 0.02441279962658882\n","Epoch 121/250, Batch 1/6, Loss: 0.01808718405663967\n","Validation Loss after Epoch 121: 0.025187085382640362\n","Epoch 122/250, Batch 1/6, Loss: 0.016631457954645157\n","Validation Loss after Epoch 122: 0.022529879584908485\n","Epoch 123/250, Batch 1/6, Loss: 0.016985606402158737\n","Validation Loss after Epoch 123: 0.022792820818722248\n","Epoch 124/250, Batch 1/6, Loss: 0.01594190113246441\n","Validation Loss after Epoch 124: 0.02310145553201437\n","Epoch 125/250, Batch 1/6, Loss: 0.013623569160699844\n","Validation Loss after Epoch 125: 0.025003078393638134\n","Epoch 126/250, Batch 1/6, Loss: 0.017210783436894417\n","Validation Loss after Epoch 126: 0.024838384240865707\n","Epoch 127/250, Batch 1/6, Loss: 0.01782527193427086\n","Validation Loss after Epoch 127: 0.02339490782469511\n","Epoch 128/250, Batch 1/6, Loss: 0.016273148357868195\n","Validation Loss after Epoch 128: 0.02361598238348961\n","Epoch 129/250, Batch 1/6, Loss: 0.015030390582978725\n","Validation Loss after Epoch 129: 0.022184171713888645\n","Epoch 130/250, Batch 1/6, Loss: 0.014382444322109222\n","Validation Loss after Epoch 130: 0.0214264253154397\n","Epoch 131/250, Batch 1/6, Loss: 0.014353705570101738\n","Validation Loss after Epoch 131: 0.023942742496728897\n","Epoch 132/250, Batch 1/6, Loss: 0.014260055497288704\n","Validation Loss after Epoch 132: 0.022113997489213943\n","Epoch 133/250, Batch 1/6, Loss: 0.013760946691036224\n","Validation Loss after Epoch 133: 0.025122021324932575\n","Epoch 134/250, Batch 1/6, Loss: 0.014746866188943386\n","Validation Loss after Epoch 134: 0.02320277690887451\n","Epoch 135/250, Batch 1/6, Loss: 0.013879708014428616\n","Validation Loss after Epoch 135: 0.023578666150569916\n","Epoch 136/250, Batch 1/6, Loss: 0.013946830295026302\n","Validation Loss after Epoch 136: 0.02575166616588831\n","Epoch 137/250, Batch 1/6, Loss: 0.012995456345379353\n","Validation Loss after Epoch 137: 0.025895848870277405\n","Epoch 138/250, Batch 1/6, Loss: 0.014025055803358555\n","Validation Loss after Epoch 138: 0.02607213892042637\n","Epoch 139/250, Batch 1/6, Loss: 0.012808868661522865\n","Validation Loss after Epoch 139: 0.02550440188497305\n","Epoch 140/250, Batch 1/6, Loss: 0.017877677455544472\n","Validation Loss after Epoch 140: 0.02882378175854683\n","Epoch 141/250, Batch 1/6, Loss: 0.0134133854880929\n","Validation Loss after Epoch 141: 0.03056331817060709\n","Epoch 142/250, Batch 1/6, Loss: 0.016462473198771477\n","Validation Loss after Epoch 142: 0.024619524367153645\n","Epoch 143/250, Batch 1/6, Loss: 0.013301027938723564\n","Validation Loss after Epoch 143: 0.026623559184372425\n","Epoch 144/250, Batch 1/6, Loss: 0.013750169426202774\n","Validation Loss after Epoch 144: 0.023982876911759377\n","Epoch 145/250, Batch 1/6, Loss: 0.013316222466528416\n","Validation Loss after Epoch 145: 0.026269295252859592\n","Epoch 146/250, Batch 1/6, Loss: 0.012522455304861069\n","Validation Loss after Epoch 146: 0.02702138666063547\n","Epoch 147/250, Batch 1/6, Loss: 0.014532623812556267\n","Validation Loss after Epoch 147: 0.026305600069463253\n","Epoch 148/250, Batch 1/6, Loss: 0.01188294030725956\n","Validation Loss after Epoch 148: 0.028705655597150326\n","Epoch 149/250, Batch 1/6, Loss: 0.014202268794178963\n","Validation Loss after Epoch 149: 0.031698462553322315\n","Epoch 150/250, Batch 1/6, Loss: 0.017923448234796524\n","Validation Loss after Epoch 150: 0.028431270271539688\n","Epoch 151/250, Batch 1/6, Loss: 0.014185156673192978\n","Validation Loss after Epoch 151: 0.023896622471511364\n","Epoch 152/250, Batch 1/6, Loss: 0.01413111574947834\n","Validation Loss after Epoch 152: 0.02402876690030098\n","Epoch 153/250, Batch 1/6, Loss: 0.013941318728029728\n","Validation Loss after Epoch 153: 0.022644036449491978\n","Epoch 154/250, Batch 1/6, Loss: 0.016086945310235023\n","Validation Loss after Epoch 154: 0.02267929632216692\n","Epoch 155/250, Batch 1/6, Loss: 0.01433529518544674\n","Validation Loss after Epoch 155: 0.02689938060939312\n","Epoch 156/250, Batch 1/6, Loss: 0.012896108441054821\n","Validation Loss after Epoch 156: 0.025568443350493908\n","Epoch 157/250, Batch 1/6, Loss: 0.013252495788037777\n","Validation Loss after Epoch 157: 0.023417879827320576\n","Epoch 158/250, Batch 1/6, Loss: 0.012151002883911133\n","Validation Loss after Epoch 158: 0.02805254515260458\n","Epoch 159/250, Batch 1/6, Loss: 0.01201297901570797\n","Validation Loss after Epoch 159: 0.024673299863934517\n","Epoch 160/250, Batch 1/6, Loss: 0.0112453019246459\n","Validation Loss after Epoch 160: 0.024813679978251457\n","Epoch 161/250, Batch 1/6, Loss: 0.013189930468797684\n","Validation Loss after Epoch 161: 0.024146823212504387\n","Epoch 162/250, Batch 1/6, Loss: 0.01166461780667305\n","Validation Loss after Epoch 162: 0.02504252176731825\n","Epoch 163/250, Batch 1/6, Loss: 0.01130569539964199\n","Validation Loss after Epoch 163: 0.025292490608990192\n","Epoch 164/250, Batch 1/6, Loss: 0.010409999638795853\n","Validation Loss after Epoch 164: 0.02324804849922657\n","Epoch 165/250, Batch 1/6, Loss: 0.016890961676836014\n","Validation Loss after Epoch 165: 0.023235773667693138\n","Epoch 166/250, Batch 1/6, Loss: 0.014617953449487686\n","Validation Loss after Epoch 166: 0.0235243896022439\n","Epoch 167/250, Batch 1/6, Loss: 0.011169493198394775\n","Validation Loss after Epoch 167: 0.024304982274770737\n","Epoch 168/250, Batch 1/6, Loss: 0.012032567523419857\n","Validation Loss after Epoch 168: 0.024928838945925236\n","Epoch 169/250, Batch 1/6, Loss: 0.010339070111513138\n","Validation Loss after Epoch 169: 0.026148289442062378\n","Epoch 170/250, Batch 1/6, Loss: 0.010685750283300877\n","Validation Loss after Epoch 170: 0.02525189984589815\n","Epoch 171/250, Batch 1/6, Loss: 0.010338843800127506\n","Validation Loss after Epoch 171: 0.025652490556240082\n","Epoch 172/250, Batch 1/6, Loss: 0.015008533373475075\n","Validation Loss after Epoch 172: 0.02451737131923437\n","Epoch 173/250, Batch 1/6, Loss: 0.011198987253010273\n","Validation Loss after Epoch 173: 0.02490554004907608\n","Epoch 174/250, Batch 1/6, Loss: 0.010474915616214275\n","Validation Loss after Epoch 174: 0.026024146005511284\n","Epoch 175/250, Batch 1/6, Loss: 0.012251179665327072\n","Validation Loss after Epoch 175: 0.024022096768021584\n","Epoch 176/250, Batch 1/6, Loss: 0.013367971405386925\n","Validation Loss after Epoch 176: 0.034612501971423626\n","Epoch 177/250, Batch 1/6, Loss: 0.011798725463449955\n","Validation Loss after Epoch 177: 0.02380401361733675\n","Epoch 178/250, Batch 1/6, Loss: 0.01198087353259325\n","Validation Loss after Epoch 178: 0.02502754796296358\n","Epoch 179/250, Batch 1/6, Loss: 0.014374461956322193\n","Validation Loss after Epoch 179: 0.04055081680417061\n","Epoch 180/250, Batch 1/6, Loss: 0.02019120380282402\n","Validation Loss after Epoch 180: 0.034836044535040855\n","Epoch 181/250, Batch 1/6, Loss: 0.018951797857880592\n","Validation Loss after Epoch 181: 0.029041402973234653\n","Epoch 182/250, Batch 1/6, Loss: 0.013410883955657482\n","Validation Loss after Epoch 182: 0.02674899995326996\n","Epoch 183/250, Batch 1/6, Loss: 0.01203758642077446\n","Validation Loss after Epoch 183: 0.03534556366503239\n","Epoch 184/250, Batch 1/6, Loss: 0.010574768297374249\n","Validation Loss after Epoch 184: 0.024236164055764675\n","Epoch 185/250, Batch 1/6, Loss: 0.012161200866103172\n","Validation Loss after Epoch 185: 0.024240467697381973\n","Epoch 186/250, Batch 1/6, Loss: 0.011563913896679878\n","Validation Loss after Epoch 186: 0.02691672369837761\n","Epoch 187/250, Batch 1/6, Loss: 0.0098882419988513\n","Validation Loss after Epoch 187: 0.03183474391698837\n","Epoch 188/250, Batch 1/6, Loss: 0.013064438477158546\n","Validation Loss after Epoch 188: 0.025227111764252186\n","Epoch 189/250, Batch 1/6, Loss: 0.010548453778028488\n","Validation Loss after Epoch 189: 0.026073364540934563\n","Epoch 190/250, Batch 1/6, Loss: 0.010244914330542088\n","Validation Loss after Epoch 190: 0.03445987030863762\n","Epoch 191/250, Batch 1/6, Loss: 0.01314105186611414\n","Validation Loss after Epoch 191: 0.02528536319732666\n","Epoch 192/250, Batch 1/6, Loss: 0.009395064786076546\n","Validation Loss after Epoch 192: 0.026488938368856907\n","Epoch 193/250, Batch 1/6, Loss: 0.009900583885610104\n","Validation Loss after Epoch 193: 0.02543679904192686\n","Epoch 194/250, Batch 1/6, Loss: 0.010317684151232243\n","Validation Loss after Epoch 194: 0.027829100377857685\n","Epoch 195/250, Batch 1/6, Loss: 0.009243800304830074\n","Validation Loss after Epoch 195: 0.02808685600757599\n","Epoch 196/250, Batch 1/6, Loss: 0.008701644837856293\n","Validation Loss after Epoch 196: 0.03289263043552637\n","Epoch 197/250, Batch 1/6, Loss: 0.009219417348504066\n","Validation Loss after Epoch 197: 0.029293999075889587\n","Epoch 198/250, Batch 1/6, Loss: 0.009625783190131187\n","Validation Loss after Epoch 198: 0.02595063205808401\n","Epoch 199/250, Batch 1/6, Loss: 0.0096031678840518\n","Validation Loss after Epoch 199: 0.024531603790819645\n","Epoch 200/250, Batch 1/6, Loss: 0.009098086506128311\n","Validation Loss after Epoch 200: 0.025923727080225945\n","Epoch 201/250, Batch 1/6, Loss: 0.009168541058897972\n","Validation Loss after Epoch 201: 0.02923370711505413\n","Epoch 202/250, Batch 1/6, Loss: 0.009557302109897137\n","Validation Loss after Epoch 202: 0.02805875614285469\n","Epoch 203/250, Batch 1/6, Loss: 0.008719559758901596\n","Validation Loss after Epoch 203: 0.029125616885721684\n","Epoch 204/250, Batch 1/6, Loss: 0.008552019484341145\n","Validation Loss after Epoch 204: 0.02948296256363392\n","Epoch 205/250, Batch 1/6, Loss: 0.012772868387401104\n","Validation Loss after Epoch 205: 0.04699535481631756\n","Epoch 206/250, Batch 1/6, Loss: 0.011336919851601124\n","Validation Loss after Epoch 206: 0.026482900604605675\n","Epoch 207/250, Batch 1/6, Loss: 0.01988375559449196\n","Validation Loss after Epoch 207: 0.02920931950211525\n","Epoch 208/250, Batch 1/6, Loss: 0.016159238293766975\n","Validation Loss after Epoch 208: 0.03996963053941727\n","Epoch 209/250, Batch 1/6, Loss: 0.015754271298646927\n","Validation Loss after Epoch 209: 0.02525702863931656\n","Epoch 210/250, Batch 1/6, Loss: 0.010731411166489124\n","Validation Loss after Epoch 210: 0.02647298574447632\n","Epoch 211/250, Batch 1/6, Loss: 0.009524752385914326\n","Validation Loss after Epoch 211: 0.025969873182475567\n","Epoch 212/250, Batch 1/6, Loss: 0.008390073664486408\n","Validation Loss after Epoch 212: 0.024674021638929844\n","Epoch 213/250, Batch 1/6, Loss: 0.009200559929013252\n","Validation Loss after Epoch 213: 0.024341287091374397\n","Epoch 214/250, Batch 1/6, Loss: 0.009428679943084717\n","Validation Loss after Epoch 214: 0.027478895150125027\n","Epoch 215/250, Batch 1/6, Loss: 0.008931147865951061\n","Validation Loss after Epoch 215: 0.025371216237545013\n","Epoch 216/250, Batch 1/6, Loss: 0.010018069297075272\n","Validation Loss after Epoch 216: 0.02480456419289112\n","Epoch 217/250, Batch 1/6, Loss: 0.007334017660468817\n","Validation Loss after Epoch 217: 0.027078920044004917\n","Epoch 218/250, Batch 1/6, Loss: 0.007850710302591324\n","Validation Loss after Epoch 218: 0.029800301417708397\n","Epoch 219/250, Batch 1/6, Loss: 0.007196739315986633\n","Validation Loss after Epoch 219: 0.032656473107635975\n","Epoch 220/250, Batch 1/6, Loss: 0.0073494501411914825\n","Validation Loss after Epoch 220: 0.031054622493684292\n","Epoch 221/250, Batch 1/6, Loss: 0.0071012647822499275\n","Validation Loss after Epoch 221: 0.03446417488157749\n","Epoch 222/250, Batch 1/6, Loss: 0.007962723262608051\n","Validation Loss after Epoch 222: 0.03347023203969002\n","Epoch 223/250, Batch 1/6, Loss: 0.007575424388051033\n","Validation Loss after Epoch 223: 0.0281157149001956\n","Epoch 224/250, Batch 1/6, Loss: 0.0066282679326832294\n","Validation Loss after Epoch 224: 0.028344443067908287\n","Epoch 225/250, Batch 1/6, Loss: 0.007842106744647026\n","Validation Loss after Epoch 225: 0.03212368581444025\n","Epoch 226/250, Batch 1/6, Loss: 0.007399492897093296\n","Validation Loss after Epoch 226: 0.02775006089359522\n","Epoch 227/250, Batch 1/6, Loss: 0.00801124144345522\n","Validation Loss after Epoch 227: 0.030358504503965378\n","Epoch 228/250, Batch 1/6, Loss: 0.00695406086742878\n","Validation Loss after Epoch 228: 0.034274088218808174\n","Epoch 229/250, Batch 1/6, Loss: 0.0072894468903541565\n","Validation Loss after Epoch 229: 0.0339700561016798\n","Epoch 230/250, Batch 1/6, Loss: 0.008224468678236008\n","Validation Loss after Epoch 230: 0.03321618866175413\n","Epoch 231/250, Batch 1/6, Loss: 0.007639517076313496\n","Validation Loss after Epoch 231: 0.03157409559935331\n","Epoch 232/250, Batch 1/6, Loss: 0.007628365885466337\n","Validation Loss after Epoch 232: 0.03178257867693901\n","Epoch 233/250, Batch 1/6, Loss: 0.007092146202921867\n","Validation Loss after Epoch 233: 0.0313035873696208\n","Epoch 234/250, Batch 1/6, Loss: 0.007314014248549938\n","Validation Loss after Epoch 234: 0.0296940878033638\n","Epoch 235/250, Batch 1/6, Loss: 0.0067133004777133465\n","Validation Loss after Epoch 235: 0.032246808521449566\n","Epoch 236/250, Batch 1/6, Loss: 0.006978736724704504\n","Validation Loss after Epoch 236: 0.027926243841648102\n","Epoch 237/250, Batch 1/6, Loss: 0.009163844399154186\n","Validation Loss after Epoch 237: 0.027530981227755547\n","Epoch 238/250, Batch 1/6, Loss: 0.008783848956227303\n","Validation Loss after Epoch 238: 0.03078414872288704\n","Epoch 239/250, Batch 1/6, Loss: 0.007148279808461666\n","Validation Loss after Epoch 239: 0.03260741475969553\n","Epoch 240/250, Batch 1/6, Loss: 0.006603059824556112\n","Validation Loss after Epoch 240: 0.03390071727335453\n","Epoch 241/250, Batch 1/6, Loss: 0.006933555938303471\n","Validation Loss after Epoch 241: 0.036557409912347794\n","Epoch 242/250, Batch 1/6, Loss: 0.00840625911951065\n","Validation Loss after Epoch 242: 0.03259313013404608\n","Epoch 243/250, Batch 1/6, Loss: 0.007075225468724966\n","Validation Loss after Epoch 243: 0.03638709895312786\n","Epoch 244/250, Batch 1/6, Loss: 0.006788499653339386\n","Validation Loss after Epoch 244: 0.03043278306722641\n","Epoch 245/250, Batch 1/6, Loss: 0.006763153709471226\n","Validation Loss after Epoch 245: 0.03138074651360512\n","Epoch 246/250, Batch 1/6, Loss: 0.006284101866185665\n","Validation Loss after Epoch 246: 0.03266439028084278\n","Epoch 247/250, Batch 1/6, Loss: 0.006438064388930798\n","Validation Loss after Epoch 247: 0.03407311998307705\n","Epoch 248/250, Batch 1/6, Loss: 0.005075449123978615\n","Validation Loss after Epoch 248: 0.03506004624068737\n","Epoch 249/250, Batch 1/6, Loss: 0.0062668537721037865\n","Validation Loss after Epoch 249: 0.03249881137162447\n","Epoch 250/250, Batch 1/6, Loss: 0.00643577566370368\n","Validation Loss after Epoch 250: 0.03122791275382042\n","Subset size 250 - Test Loss: 0.0278, Test Accuracy: 99.07%, Average Dice Score: 0.9909\n","\n","Training on subset size: 500\n","Epoch 1/250, Batch 1/11, Loss: 1.1372126340866089\n","Epoch 1/250, Batch 11/11, Loss: 0.17598779499530792\n","Validation Loss after Epoch 1: 1.1203075647354126\n","Epoch 2/250, Batch 1/11, Loss: 0.16468524932861328\n","Epoch 2/250, Batch 11/11, Loss: 0.10980699956417084\n","Validation Loss after Epoch 2: 1.206481138865153\n","Epoch 3/250, Batch 1/11, Loss: 0.1112859696149826\n","Epoch 3/250, Batch 11/11, Loss: 0.08002862334251404\n","Validation Loss after Epoch 3: 0.8801899154980978\n","Epoch 4/250, Batch 1/11, Loss: 0.07782051712274551\n","Epoch 4/250, Batch 11/11, Loss: 0.07991233468055725\n","Validation Loss after Epoch 4: 0.8551488121350607\n","Epoch 5/250, Batch 1/11, Loss: 0.0642227828502655\n","Epoch 5/250, Batch 11/11, Loss: 0.05665496736764908\n","Validation Loss after Epoch 5: 0.5719077388445536\n","Epoch 6/250, Batch 1/11, Loss: 0.05703214555978775\n","Epoch 6/250, Batch 11/11, Loss: 0.04568994417786598\n","Validation Loss after Epoch 6: 0.09789445747931798\n","Epoch 7/250, Batch 1/11, Loss: 0.04947792738676071\n","Epoch 7/250, Batch 11/11, Loss: 0.051900070160627365\n","Validation Loss after Epoch 7: 0.043876120199759804\n","Epoch 8/250, Batch 1/11, Loss: 0.043856412172317505\n","Epoch 8/250, Batch 11/11, Loss: 0.054107505828142166\n","Validation Loss after Epoch 8: 0.041940487921237946\n","Epoch 9/250, Batch 1/11, Loss: 0.04552309215068817\n","Epoch 9/250, Batch 11/11, Loss: 0.04363348335027695\n","Validation Loss after Epoch 9: 0.0830904593070348\n","Epoch 10/250, Batch 1/11, Loss: 0.04262245446443558\n","Epoch 10/250, Batch 11/11, Loss: 0.043317440897226334\n","Validation Loss after Epoch 10: 0.035372935235500336\n","Epoch 11/250, Batch 1/11, Loss: 0.04510830342769623\n","Epoch 11/250, Batch 11/11, Loss: 0.03717746585607529\n","Validation Loss after Epoch 11: 0.06271596377094586\n","Epoch 12/250, Batch 1/11, Loss: 0.04090811312198639\n","Epoch 12/250, Batch 11/11, Loss: 0.0422823540866375\n","Validation Loss after Epoch 12: 0.032276928424835205\n","Epoch 13/250, Batch 1/11, Loss: 0.04304395243525505\n","Epoch 13/250, Batch 11/11, Loss: 0.0498940572142601\n","Validation Loss after Epoch 13: 0.03476464624206225\n","Epoch 14/250, Batch 1/11, Loss: 0.04256538301706314\n","Epoch 14/250, Batch 11/11, Loss: 0.04032188281416893\n","Validation Loss after Epoch 14: 0.039278159538904824\n","Epoch 15/250, Batch 1/11, Loss: 0.03226328641176224\n","Epoch 15/250, Batch 11/11, Loss: 0.035826392471790314\n","Validation Loss after Epoch 15: 0.03282677630583445\n","Epoch 16/250, Batch 1/11, Loss: 0.036075443029403687\n","Epoch 16/250, Batch 11/11, Loss: 0.039345793426036835\n","Validation Loss after Epoch 16: 0.04670140768090884\n","Epoch 17/250, Batch 1/11, Loss: 0.031217802315950394\n","Epoch 17/250, Batch 11/11, Loss: 0.03305137902498245\n","Validation Loss after Epoch 17: 0.040719177573919296\n","Epoch 18/250, Batch 1/11, Loss: 0.032811280339956284\n","Epoch 18/250, Batch 11/11, Loss: 0.03891245275735855\n","Validation Loss after Epoch 18: 0.028713906183838844\n","Epoch 19/250, Batch 1/11, Loss: 0.03843533992767334\n","Epoch 19/250, Batch 11/11, Loss: 0.034128908067941666\n","Validation Loss after Epoch 19: 0.03067540501554807\n","Epoch 20/250, Batch 1/11, Loss: 0.027856886386871338\n","Epoch 20/250, Batch 11/11, Loss: 0.02874770574271679\n","Validation Loss after Epoch 20: 0.03283678119381269\n","Epoch 21/250, Batch 1/11, Loss: 0.03406568989157677\n","Epoch 21/250, Batch 11/11, Loss: 0.02722511999309063\n","Validation Loss after Epoch 21: 0.03793215875824293\n","Epoch 22/250, Batch 1/11, Loss: 0.03774821758270264\n","Epoch 22/250, Batch 11/11, Loss: 0.038624271750450134\n","Validation Loss after Epoch 22: 0.028323020165165264\n","Epoch 23/250, Batch 1/11, Loss: 0.02900760807096958\n","Epoch 23/250, Batch 11/11, Loss: 0.02629130706191063\n","Validation Loss after Epoch 23: 0.027997018148501713\n","Epoch 24/250, Batch 1/11, Loss: 0.03755392134189606\n","Epoch 24/250, Batch 11/11, Loss: 0.028237657621502876\n","Validation Loss after Epoch 24: 0.02487657529612382\n","Epoch 25/250, Batch 1/11, Loss: 0.026664797216653824\n","Epoch 25/250, Batch 11/11, Loss: 0.030866218730807304\n","Validation Loss after Epoch 25: 0.025137493386864662\n","Epoch 26/250, Batch 1/11, Loss: 0.02737232856452465\n","Epoch 26/250, Batch 11/11, Loss: 0.02862728200852871\n","Validation Loss after Epoch 26: 0.025781196852525074\n","Epoch 27/250, Batch 1/11, Loss: 0.030676741153001785\n","Epoch 27/250, Batch 11/11, Loss: 0.02884315513074398\n","Validation Loss after Epoch 27: 0.025860485931237537\n","Epoch 28/250, Batch 1/11, Loss: 0.02597224898636341\n","Epoch 28/250, Batch 11/11, Loss: 0.02657744660973549\n","Validation Loss after Epoch 28: 0.023627331480383873\n","Epoch 29/250, Batch 1/11, Loss: 0.025765761733055115\n","Epoch 29/250, Batch 11/11, Loss: 0.025297870859503746\n","Validation Loss after Epoch 29: 0.031115670998891194\n","Epoch 30/250, Batch 1/11, Loss: 0.0224990826100111\n","Epoch 30/250, Batch 11/11, Loss: 0.031262196600437164\n","Validation Loss after Epoch 30: 0.024589067324995995\n","Epoch 31/250, Batch 1/11, Loss: 0.025946972891688347\n","Epoch 31/250, Batch 11/11, Loss: 0.027145227417349815\n","Validation Loss after Epoch 31: 0.03904247904817263\n","Epoch 32/250, Batch 1/11, Loss: 0.02669632062315941\n","Epoch 32/250, Batch 11/11, Loss: 0.03702763095498085\n","Validation Loss after Epoch 32: 0.03360728298624357\n","Epoch 33/250, Batch 1/11, Loss: 0.022955801337957382\n","Epoch 33/250, Batch 11/11, Loss: 0.02442806027829647\n","Validation Loss after Epoch 33: 0.03795709212621053\n","Epoch 34/250, Batch 1/11, Loss: 0.02562418207526207\n","Epoch 34/250, Batch 11/11, Loss: 0.028467660769820213\n","Validation Loss after Epoch 34: 0.030125783756375313\n","Epoch 35/250, Batch 1/11, Loss: 0.022815659642219543\n","Epoch 35/250, Batch 11/11, Loss: 0.04115033149719238\n","Validation Loss after Epoch 35: 0.04921585942308108\n","Epoch 36/250, Batch 1/11, Loss: 0.023564761504530907\n","Epoch 36/250, Batch 11/11, Loss: 0.023281706497073174\n","Validation Loss after Epoch 36: 0.025565423692266147\n","Epoch 37/250, Batch 1/11, Loss: 0.026706114411354065\n","Epoch 37/250, Batch 11/11, Loss: 0.02437637560069561\n","Validation Loss after Epoch 37: 0.026221389571825664\n","Epoch 38/250, Batch 1/11, Loss: 0.023071249946951866\n","Epoch 38/250, Batch 11/11, Loss: 0.024317478761076927\n","Validation Loss after Epoch 38: 0.02399297182758649\n","Epoch 39/250, Batch 1/11, Loss: 0.022455500438809395\n","Epoch 39/250, Batch 11/11, Loss: 0.027655761688947678\n","Validation Loss after Epoch 39: 0.026367762436469395\n","Epoch 40/250, Batch 1/11, Loss: 0.028241712599992752\n","Epoch 40/250, Batch 11/11, Loss: 0.02895713597536087\n","Validation Loss after Epoch 40: 0.02434852346777916\n","Epoch 41/250, Batch 1/11, Loss: 0.02598700486123562\n","Epoch 41/250, Batch 11/11, Loss: 0.021734293550252914\n","Validation Loss after Epoch 41: 0.029156075169642765\n","Epoch 42/250, Batch 1/11, Loss: 0.023544177412986755\n","Epoch 42/250, Batch 11/11, Loss: 0.02632836438715458\n","Validation Loss after Epoch 42: 0.03253745039304098\n","Epoch 43/250, Batch 1/11, Loss: 0.021658243611454964\n","Epoch 43/250, Batch 11/11, Loss: 0.023680469021201134\n","Validation Loss after Epoch 43: 0.02228902404507001\n","Epoch 44/250, Batch 1/11, Loss: 0.0231300201267004\n","Epoch 44/250, Batch 11/11, Loss: 0.029537642374634743\n","Validation Loss after Epoch 44: 0.02369970145324866\n","Epoch 45/250, Batch 1/11, Loss: 0.022021301090717316\n","Epoch 45/250, Batch 11/11, Loss: 0.028103439137339592\n","Validation Loss after Epoch 45: 0.02104450638095538\n","Epoch 46/250, Batch 1/11, Loss: 0.02076643332839012\n","Epoch 46/250, Batch 11/11, Loss: 0.019802935421466827\n","Validation Loss after Epoch 46: 0.03064693696796894\n","Epoch 47/250, Batch 1/11, Loss: 0.022366879507899284\n","Epoch 47/250, Batch 11/11, Loss: 0.021362625062465668\n","Validation Loss after Epoch 47: 0.03131584512690703\n","Epoch 48/250, Batch 1/11, Loss: 0.01895556040108204\n","Epoch 48/250, Batch 11/11, Loss: 0.019228337332606316\n","Validation Loss after Epoch 48: 0.022091425955295563\n","Epoch 49/250, Batch 1/11, Loss: 0.022600390017032623\n","Epoch 49/250, Batch 11/11, Loss: 0.02028586156666279\n","Validation Loss after Epoch 49: 0.0337605228026708\n","Epoch 50/250, Batch 1/11, Loss: 0.023014351725578308\n","Epoch 50/250, Batch 11/11, Loss: 0.02707851305603981\n","Validation Loss after Epoch 50: 0.02093275139729182\n","Epoch 51/250, Batch 1/11, Loss: 0.02303117699921131\n","Epoch 51/250, Batch 11/11, Loss: 0.021028969436883926\n","Validation Loss after Epoch 51: 0.02879901168247064\n","Epoch 52/250, Batch 1/11, Loss: 0.02046215906739235\n","Epoch 52/250, Batch 11/11, Loss: 0.021353216841816902\n","Validation Loss after Epoch 52: 0.02183067922790845\n","Epoch 53/250, Batch 1/11, Loss: 0.019840138033032417\n","Epoch 53/250, Batch 11/11, Loss: 0.02568736858665943\n","Validation Loss after Epoch 53: 0.02285231649875641\n","Epoch 54/250, Batch 1/11, Loss: 0.01875985600054264\n","Epoch 54/250, Batch 11/11, Loss: 0.022580236196517944\n","Validation Loss after Epoch 54: 0.01982908882200718\n","Epoch 55/250, Batch 1/11, Loss: 0.028162652626633644\n","Epoch 55/250, Batch 11/11, Loss: 0.029965637251734734\n","Validation Loss after Epoch 55: 0.02362457662820816\n","Epoch 56/250, Batch 1/11, Loss: 0.024142997339367867\n","Epoch 56/250, Batch 11/11, Loss: 0.021580638363957405\n","Validation Loss after Epoch 56: 0.022286780178546906\n","Epoch 57/250, Batch 1/11, Loss: 0.021968882530927658\n","Epoch 57/250, Batch 11/11, Loss: 0.019418805837631226\n","Validation Loss after Epoch 57: 0.029475601390004158\n","Epoch 58/250, Batch 1/11, Loss: 0.02201198786497116\n","Epoch 58/250, Batch 11/11, Loss: 0.021088553592562675\n","Validation Loss after Epoch 58: 0.022809583072861035\n","Epoch 59/250, Batch 1/11, Loss: 0.01909187063574791\n","Epoch 59/250, Batch 11/11, Loss: 0.01923755370080471\n","Validation Loss after Epoch 59: 0.021687933554251988\n","Epoch 60/250, Batch 1/11, Loss: 0.020780405029654503\n","Epoch 60/250, Batch 11/11, Loss: 0.019489804282784462\n","Validation Loss after Epoch 60: 0.020790905381242435\n","Epoch 61/250, Batch 1/11, Loss: 0.017685895785689354\n","Epoch 61/250, Batch 11/11, Loss: 0.019226305186748505\n","Validation Loss after Epoch 61: 0.02070240055521329\n","Epoch 62/250, Batch 1/11, Loss: 0.01927974447607994\n","Epoch 62/250, Batch 11/11, Loss: 0.021031904965639114\n","Validation Loss after Epoch 62: 0.020230707402030628\n","Epoch 63/250, Batch 1/11, Loss: 0.018199373036623\n","Epoch 63/250, Batch 11/11, Loss: 0.01859229989349842\n","Validation Loss after Epoch 63: 0.02409638154009978\n","Epoch 64/250, Batch 1/11, Loss: 0.019450372084975243\n","Epoch 64/250, Batch 11/11, Loss: 0.019819892942905426\n","Validation Loss after Epoch 64: 0.022051948433121044\n","Epoch 65/250, Batch 1/11, Loss: 0.019596336409449577\n","Epoch 65/250, Batch 11/11, Loss: 0.017897604033350945\n","Validation Loss after Epoch 65: 0.022725191588203113\n","Epoch 66/250, Batch 1/11, Loss: 0.018541928380727768\n","Epoch 66/250, Batch 11/11, Loss: 0.018821561709046364\n","Validation Loss after Epoch 66: 0.020427986979484558\n","Epoch 67/250, Batch 1/11, Loss: 0.01848139800131321\n","Epoch 67/250, Batch 11/11, Loss: 0.018682103604078293\n","Validation Loss after Epoch 67: 0.02561168745160103\n","Epoch 68/250, Batch 1/11, Loss: 0.01932309940457344\n","Epoch 68/250, Batch 11/11, Loss: 0.018614768981933594\n","Validation Loss after Epoch 68: 0.02410705511768659\n","Epoch 69/250, Batch 1/11, Loss: 0.017288852483034134\n","Epoch 69/250, Batch 11/11, Loss: 0.018329931423068047\n","Validation Loss after Epoch 69: 0.021120500440398853\n","Epoch 70/250, Batch 1/11, Loss: 0.019815027713775635\n","Epoch 70/250, Batch 11/11, Loss: 0.021107880398631096\n","Validation Loss after Epoch 70: 0.03130750854810079\n","Epoch 71/250, Batch 1/11, Loss: 0.018656594678759575\n","Epoch 71/250, Batch 11/11, Loss: 0.021298302337527275\n","Validation Loss after Epoch 71: 0.022822937617699306\n","Epoch 72/250, Batch 1/11, Loss: 0.01799824833869934\n","Epoch 72/250, Batch 11/11, Loss: 0.02043578028678894\n","Validation Loss after Epoch 72: 0.023467503488063812\n","Epoch 73/250, Batch 1/11, Loss: 0.01706836000084877\n","Epoch 73/250, Batch 11/11, Loss: 0.018391478806734085\n","Validation Loss after Epoch 73: 0.02362401969730854\n","Epoch 74/250, Batch 1/11, Loss: 0.01734916679561138\n","Epoch 74/250, Batch 11/11, Loss: 0.017083045095205307\n","Validation Loss after Epoch 74: 0.03579006840785345\n","Epoch 75/250, Batch 1/11, Loss: 0.019157053902745247\n","Epoch 75/250, Batch 11/11, Loss: 0.021298320963978767\n","Validation Loss after Epoch 75: 0.02466920142372449\n","Epoch 76/250, Batch 1/11, Loss: 0.017115166410803795\n","Epoch 76/250, Batch 11/11, Loss: 0.018627682700753212\n","Validation Loss after Epoch 76: 0.028250960633158684\n","Epoch 77/250, Batch 1/11, Loss: 0.01694442890584469\n","Epoch 77/250, Batch 11/11, Loss: 0.017057891935110092\n","Validation Loss after Epoch 77: 0.02665509780248006\n","Epoch 78/250, Batch 1/11, Loss: 0.018452825024724007\n","Epoch 78/250, Batch 11/11, Loss: 0.019360003992915154\n","Validation Loss after Epoch 78: 0.019540963073571522\n","Epoch 79/250, Batch 1/11, Loss: 0.01646742783486843\n","Epoch 79/250, Batch 11/11, Loss: 0.020265694707632065\n","Validation Loss after Epoch 79: 0.02393303873638312\n","Epoch 80/250, Batch 1/11, Loss: 0.02140984684228897\n","Epoch 80/250, Batch 11/11, Loss: 0.018939703702926636\n","Validation Loss after Epoch 80: 0.022211348017056782\n","Epoch 81/250, Batch 1/11, Loss: 0.022952575236558914\n","Epoch 81/250, Batch 11/11, Loss: 0.01760828122496605\n","Validation Loss after Epoch 81: 0.02098981725672881\n","Epoch 82/250, Batch 1/11, Loss: 0.02277546375989914\n","Epoch 82/250, Batch 11/11, Loss: 0.01788201741874218\n","Validation Loss after Epoch 82: 0.021652047832806904\n","Epoch 83/250, Batch 1/11, Loss: 0.016879260540008545\n","Epoch 83/250, Batch 11/11, Loss: 0.02143215946853161\n","Validation Loss after Epoch 83: 0.01956753060221672\n","Epoch 84/250, Batch 1/11, Loss: 0.017744965851306915\n","Epoch 84/250, Batch 11/11, Loss: 0.017062464728951454\n","Validation Loss after Epoch 84: 0.0260141318043073\n","Epoch 85/250, Batch 1/11, Loss: 0.01530684344470501\n","Epoch 85/250, Batch 11/11, Loss: 0.016698207706212997\n","Validation Loss after Epoch 85: 0.024412245179216068\n","Epoch 86/250, Batch 1/11, Loss: 0.016620205715298653\n","Epoch 86/250, Batch 11/11, Loss: 0.015746843069791794\n","Validation Loss after Epoch 86: 0.019763904934128124\n","Epoch 87/250, Batch 1/11, Loss: 0.016445660963654518\n","Epoch 87/250, Batch 11/11, Loss: 0.015712497755885124\n","Validation Loss after Epoch 87: 0.021968590716520946\n","Epoch 88/250, Batch 1/11, Loss: 0.014037596061825752\n","Epoch 88/250, Batch 11/11, Loss: 0.016874734312295914\n","Validation Loss after Epoch 88: 0.021487197528282802\n","Epoch 89/250, Batch 1/11, Loss: 0.016396723687648773\n","Epoch 89/250, Batch 11/11, Loss: 0.021519511938095093\n","Validation Loss after Epoch 89: 0.024808232362071674\n","Epoch 90/250, Batch 1/11, Loss: 0.01714283600449562\n","Epoch 90/250, Batch 11/11, Loss: 0.016832934692502022\n","Validation Loss after Epoch 90: 0.023165979112188022\n","Epoch 91/250, Batch 1/11, Loss: 0.015373291447758675\n","Epoch 91/250, Batch 11/11, Loss: 0.017459187656641006\n","Validation Loss after Epoch 91: 0.019571468854943912\n","Epoch 92/250, Batch 1/11, Loss: 0.015605205669999123\n","Epoch 92/250, Batch 11/11, Loss: 0.015247773379087448\n","Validation Loss after Epoch 92: 0.022942776481310528\n","Epoch 93/250, Batch 1/11, Loss: 0.017541052773594856\n","Epoch 93/250, Batch 11/11, Loss: 0.01506879087537527\n","Validation Loss after Epoch 93: 0.019812894985079765\n","Epoch 94/250, Batch 1/11, Loss: 0.015576967969536781\n","Epoch 94/250, Batch 11/11, Loss: 0.01582319103181362\n","Validation Loss after Epoch 94: 0.023455390706658363\n","Epoch 95/250, Batch 1/11, Loss: 0.014021215960383415\n","Epoch 95/250, Batch 11/11, Loss: 0.013863282278180122\n","Validation Loss after Epoch 95: 0.021972352638840675\n","Epoch 96/250, Batch 1/11, Loss: 0.013477051630616188\n","Epoch 96/250, Batch 11/11, Loss: 0.014614807441830635\n","Validation Loss after Epoch 96: 0.01928713545203209\n","Epoch 97/250, Batch 1/11, Loss: 0.015187430195510387\n","Epoch 97/250, Batch 11/11, Loss: 0.015212548896670341\n","Validation Loss after Epoch 97: 0.027227403596043587\n","Epoch 98/250, Batch 1/11, Loss: 0.014695669524371624\n","Epoch 98/250, Batch 11/11, Loss: 0.016308939084410667\n","Validation Loss after Epoch 98: 0.018983503182729084\n","Epoch 99/250, Batch 1/11, Loss: 0.017993822693824768\n","Epoch 99/250, Batch 11/11, Loss: 0.015029549598693848\n","Validation Loss after Epoch 99: 0.024127556011080742\n","Epoch 100/250, Batch 1/11, Loss: 0.014914360828697681\n","Epoch 100/250, Batch 11/11, Loss: 0.017004389315843582\n","Validation Loss after Epoch 100: 0.025494436422983806\n","Epoch 101/250, Batch 1/11, Loss: 0.014484575018286705\n","Epoch 101/250, Batch 11/11, Loss: 0.014732051640748978\n","Validation Loss after Epoch 101: 0.02987603098154068\n","Epoch 102/250, Batch 1/11, Loss: 0.013881735503673553\n","Epoch 102/250, Batch 11/11, Loss: 0.013962843455374241\n","Validation Loss after Epoch 102: 0.03055712270239989\n","Epoch 103/250, Batch 1/11, Loss: 0.01397702470421791\n","Epoch 103/250, Batch 11/11, Loss: 0.01436771359294653\n","Validation Loss after Epoch 103: 0.02144194394350052\n","Epoch 104/250, Batch 1/11, Loss: 0.014238106086850166\n","Epoch 104/250, Batch 11/11, Loss: 0.013907140120863914\n","Validation Loss after Epoch 104: 0.02162609373529752\n","Epoch 105/250, Batch 1/11, Loss: 0.01409225445240736\n","Epoch 105/250, Batch 11/11, Loss: 0.014035822823643684\n","Validation Loss after Epoch 105: 0.02437477745115757\n","Epoch 106/250, Batch 1/11, Loss: 0.01301977876573801\n","Epoch 106/250, Batch 11/11, Loss: 0.014389160089194775\n","Validation Loss after Epoch 106: 0.028149032965302467\n","Epoch 107/250, Batch 1/11, Loss: 0.013549309223890305\n","Epoch 107/250, Batch 11/11, Loss: 0.014516958966851234\n","Validation Loss after Epoch 107: 0.02066025013724963\n","Epoch 108/250, Batch 1/11, Loss: 0.013598323799669743\n","Epoch 108/250, Batch 11/11, Loss: 0.014373376034200191\n","Validation Loss after Epoch 108: 0.019930105656385422\n","Epoch 109/250, Batch 1/11, Loss: 0.014814801514148712\n","Epoch 109/250, Batch 11/11, Loss: 0.014538103714585304\n","Validation Loss after Epoch 109: 0.020054145405689876\n","Epoch 110/250, Batch 1/11, Loss: 0.014176065102219582\n","Epoch 110/250, Batch 11/11, Loss: 0.012593826279044151\n","Validation Loss after Epoch 110: 0.02249050885438919\n","Epoch 111/250, Batch 1/11, Loss: 0.014369301497936249\n","Epoch 111/250, Batch 11/11, Loss: 0.013206440955400467\n","Validation Loss after Epoch 111: 0.022188307717442513\n","Epoch 112/250, Batch 1/11, Loss: 0.012182457372546196\n","Epoch 112/250, Batch 11/11, Loss: 0.012510999105870724\n","Validation Loss after Epoch 112: 0.02122634028395017\n","Epoch 113/250, Batch 1/11, Loss: 0.012831959873437881\n","Epoch 113/250, Batch 11/11, Loss: 0.015352764166891575\n","Validation Loss after Epoch 113: 0.033493159959713616\n","Epoch 114/250, Batch 1/11, Loss: 0.017099451273679733\n","Epoch 114/250, Batch 11/11, Loss: 0.013658718205988407\n","Validation Loss after Epoch 114: 0.02844778137902419\n","Epoch 115/250, Batch 1/11, Loss: 0.01702027954161167\n","Epoch 115/250, Batch 11/11, Loss: 0.01522514596581459\n","Validation Loss after Epoch 115: 0.02236406443019708\n","Epoch 116/250, Batch 1/11, Loss: 0.01438763178884983\n","Epoch 116/250, Batch 11/11, Loss: 0.014508293010294437\n","Validation Loss after Epoch 116: 0.021128928909699123\n","Epoch 117/250, Batch 1/11, Loss: 0.011947637423872948\n","Epoch 117/250, Batch 11/11, Loss: 0.013863525353372097\n","Validation Loss after Epoch 117: 0.027129501725236576\n","Epoch 118/250, Batch 1/11, Loss: 0.014316744171082973\n","Epoch 118/250, Batch 11/11, Loss: 0.01565110869705677\n","Validation Loss after Epoch 118: 0.023843465993801754\n","Epoch 119/250, Batch 1/11, Loss: 0.018875226378440857\n","Epoch 119/250, Batch 11/11, Loss: 0.015392990782856941\n","Validation Loss after Epoch 119: 0.022342507417003315\n","Epoch 120/250, Batch 1/11, Loss: 0.013860287144780159\n","Epoch 120/250, Batch 11/11, Loss: 0.017865058034658432\n","Validation Loss after Epoch 120: 0.03892959530154864\n","Epoch 121/250, Batch 1/11, Loss: 0.01857895590364933\n","Epoch 121/250, Batch 11/11, Loss: 0.013528307899832726\n","Validation Loss after Epoch 121: 0.018868209794163704\n","Epoch 122/250, Batch 1/11, Loss: 0.013811848126351833\n","Epoch 122/250, Batch 11/11, Loss: 0.012168596498668194\n","Validation Loss after Epoch 122: 0.020058921227852505\n","Epoch 123/250, Batch 1/11, Loss: 0.01253265980631113\n","Epoch 123/250, Batch 11/11, Loss: 0.012400106526911259\n","Validation Loss after Epoch 123: 0.027672141790390015\n","Epoch 124/250, Batch 1/11, Loss: 0.013075992465019226\n","Epoch 124/250, Batch 11/11, Loss: 0.012551975436508656\n","Validation Loss after Epoch 124: 0.029670610403021175\n","Epoch 125/250, Batch 1/11, Loss: 0.013538551516830921\n","Epoch 125/250, Batch 11/11, Loss: 0.013489403761923313\n","Validation Loss after Epoch 125: 0.030214902013540268\n","Epoch 126/250, Batch 1/11, Loss: 0.012091828510165215\n","Epoch 126/250, Batch 11/11, Loss: 0.012095705606043339\n","Validation Loss after Epoch 126: 0.01887041764954726\n","Epoch 127/250, Batch 1/11, Loss: 0.012139313854277134\n","Epoch 127/250, Batch 11/11, Loss: 0.011572476476430893\n","Validation Loss after Epoch 127: 0.021288114910324413\n","Epoch 128/250, Batch 1/11, Loss: 0.010861638002097607\n","Epoch 128/250, Batch 11/11, Loss: 0.01264895498752594\n","Validation Loss after Epoch 128: 0.021394579981764156\n","Epoch 129/250, Batch 1/11, Loss: 0.011085928417742252\n","Epoch 129/250, Batch 11/11, Loss: 0.011677775532007217\n","Validation Loss after Epoch 129: 0.025266727432608604\n","Epoch 130/250, Batch 1/11, Loss: 0.012860393151640892\n","Epoch 130/250, Batch 11/11, Loss: 0.016179833561182022\n","Validation Loss after Epoch 130: 0.021613600353399914\n","Epoch 131/250, Batch 1/11, Loss: 0.011985810473561287\n","Epoch 131/250, Batch 11/11, Loss: 0.01263579074293375\n","Validation Loss after Epoch 131: 0.018596261739730835\n","Epoch 132/250, Batch 1/11, Loss: 0.010940566658973694\n","Epoch 132/250, Batch 11/11, Loss: 0.011611363850533962\n","Validation Loss after Epoch 132: 0.027689099311828613\n","Epoch 133/250, Batch 1/11, Loss: 0.011843589134514332\n","Epoch 133/250, Batch 11/11, Loss: 0.011804267764091492\n","Validation Loss after Epoch 133: 0.02942026158173879\n","Epoch 134/250, Batch 1/11, Loss: 0.011743488721549511\n","Epoch 134/250, Batch 11/11, Loss: 0.011713317595422268\n","Validation Loss after Epoch 134: 0.029601068546374638\n","Epoch 135/250, Batch 1/11, Loss: 0.012934509664773941\n","Epoch 135/250, Batch 11/11, Loss: 0.010927622206509113\n","Validation Loss after Epoch 135: 0.020210696384310722\n","Epoch 136/250, Batch 1/11, Loss: 0.013032055459916592\n","Epoch 136/250, Batch 11/11, Loss: 0.011678014881908894\n","Validation Loss after Epoch 136: 0.03187045454978943\n","Epoch 137/250, Batch 1/11, Loss: 0.011402684263885021\n","Epoch 137/250, Batch 11/11, Loss: 0.012962066568434238\n","Validation Loss after Epoch 137: 0.01876617098848025\n","Epoch 138/250, Batch 1/11, Loss: 0.0113176628947258\n","Epoch 138/250, Batch 11/11, Loss: 0.012136016972362995\n","Validation Loss after Epoch 138: 0.028065485879778862\n","Epoch 139/250, Batch 1/11, Loss: 0.011329369619488716\n","Epoch 139/250, Batch 11/11, Loss: 0.010974754579365253\n","Validation Loss after Epoch 139: 0.021436055501302082\n","Epoch 140/250, Batch 1/11, Loss: 0.011194279417395592\n","Epoch 140/250, Batch 11/11, Loss: 0.011350403539836407\n","Validation Loss after Epoch 140: 0.021867694333195686\n","Epoch 141/250, Batch 1/11, Loss: 0.009992453269660473\n","Epoch 141/250, Batch 11/11, Loss: 0.011574280448257923\n","Validation Loss after Epoch 141: 0.02017456665635109\n","Epoch 142/250, Batch 1/11, Loss: 0.010312982834875584\n","Epoch 142/250, Batch 11/11, Loss: 0.011145472526550293\n","Validation Loss after Epoch 142: 0.030202153449257214\n","Epoch 143/250, Batch 1/11, Loss: 0.009817581623792648\n","Epoch 143/250, Batch 11/11, Loss: 0.01047188974916935\n","Validation Loss after Epoch 143: 0.0270921029150486\n","Epoch 144/250, Batch 1/11, Loss: 0.010186155326664448\n","Epoch 144/250, Batch 11/11, Loss: 0.010245450772345066\n","Validation Loss after Epoch 144: 0.030545733248194058\n","Epoch 145/250, Batch 1/11, Loss: 0.011623273603618145\n","Epoch 145/250, Batch 11/11, Loss: 0.010562602430582047\n","Validation Loss after Epoch 145: 0.019312956059972446\n","Epoch 146/250, Batch 1/11, Loss: 0.011562834493815899\n","Epoch 146/250, Batch 11/11, Loss: 0.012000204995274544\n","Validation Loss after Epoch 146: 0.03013864780465762\n","Epoch 147/250, Batch 1/11, Loss: 0.009830033406615257\n","Epoch 147/250, Batch 11/11, Loss: 0.010823952034115791\n","Validation Loss after Epoch 147: 0.019558407987157505\n","Epoch 148/250, Batch 1/11, Loss: 0.009533981792628765\n","Epoch 148/250, Batch 11/11, Loss: 0.009904155507683754\n","Validation Loss after Epoch 148: 0.024543202792604763\n","Epoch 149/250, Batch 1/11, Loss: 0.008768143132328987\n","Epoch 149/250, Batch 11/11, Loss: 0.01014280691742897\n","Validation Loss after Epoch 149: 0.02214737671116988\n","Epoch 150/250, Batch 1/11, Loss: 0.009907104074954987\n","Epoch 150/250, Batch 11/11, Loss: 0.009782655164599419\n","Validation Loss after Epoch 150: 0.02178495128949483\n","Epoch 151/250, Batch 1/11, Loss: 0.009789823554456234\n","Epoch 151/250, Batch 11/11, Loss: 0.010057286359369755\n","Validation Loss after Epoch 151: 0.027501892298460007\n","Epoch 152/250, Batch 1/11, Loss: 0.008978886529803276\n","Epoch 152/250, Batch 11/11, Loss: 0.009401758201420307\n","Validation Loss after Epoch 152: 0.024734235058228176\n","Epoch 153/250, Batch 1/11, Loss: 0.00945249292999506\n","Epoch 153/250, Batch 11/11, Loss: 0.012273216620087624\n","Validation Loss after Epoch 153: 0.026221984376509983\n","Epoch 154/250, Batch 1/11, Loss: 0.008408505469560623\n","Epoch 154/250, Batch 11/11, Loss: 0.009450183250010014\n","Validation Loss after Epoch 154: 0.021768889700373013\n","Epoch 155/250, Batch 1/11, Loss: 0.009888635016977787\n","Epoch 155/250, Batch 11/11, Loss: 0.008802960626780987\n","Validation Loss after Epoch 155: 0.027761956055959065\n","Epoch 156/250, Batch 1/11, Loss: 0.010294822044670582\n","Epoch 156/250, Batch 11/11, Loss: 0.008461025543510914\n","Validation Loss after Epoch 156: 0.0251262616366148\n","Epoch 157/250, Batch 1/11, Loss: 0.010187830775976181\n","Epoch 157/250, Batch 11/11, Loss: 0.008811041712760925\n","Validation Loss after Epoch 157: 0.028998597835501034\n","Epoch 158/250, Batch 1/11, Loss: 0.008643921464681625\n","Epoch 158/250, Batch 11/11, Loss: 0.008775681257247925\n","Validation Loss after Epoch 158: 0.028716059401631355\n","Epoch 159/250, Batch 1/11, Loss: 0.008622267283499241\n","Epoch 159/250, Batch 11/11, Loss: 0.008163783699274063\n","Validation Loss after Epoch 159: 0.02746746564904849\n","Epoch 160/250, Batch 1/11, Loss: 0.007984768599271774\n","Epoch 160/250, Batch 11/11, Loss: 0.00837050098925829\n","Validation Loss after Epoch 160: 0.021722738320628803\n","Epoch 161/250, Batch 1/11, Loss: 0.008495800197124481\n","Epoch 161/250, Batch 11/11, Loss: 0.008847353979945183\n","Validation Loss after Epoch 161: 0.02998657524585724\n","Epoch 162/250, Batch 1/11, Loss: 0.010393843054771423\n","Epoch 162/250, Batch 11/11, Loss: 0.008979272097349167\n","Validation Loss after Epoch 162: 0.028080668300390244\n","Epoch 163/250, Batch 1/11, Loss: 0.008413728326559067\n","Epoch 163/250, Batch 11/11, Loss: 0.008228336460888386\n","Validation Loss after Epoch 163: 0.018146760140856106\n","Epoch 164/250, Batch 1/11, Loss: 0.009330119006335735\n","Epoch 164/250, Batch 11/11, Loss: 0.008315561339259148\n","Validation Loss after Epoch 164: 0.020632849385341007\n","Epoch 165/250, Batch 1/11, Loss: 0.008959013968706131\n","Epoch 165/250, Batch 11/11, Loss: 0.009210007265210152\n","Validation Loss after Epoch 165: 0.025158547485868137\n","Epoch 166/250, Batch 1/11, Loss: 0.01009484101086855\n","Epoch 166/250, Batch 11/11, Loss: 0.008244085125625134\n","Validation Loss after Epoch 166: 0.02993376987675826\n","Epoch 167/250, Batch 1/11, Loss: 0.008453083224594593\n","Epoch 167/250, Batch 11/11, Loss: 0.008609875105321407\n","Validation Loss after Epoch 167: 0.019943333541353542\n","Epoch 168/250, Batch 1/11, Loss: 0.008479692973196507\n","Epoch 168/250, Batch 11/11, Loss: 0.008536937646567822\n","Validation Loss after Epoch 168: 0.028940510004758835\n","Epoch 169/250, Batch 1/11, Loss: 0.007727649994194508\n","Epoch 169/250, Batch 11/11, Loss: 0.007987680844962597\n","Validation Loss after Epoch 169: 0.032664054383834205\n","Epoch 170/250, Batch 1/11, Loss: 0.009276201948523521\n","Epoch 170/250, Batch 11/11, Loss: 0.00840137992054224\n","Validation Loss after Epoch 170: 0.021884606530268986\n","Epoch 171/250, Batch 1/11, Loss: 0.00944540835916996\n","Epoch 171/250, Batch 11/11, Loss: 0.007921595126390457\n","Validation Loss after Epoch 171: 0.02404513085881869\n","Epoch 172/250, Batch 1/11, Loss: 0.007473961915820837\n","Epoch 172/250, Batch 11/11, Loss: 0.009263952262699604\n","Validation Loss after Epoch 172: 0.052846179654200874\n","Epoch 173/250, Batch 1/11, Loss: 0.014630617573857307\n","Epoch 173/250, Batch 11/11, Loss: 0.010128247551620007\n","Validation Loss after Epoch 173: 0.04662845035394033\n","Epoch 174/250, Batch 1/11, Loss: 0.00946718268096447\n","Epoch 174/250, Batch 11/11, Loss: 0.00920326542109251\n","Validation Loss after Epoch 174: 0.025213670606414478\n","Epoch 175/250, Batch 1/11, Loss: 0.007212094031274319\n","Epoch 175/250, Batch 11/11, Loss: 0.008272988721728325\n","Validation Loss after Epoch 175: 0.025829078008731205\n","Epoch 176/250, Batch 1/11, Loss: 0.007420997601002455\n","Epoch 176/250, Batch 11/11, Loss: 0.007081146351993084\n","Validation Loss after Epoch 176: 0.03065856049458186\n","Epoch 177/250, Batch 1/11, Loss: 0.007249629124999046\n","Epoch 177/250, Batch 11/11, Loss: 0.008272946812212467\n","Validation Loss after Epoch 177: 0.0246979674945275\n","Epoch 178/250, Batch 1/11, Loss: 0.006919629871845245\n","Epoch 178/250, Batch 11/11, Loss: 0.008184252306818962\n","Validation Loss after Epoch 178: 0.023443347464005154\n","Epoch 179/250, Batch 1/11, Loss: 0.006806730292737484\n","Epoch 179/250, Batch 11/11, Loss: 0.007692473009228706\n","Validation Loss after Epoch 179: 0.03018085223933061\n","Epoch 180/250, Batch 1/11, Loss: 0.007142435759305954\n","Epoch 180/250, Batch 11/11, Loss: 0.007173876743763685\n","Validation Loss after Epoch 180: 0.02418059917787711\n","Epoch 181/250, Batch 1/11, Loss: 0.0067740450613200665\n","Epoch 181/250, Batch 11/11, Loss: 0.007963558658957481\n","Validation Loss after Epoch 181: 0.04351238161325455\n","Epoch 182/250, Batch 1/11, Loss: 0.01141093298792839\n","Epoch 182/250, Batch 11/11, Loss: 0.007775281555950642\n","Validation Loss after Epoch 182: 0.050680639843146004\n","Epoch 183/250, Batch 1/11, Loss: 0.007504619657993317\n","Epoch 183/250, Batch 11/11, Loss: 0.007503516506403685\n","Validation Loss after Epoch 183: 0.026278819888830185\n","Epoch 184/250, Batch 1/11, Loss: 0.007396634668111801\n","Epoch 184/250, Batch 11/11, Loss: 0.0075973100028932095\n","Validation Loss after Epoch 184: 0.022868327796459198\n","Epoch 185/250, Batch 1/11, Loss: 0.008909417316317558\n","Epoch 185/250, Batch 11/11, Loss: 0.007766467984765768\n","Validation Loss after Epoch 185: 0.02270649808148543\n","Epoch 186/250, Batch 1/11, Loss: 0.007309694308787584\n","Epoch 186/250, Batch 11/11, Loss: 0.007864768616855145\n","Validation Loss after Epoch 186: 0.023143713052074116\n","Epoch 187/250, Batch 1/11, Loss: 0.007016438525170088\n","Epoch 187/250, Batch 11/11, Loss: 0.008319537155330181\n","Validation Loss after Epoch 187: 0.02798541511098544\n","Epoch 188/250, Batch 1/11, Loss: 0.0072860922664403915\n","Epoch 188/250, Batch 11/11, Loss: 0.006981062702834606\n","Validation Loss after Epoch 188: 0.029552993054191273\n","Epoch 189/250, Batch 1/11, Loss: 0.006392334122210741\n","Epoch 189/250, Batch 11/11, Loss: 0.00710506783798337\n","Validation Loss after Epoch 189: 0.0298561646292607\n","Epoch 190/250, Batch 1/11, Loss: 0.00673279631882906\n","Epoch 190/250, Batch 11/11, Loss: 0.006832359358668327\n","Validation Loss after Epoch 190: 0.026030780126651127\n","Epoch 191/250, Batch 1/11, Loss: 0.008007253520190716\n","Epoch 191/250, Batch 11/11, Loss: 0.00746404891833663\n","Validation Loss after Epoch 191: 0.03030632808804512\n","Epoch 192/250, Batch 1/11, Loss: 0.007030240725725889\n","Epoch 192/250, Batch 11/11, Loss: 0.0068006510846316814\n","Validation Loss after Epoch 192: 0.02385145736237367\n","Epoch 193/250, Batch 1/11, Loss: 0.008432882837951183\n","Epoch 193/250, Batch 11/11, Loss: 0.009335875511169434\n","Validation Loss after Epoch 193: 0.022337314983208973\n","Epoch 194/250, Batch 1/11, Loss: 0.007850279100239277\n","Epoch 194/250, Batch 11/11, Loss: 0.0065865847282111645\n","Validation Loss after Epoch 194: 0.025585437814394634\n","Epoch 195/250, Batch 1/11, Loss: 0.00703005027025938\n","Epoch 195/250, Batch 11/11, Loss: 0.007267976179718971\n","Validation Loss after Epoch 195: 0.030637728050351143\n","Epoch 196/250, Batch 1/11, Loss: 0.006697802804410458\n","Epoch 196/250, Batch 11/11, Loss: 0.00722988648340106\n","Validation Loss after Epoch 196: 0.022248216594258945\n","Epoch 197/250, Batch 1/11, Loss: 0.006237425841391087\n","Epoch 197/250, Batch 11/11, Loss: 0.007560279686003923\n","Validation Loss after Epoch 197: 0.03359533101320267\n","Epoch 198/250, Batch 1/11, Loss: 0.00655325036495924\n","Epoch 198/250, Batch 11/11, Loss: 0.006571466103196144\n","Validation Loss after Epoch 198: 0.028355618317921955\n","Epoch 199/250, Batch 1/11, Loss: 0.006942820735275745\n","Epoch 199/250, Batch 11/11, Loss: 0.0063954368233680725\n","Validation Loss after Epoch 199: 0.026904720813035965\n","Epoch 200/250, Batch 1/11, Loss: 0.006090092938393354\n","Epoch 200/250, Batch 11/11, Loss: 0.007903296500444412\n","Validation Loss after Epoch 200: 0.025342602282762527\n","Epoch 201/250, Batch 1/11, Loss: 0.007911236956715584\n","Epoch 201/250, Batch 11/11, Loss: 0.0067450362257659435\n","Validation Loss after Epoch 201: 0.028363017365336418\n","Epoch 202/250, Batch 1/11, Loss: 0.006725207902491093\n","Epoch 202/250, Batch 11/11, Loss: 0.007435482461005449\n","Validation Loss after Epoch 202: 0.023368805026014645\n","Epoch 203/250, Batch 1/11, Loss: 0.005634211003780365\n","Epoch 203/250, Batch 11/11, Loss: 0.00627092132344842\n","Validation Loss after Epoch 203: 0.024965786064664524\n","Epoch 204/250, Batch 1/11, Loss: 0.006899015977978706\n","Epoch 204/250, Batch 11/11, Loss: 0.006951274815946817\n","Validation Loss after Epoch 204: 0.024580379327138264\n","Epoch 205/250, Batch 1/11, Loss: 0.0060365162789821625\n","Epoch 205/250, Batch 11/11, Loss: 0.007831812836229801\n","Validation Loss after Epoch 205: 0.024148868396878242\n","Epoch 206/250, Batch 1/11, Loss: 0.0053832679986953735\n","Epoch 206/250, Batch 11/11, Loss: 0.009188330732285976\n","Validation Loss after Epoch 206: 0.03962970773379008\n","Epoch 207/250, Batch 1/11, Loss: 0.006260755937546492\n","Epoch 207/250, Batch 11/11, Loss: 0.006794308312237263\n","Validation Loss after Epoch 207: 0.030164826040466625\n","Epoch 208/250, Batch 1/11, Loss: 0.006716840900480747\n","Epoch 208/250, Batch 11/11, Loss: 0.006453545764088631\n","Validation Loss after Epoch 208: 0.02549666166305542\n","Epoch 209/250, Batch 1/11, Loss: 0.005875500850379467\n","Epoch 209/250, Batch 11/11, Loss: 0.006829237565398216\n","Validation Loss after Epoch 209: 0.02304883549610774\n","Epoch 210/250, Batch 1/11, Loss: 0.006221646908670664\n","Epoch 210/250, Batch 11/11, Loss: 0.006731246132403612\n","Validation Loss after Epoch 210: 0.026525558282931645\n","Epoch 211/250, Batch 1/11, Loss: 0.005573248956352472\n","Epoch 211/250, Batch 11/11, Loss: 0.005049938336014748\n","Validation Loss after Epoch 211: 0.021643250559767086\n","Epoch 212/250, Batch 1/11, Loss: 0.007694140076637268\n","Epoch 212/250, Batch 11/11, Loss: 0.005483315326273441\n","Validation Loss after Epoch 212: 0.02735429381330808\n","Epoch 213/250, Batch 1/11, Loss: 0.005405189003795385\n","Epoch 213/250, Batch 11/11, Loss: 0.005094872321933508\n","Validation Loss after Epoch 213: 0.027115434408187866\n","Epoch 214/250, Batch 1/11, Loss: 0.0062247454188764095\n","Epoch 214/250, Batch 11/11, Loss: 0.005330027546733618\n","Validation Loss after Epoch 214: 0.025653650363286335\n","Epoch 215/250, Batch 1/11, Loss: 0.00551723875105381\n","Epoch 215/250, Batch 11/11, Loss: 0.005912578199058771\n","Validation Loss after Epoch 215: 0.028970540190736454\n","Epoch 216/250, Batch 1/11, Loss: 0.005597976036369801\n","Epoch 216/250, Batch 11/11, Loss: 0.005528929177671671\n","Validation Loss after Epoch 216: 0.030880197261770565\n","Epoch 217/250, Batch 1/11, Loss: 0.005452134180814028\n","Epoch 217/250, Batch 11/11, Loss: 0.005164479371160269\n","Validation Loss after Epoch 217: 0.03322182595729828\n","Epoch 218/250, Batch 1/11, Loss: 0.005668651312589645\n","Epoch 218/250, Batch 11/11, Loss: 0.005706696771085262\n","Validation Loss after Epoch 218: 0.023259427398443222\n","Epoch 219/250, Batch 1/11, Loss: 0.005670963786542416\n","Epoch 219/250, Batch 11/11, Loss: 0.0053403377532958984\n","Validation Loss after Epoch 219: 0.0297427245726188\n","Epoch 220/250, Batch 1/11, Loss: 0.005388450808823109\n","Epoch 220/250, Batch 11/11, Loss: 0.004946632776409388\n","Validation Loss after Epoch 220: 0.03331586470206579\n","Epoch 221/250, Batch 1/11, Loss: 0.004690676461905241\n","Epoch 221/250, Batch 11/11, Loss: 0.00679518049582839\n","Validation Loss after Epoch 221: 0.02393345534801483\n","Epoch 222/250, Batch 1/11, Loss: 0.005556080956012011\n","Epoch 222/250, Batch 11/11, Loss: 0.005377928260713816\n","Validation Loss after Epoch 222: 0.03999129682779312\n","Epoch 223/250, Batch 1/11, Loss: 0.005486280657351017\n","Epoch 223/250, Batch 11/11, Loss: 0.0062522548250854015\n","Validation Loss after Epoch 223: 0.030550658702850342\n","Epoch 224/250, Batch 1/11, Loss: 0.005284629762172699\n","Epoch 224/250, Batch 11/11, Loss: 0.005519940983504057\n","Validation Loss after Epoch 224: 0.03561915581425031\n","Epoch 225/250, Batch 1/11, Loss: 0.005145665258169174\n","Epoch 225/250, Batch 11/11, Loss: 0.00486248591914773\n","Validation Loss after Epoch 225: 0.024474068234364193\n","Epoch 226/250, Batch 1/11, Loss: 0.005155999679118395\n","Epoch 226/250, Batch 11/11, Loss: 0.005029846448451281\n","Validation Loss after Epoch 226: 0.031081892549991608\n","Epoch 227/250, Batch 1/11, Loss: 0.004788147285580635\n","Epoch 227/250, Batch 11/11, Loss: 0.007195261772722006\n","Validation Loss after Epoch 227: 0.02728652333219846\n","Epoch 228/250, Batch 1/11, Loss: 0.0044668265618383884\n","Epoch 228/250, Batch 11/11, Loss: 0.005448765587061644\n","Validation Loss after Epoch 228: 0.02263812596599261\n","Epoch 229/250, Batch 1/11, Loss: 0.004577642306685448\n","Epoch 229/250, Batch 11/11, Loss: 0.005530200432986021\n","Validation Loss after Epoch 229: 0.03253623843193054\n","Epoch 230/250, Batch 1/11, Loss: 0.005007801577448845\n","Epoch 230/250, Batch 11/11, Loss: 0.00555116031318903\n","Validation Loss after Epoch 230: 0.030473455786705017\n","Epoch 231/250, Batch 1/11, Loss: 0.004581061191856861\n","Epoch 231/250, Batch 11/11, Loss: 0.004803843330591917\n","Validation Loss after Epoch 231: 0.03066705788175265\n","Epoch 232/250, Batch 1/11, Loss: 0.004261248745024204\n","Epoch 232/250, Batch 11/11, Loss: 0.005117966793477535\n","Validation Loss after Epoch 232: 0.03158044504622618\n","Epoch 233/250, Batch 1/11, Loss: 0.005116964224725962\n","Epoch 233/250, Batch 11/11, Loss: 0.004177543800324202\n","Validation Loss after Epoch 233: 0.025411066909631092\n","Epoch 234/250, Batch 1/11, Loss: 0.004884528461843729\n","Epoch 234/250, Batch 11/11, Loss: 0.004778755363076925\n","Validation Loss after Epoch 234: 0.03743592773874601\n","Epoch 235/250, Batch 1/11, Loss: 0.004215540364384651\n","Epoch 235/250, Batch 11/11, Loss: 0.005950067657977343\n","Validation Loss after Epoch 235: 0.029742476840813954\n","Epoch 236/250, Batch 1/11, Loss: 0.005146829877048731\n","Epoch 236/250, Batch 11/11, Loss: 0.0053751785308122635\n","Validation Loss after Epoch 236: 0.03413333867986997\n","Epoch 237/250, Batch 1/11, Loss: 0.004640025086700916\n","Epoch 237/250, Batch 11/11, Loss: 0.009778639301657677\n","Validation Loss after Epoch 237: 0.03237072626749674\n","Epoch 238/250, Batch 1/11, Loss: 0.005615228787064552\n","Epoch 238/250, Batch 11/11, Loss: 0.004904480651021004\n","Validation Loss after Epoch 238: 0.022562623644868534\n","Epoch 239/250, Batch 1/11, Loss: 0.006200980860739946\n","Epoch 239/250, Batch 11/11, Loss: 0.0067315674386918545\n","Validation Loss after Epoch 239: 0.030387628823518753\n","Epoch 240/250, Batch 1/11, Loss: 0.0056066797114908695\n","Epoch 240/250, Batch 11/11, Loss: 0.00524937966838479\n","Validation Loss after Epoch 240: 0.03194333675007025\n","Epoch 241/250, Batch 1/11, Loss: 0.006474765948951244\n","Epoch 241/250, Batch 11/11, Loss: 0.004433172754943371\n","Validation Loss after Epoch 241: 0.027972933525840443\n","Epoch 242/250, Batch 1/11, Loss: 0.004156517796218395\n","Epoch 242/250, Batch 11/11, Loss: 0.00536321522668004\n","Validation Loss after Epoch 242: 0.028156540046135586\n","Epoch 243/250, Batch 1/11, Loss: 0.004356037359684706\n","Epoch 243/250, Batch 11/11, Loss: 0.005186520516872406\n","Validation Loss after Epoch 243: 0.02788424864411354\n","Epoch 244/250, Batch 1/11, Loss: 0.006161362864077091\n","Epoch 244/250, Batch 11/11, Loss: 0.005232052877545357\n","Validation Loss after Epoch 244: 0.04762533058722814\n","Epoch 245/250, Batch 1/11, Loss: 0.0055968803353607655\n","Epoch 245/250, Batch 11/11, Loss: 0.005090846214443445\n","Validation Loss after Epoch 245: 0.023016309986511867\n","Epoch 246/250, Batch 1/11, Loss: 0.004228868987411261\n","Epoch 246/250, Batch 11/11, Loss: 0.006006504874676466\n","Validation Loss after Epoch 246: 0.024661092087626457\n","Epoch 247/250, Batch 1/11, Loss: 0.004951665177941322\n","Epoch 247/250, Batch 11/11, Loss: 0.011464386247098446\n","Validation Loss after Epoch 247: 0.07029320547978084\n","Epoch 248/250, Batch 1/11, Loss: 0.016577977687120438\n","Epoch 248/250, Batch 11/11, Loss: 0.013990829698741436\n","Validation Loss after Epoch 248: 0.022085823118686676\n","Epoch 249/250, Batch 1/11, Loss: 0.01132351066917181\n","Epoch 249/250, Batch 11/11, Loss: 0.0074131968431174755\n","Validation Loss after Epoch 249: 0.03704637413223585\n","Epoch 250/250, Batch 1/11, Loss: 0.007491082884371281\n","Epoch 250/250, Batch 11/11, Loss: 0.006218728143721819\n","Validation Loss after Epoch 250: 0.022169047345717747\n","Subset size 500 - Test Loss: 0.0266, Test Accuracy: 99.15%, Average Dice Score: 0.9909\n"]}]}]}