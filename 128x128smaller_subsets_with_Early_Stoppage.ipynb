{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNInvOwTg9S4Ome4+MSa6qq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlZIzh7_h_bN","executionInfo":{"status":"ok","timestamp":1701183422988,"user_tz":-60,"elapsed":17519,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}},"outputId":"e460c787-bc57-4918-c11b-b3a203ca4f77"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","from torch.nn.functional import relu\n","import torch.nn.functional as F\n","\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import numpy as np\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, image_paths, label_paths):\n","        self.image_paths = image_paths\n","        self.label_paths = label_paths\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        # Load the image\n","        image = Image.open(self.image_paths[idx])\n","        np_image = np.array(image, dtype=np.float32)\n","\n","        # Normalize the image\n","        normalized_image = np_image / 65535.0  # For 16-bit images\n","\n","        # Load and process the label data\n","        label_image = Image.open(self.label_paths[idx])\n","        label_array = np.array(label_image, dtype=np.float32)\n","\n","        grayscale_to_class_mapping = {0: 0, 128: 1, 255: 2} # a set that maps gray-levels to a class\n","\n","        # Map grayscale values to class labels\n","        mapped_labels = np.copy(label_array)\n","        for grayscale_value, class_id in grayscale_to_class_mapping.items():\n","            mapped_labels[label_array == grayscale_value] = class_id\n","\n","        # Convert to PyTorch tensors\n","        image_tensor = torch.from_numpy(normalized_image).unsqueeze(0) # unsqueeze to enable channel dimension, was gone due to being a grayscale image\n","        label_tensor = torch.from_numpy(mapped_labels)\n","\n","        return image_tensor, label_tensor\n"],"metadata":{"id":"m95UJeDziH6e","executionInfo":{"status":"ok","timestamp":1701183430302,"user_tz":-60,"elapsed":5330,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["### Label images ###\n","# white class - 255 nickel\n","# gray class - 128 ysz\n","# black class - 0 pores\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_class):\n","        super().__init__()\n","\n","        # Define a helper function for creating a block\n","        def conv_block(in_channels, out_channels):\n","            return nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels),\n","                nn.ReLU(),\n","                nn.Dropout(p=0.1)\n","            )\n","\n","        # Encoder\n","        self.e11 = conv_block(1, 64)\n","        self.e12 = conv_block(64, 64)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.e21 = conv_block(64, 128)\n","        self.e22 = conv_block(128, 128)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.e31 = conv_block(128, 256)\n","        self.e32 = conv_block(256, 256)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.e41 = conv_block(256, 512)\n","        self.e42 = conv_block(512, 512)\n","        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.e51 = conv_block(512, 1024)\n","        self.e52 = conv_block(1024, 1024)\n","\n","        # Decoder\n","        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n","        self.d11 = conv_block(1024, 512)\n","        self.d12 = conv_block(512, 512)\n","\n","        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","        self.d21 = conv_block(512, 256)\n","        self.d22 = conv_block(256, 256)\n","\n","        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.d31 = conv_block(256, 128)\n","        self.d32 = conv_block(128, 128)\n","\n","        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.d41 = conv_block(128, 64)\n","        self.d42 = conv_block(64, 64)\n","\n","        # Output layer\n","        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n","\n","    def forward(self, x):\n","        # Encoder\n","        xe11 = self.e11(x)\n","        xe12 = self.e12(xe11)\n","        xp1 = self.pool1(xe12)\n","\n","        xe21 = self.e21(xp1)\n","        xe22 = self.e22(xe21)\n","        xp2 = self.pool2(xe22)\n","\n","        xe31 = self.e31(xp2)\n","        xe32 = self.e32(xe31)\n","        xp3 = self.pool3(xe32)\n","\n","        xe41 = self.e41(xp3)\n","        xe42 = self.e42(xe41)\n","        xp4 = self.pool4(xe42)\n","\n","        xe51 = self.e51(xp4)\n","        xe52 = self.e52(xe51)\n","\n","        # Decoder\n","        xu1 = self.upconv1(xe52)\n","        xu11 = torch.cat([xu1, xe42], dim=1)\n","        xd11 = self.d11(xu11)\n","        xd12 = self.d12(xd11)\n","\n","        xu2 = self.upconv2(xd12)\n","        xu22 = torch.cat([xu2, xe32], dim=1)\n","        xd21 = self.d21(xu22)\n","        xd22 = self.d22(xd21)\n","\n","        xu3 = self.upconv3(xd22)\n","        xu33 = torch.cat([xu3, xe22], dim=1)\n","        xd31 = self.d31(xu33)\n","        xd32 = self.d32(xd31)\n","\n","        xu4 = self.upconv4(xd32)\n","        xu44 = torch.cat([xu4, xe12], dim=1)\n","        xd41 = self.d41(xu44)\n","        xd42 = self.d42(xd41)\n","\n","        # Output layer\n","        out = self.outconv(xd42)\n","\n","        return out"],"metadata":{"id":"7Bc52XUliJtl","executionInfo":{"status":"ok","timestamp":1701183439654,"user_tz":-60,"elapsed":452,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, random_split\n","from torch import optim\n","import torch\n","import os\n","import matplotlib.pyplot as plt\n","\n","def dice_coefficient(predicted, target, num_classes):\n","    dice_scores = []  # To store dice coefficient for each class\n","\n","    # Convert predictions and targets to one-hot encoded form\n","    predicted_one_hot = F.one_hot(predicted, num_classes).permute(0, 3, 1, 2).float()\n","    target_one_hot = F.one_hot(target, num_classes).permute(0, 3, 1, 2).float()\n","\n","    # Calculate Dice coefficient for each class\n","    for class_index in range(num_classes):\n","        intersection = (predicted_one_hot[:, class_index, :, :] * target_one_hot[:, class_index, :, :]).sum()\n","        union = predicted_one_hot[:, class_index, :, :].sum() + target_one_hot[:, class_index, :, :].sum()\n","        dice_score = (2 * intersection + 1e-6) / (union + 1e-6)  # Adding a small epsilon to avoid division by zero\n","        dice_scores.append(dice_score)\n","\n","    # Average Dice score across all classes\n","    avg_dice_score = sum(dice_scores) / len(dice_scores)\n","    return avg_dice_score.item()  # Return the value as a Python scalar\n","\n","def get_image_paths(data_dir, label_dir):\n","    data_paths = [os.path.join(data_dir, img) for img in sorted(os.listdir(data_dir))]\n","    label_paths = [os.path.join(label_dir, lbl) for lbl in sorted(os.listdir(label_dir))]\n","    return data_paths, label_paths\n","\n","def create_subsets(dataset, subset_sizes):\n","    subsets = {}\n","    for size in subset_sizes:\n","        if size == len(dataset):\n","            subsets[size] = dataset  # Use the full dataset\n","        else:\n","            subset, _ = random_split(dataset, [size, len(dataset) - size])\n","            subsets[size] = subset\n","    return subsets\n"],"metadata":{"id":"tjYoI2YS8WpH","executionInfo":{"status":"ok","timestamp":1701183444190,"user_tz":-60,"elapsed":284,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Define your dataset paths\n","data_dir = '/content/gdrive/MyDrive/training_dataset/data_crop/'\n","label_dir = '/content/gdrive/MyDrive/training_dataset/label_crop/'\n","\n","# Get image paths and create the full dataset\n","image_paths, label_paths = get_image_paths(data_dir, label_dir)\n","dataset = CustomDataset(image_paths=image_paths, label_paths=label_paths)\n","\n","# Define subset sizes including the full dataset size\n","subset_sizes = [50, 125, 250, len(dataset)]  # Add the full dataset size\n","\n","# Create subsets\n","dataset_subsets = create_subsets(dataset, subset_sizes)\n","\n","# Device setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using {device}\")\n","\n","# Training configurations\n","learning_rate = 0.001\n","num_epochs = 250  # Adjust as needed\n","\n","# Define early stopping parameters\n","patience = 10  # Number of epochs to wait for improvement\n","min_delta = 0.0005  # Minimum change to signify an improvement\n","best_loss = float('inf')  # Initialize best loss to a high value\n","epochs_no_improve = 0  # Counter for epochs with no improvement\n","\n","# Loop over subsets and train the model\n","for size, subset in dataset_subsets.items():\n","    print(f\"\\nTraining on subset size: {size}\")\n","\n","    # Initialize early stopping parameters for each subset\n","    best_loss = float('inf')\n","    epochs_no_improve = 0\n","\n","    # Split the subset into training, validation, and test datasets\n","    train_size = int(0.70 * len(subset))\n","    val_size = int(0.15 * len(subset))\n","    test_size = len(subset) - train_size - val_size\n","    train_dataset, val_dataset, test_dataset = random_split(subset, [train_size, val_size, test_size])\n","\n","    # DataLoader setup\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","    # Model, loss function, and optimizer setup\n","    model = UNet(n_class=3).to(device)\n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for batch_idx, (images, labels) in enumerate(train_loader):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            labels = labels.squeeze(1).long()\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            if batch_idx % 10 == 0:\n","                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item()}\")\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            val_loss = 0\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                labels = labels.squeeze(1).long()\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","            val_loss /= len(val_loader)\n","            print(f\"Validation Loss after Epoch {epoch+1}: {val_loss}\")\n","\n","        # Early stopping logic\n","        if val_loss < best_loss - min_delta:\n","            best_loss = val_loss\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","\n","        if epochs_no_improve == patience:\n","            print(f\"Early stopping triggered at epoch {epoch+1}\")\n","            break\n","\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        test_loss = 0\n","        correct = 0\n","        total = 0\n","        dice_scores = []\n","\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            probabilities = F.softmax(outputs, dim=1)\n","            _, predicted = torch.max(probabilities, 1)\n","            labels = labels.squeeze(1).long()\n","\n","            loss = criterion(outputs, labels)\n","            test_loss += loss.item()\n","            total += labels.numel()\n","            correct += (predicted == labels).sum().item()\n","\n","            dice_score = dice_coefficient(predicted, labels, num_classes=3)\n","            dice_scores.append(dice_score)\n","\n","        test_loss /= len(test_loader)\n","        test_accuracy = 100 * correct / total\n","        average_dice_score = sum(dice_scores) / len(dice_scores)\n","\n","        print(f\"Subset size {size} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Average Dice Score: {average_dice_score:.4f}\")\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zbb85T-1iL01","executionInfo":{"status":"ok","timestamp":1701184013139,"user_tz":-60,"elapsed":564319,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}},"outputId":"b890054b-9df8-47e7-a39d-7ab50a046184"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda\n","\n","Training on subset size: 50\n","Epoch 1/250, Batch 1/2, Loss: 1.026074767112732\n","Validation Loss after Epoch 1: 1.129307508468628\n","Epoch 2/250, Batch 1/2, Loss: 0.32891735434532166\n","Validation Loss after Epoch 2: 1.1307401657104492\n","Epoch 3/250, Batch 1/2, Loss: 0.22636155784130096\n","Validation Loss after Epoch 3: 1.1267355680465698\n","Epoch 4/250, Batch 1/2, Loss: 0.1869436651468277\n","Validation Loss after Epoch 4: 1.163140058517456\n","Epoch 5/250, Batch 1/2, Loss: 0.16609235107898712\n","Validation Loss after Epoch 5: 1.276520848274231\n","Epoch 6/250, Batch 1/2, Loss: 0.15283140540122986\n","Validation Loss after Epoch 6: 1.3516277074813843\n","Epoch 7/250, Batch 1/2, Loss: 0.1448906809091568\n","Validation Loss after Epoch 7: 1.3861205577850342\n","Epoch 8/250, Batch 1/2, Loss: 0.12921003997325897\n","Validation Loss after Epoch 8: 1.4303234815597534\n","Epoch 9/250, Batch 1/2, Loss: 0.12107778340578079\n","Validation Loss after Epoch 9: 1.763514757156372\n","Epoch 10/250, Batch 1/2, Loss: 0.11733874678611755\n","Validation Loss after Epoch 10: 4.20636510848999\n","Epoch 11/250, Batch 1/2, Loss: 0.11027271300554276\n","Validation Loss after Epoch 11: 6.379006862640381\n","Epoch 12/250, Batch 1/2, Loss: 0.10170046985149384\n","Validation Loss after Epoch 12: 2.3481502532958984\n","Epoch 13/250, Batch 1/2, Loss: 0.09295923262834549\n","Validation Loss after Epoch 13: 0.8860384821891785\n","Epoch 14/250, Batch 1/2, Loss: 0.09028324484825134\n","Validation Loss after Epoch 14: 1.0616180896759033\n","Epoch 15/250, Batch 1/2, Loss: 0.0902230367064476\n","Validation Loss after Epoch 15: 0.9203811287879944\n","Epoch 16/250, Batch 1/2, Loss: 0.0842648446559906\n","Validation Loss after Epoch 16: 0.6569342613220215\n","Epoch 17/250, Batch 1/2, Loss: 0.07838952541351318\n","Validation Loss after Epoch 17: 0.6403409838676453\n","Epoch 18/250, Batch 1/2, Loss: 0.07477810233831406\n","Validation Loss after Epoch 18: 0.6269984841346741\n","Epoch 19/250, Batch 1/2, Loss: 0.07673074305057526\n","Validation Loss after Epoch 19: 0.711451530456543\n","Epoch 20/250, Batch 1/2, Loss: 0.07424122840166092\n","Validation Loss after Epoch 20: 0.9338251948356628\n","Epoch 21/250, Batch 1/2, Loss: 0.06835576146841049\n","Validation Loss after Epoch 21: 0.9141313433647156\n","Epoch 22/250, Batch 1/2, Loss: 0.06712399423122406\n","Validation Loss after Epoch 22: 0.9066987633705139\n","Epoch 23/250, Batch 1/2, Loss: 0.06219195947051048\n","Validation Loss after Epoch 23: 0.7760693430900574\n","Epoch 24/250, Batch 1/2, Loss: 0.06091517210006714\n","Validation Loss after Epoch 24: 0.6905665397644043\n","Epoch 25/250, Batch 1/2, Loss: 0.059472206979990005\n","Validation Loss after Epoch 25: 0.6251140832901001\n","Epoch 26/250, Batch 1/2, Loss: 0.060231249779462814\n","Validation Loss after Epoch 26: 0.581032931804657\n","Epoch 27/250, Batch 1/2, Loss: 0.06029883027076721\n","Validation Loss after Epoch 27: 0.5151248574256897\n","Epoch 28/250, Batch 1/2, Loss: 0.056774355471134186\n","Validation Loss after Epoch 28: 0.43125322461128235\n","Epoch 29/250, Batch 1/2, Loss: 0.05659182742238045\n","Validation Loss after Epoch 29: 0.3635628819465637\n","Epoch 30/250, Batch 1/2, Loss: 0.052719924598932266\n","Validation Loss after Epoch 30: 0.30641961097717285\n","Epoch 31/250, Batch 1/2, Loss: 0.04932059347629547\n","Validation Loss after Epoch 31: 0.24565206468105316\n","Epoch 32/250, Batch 1/2, Loss: 0.04816629737615585\n","Validation Loss after Epoch 32: 0.20471976697444916\n","Epoch 33/250, Batch 1/2, Loss: 0.04879763722419739\n","Validation Loss after Epoch 33: 0.2338174283504486\n","Epoch 34/250, Batch 1/2, Loss: 0.046494632959365845\n","Validation Loss after Epoch 34: 0.21079044044017792\n","Epoch 35/250, Batch 1/2, Loss: 0.049299437552690506\n","Validation Loss after Epoch 35: 0.14862509071826935\n","Epoch 36/250, Batch 1/2, Loss: 0.04524075984954834\n","Validation Loss after Epoch 36: 0.10000202804803848\n","Epoch 37/250, Batch 1/2, Loss: 0.04647573083639145\n","Validation Loss after Epoch 37: 0.0918414443731308\n","Epoch 38/250, Batch 1/2, Loss: 0.04654401168227196\n","Validation Loss after Epoch 38: 0.08426059037446976\n","Epoch 39/250, Batch 1/2, Loss: 0.04491214454174042\n","Validation Loss after Epoch 39: 0.09319663047790527\n","Epoch 40/250, Batch 1/2, Loss: 0.044753897935152054\n","Validation Loss after Epoch 40: 0.10957662016153336\n","Epoch 41/250, Batch 1/2, Loss: 0.04466307908296585\n","Validation Loss after Epoch 41: 0.11315163224935532\n","Epoch 42/250, Batch 1/2, Loss: 0.04240397363901138\n","Validation Loss after Epoch 42: 0.0981307253241539\n","Epoch 43/250, Batch 1/2, Loss: 0.041816454380750656\n","Validation Loss after Epoch 43: 0.08588789403438568\n","Epoch 44/250, Batch 1/2, Loss: 0.0429689921438694\n","Validation Loss after Epoch 44: 0.06682085990905762\n","Epoch 45/250, Batch 1/2, Loss: 0.04347657039761543\n","Validation Loss after Epoch 45: 0.05643549561500549\n","Epoch 46/250, Batch 1/2, Loss: 0.04344029724597931\n","Validation Loss after Epoch 46: 0.06138097122311592\n","Epoch 47/250, Batch 1/2, Loss: 0.04583117365837097\n","Validation Loss after Epoch 47: 0.07071922719478607\n","Epoch 48/250, Batch 1/2, Loss: 0.04739954695105553\n","Validation Loss after Epoch 48: 0.07786744087934494\n","Epoch 49/250, Batch 1/2, Loss: 0.04351022094488144\n","Validation Loss after Epoch 49: 0.06144721060991287\n","Epoch 50/250, Batch 1/2, Loss: 0.0427110493183136\n","Validation Loss after Epoch 50: 0.05295133963227272\n","Epoch 51/250, Batch 1/2, Loss: 0.041043274104595184\n","Validation Loss after Epoch 51: 0.051918286830186844\n","Epoch 52/250, Batch 1/2, Loss: 0.04392223805189133\n","Validation Loss after Epoch 52: 0.06875555962324142\n","Epoch 53/250, Batch 1/2, Loss: 0.0425993837416172\n","Validation Loss after Epoch 53: 0.07614269107580185\n","Epoch 54/250, Batch 1/2, Loss: 0.041471436619758606\n","Validation Loss after Epoch 54: 0.06016288325190544\n","Epoch 55/250, Batch 1/2, Loss: 0.042474910616874695\n","Validation Loss after Epoch 55: 0.052967216819524765\n","Epoch 56/250, Batch 1/2, Loss: 0.04310138523578644\n","Validation Loss after Epoch 56: 0.05201021209359169\n","Epoch 57/250, Batch 1/2, Loss: 0.039538852870464325\n","Validation Loss after Epoch 57: 0.05135760083794594\n","Epoch 58/250, Batch 1/2, Loss: 0.038992103189229965\n","Validation Loss after Epoch 58: 0.05342843383550644\n","Epoch 59/250, Batch 1/2, Loss: 0.03748711943626404\n","Validation Loss after Epoch 59: 0.05260743945837021\n","Epoch 60/250, Batch 1/2, Loss: 0.040292609483003616\n","Validation Loss after Epoch 60: 0.051045093685388565\n","Epoch 61/250, Batch 1/2, Loss: 0.03921644389629364\n","Validation Loss after Epoch 61: 0.05697329714894295\n","Epoch 62/250, Batch 1/2, Loss: 0.04008081555366516\n","Validation Loss after Epoch 62: 0.06950002163648605\n","Epoch 63/250, Batch 1/2, Loss: 0.0381305105984211\n","Validation Loss after Epoch 63: 0.06220211461186409\n","Epoch 64/250, Batch 1/2, Loss: 0.035147298127412796\n","Validation Loss after Epoch 64: 0.05021372437477112\n","Epoch 65/250, Batch 1/2, Loss: 0.038151226937770844\n","Validation Loss after Epoch 65: 0.04830498248338699\n","Epoch 66/250, Batch 1/2, Loss: 0.03724515438079834\n","Validation Loss after Epoch 66: 0.04705105349421501\n","Epoch 67/250, Batch 1/2, Loss: 0.03481012582778931\n","Validation Loss after Epoch 67: 0.04714963212609291\n","Epoch 68/250, Batch 1/2, Loss: 0.034150630235672\n","Validation Loss after Epoch 68: 0.05939839407801628\n","Epoch 69/250, Batch 1/2, Loss: 0.033458683639764786\n","Validation Loss after Epoch 69: 0.07585663348436356\n","Epoch 70/250, Batch 1/2, Loss: 0.033360738307237625\n","Validation Loss after Epoch 70: 0.06590040028095245\n","Epoch 71/250, Batch 1/2, Loss: 0.03381005674600601\n","Validation Loss after Epoch 71: 0.060002539306879044\n","Epoch 72/250, Batch 1/2, Loss: 0.03336343541741371\n","Validation Loss after Epoch 72: 0.057580530643463135\n","Epoch 73/250, Batch 1/2, Loss: 0.03201070800423622\n","Validation Loss after Epoch 73: 0.050767626613378525\n","Epoch 74/250, Batch 1/2, Loss: 0.033532001078128815\n","Validation Loss after Epoch 74: 0.04818873852491379\n","Epoch 75/250, Batch 1/2, Loss: 0.03726150095462799\n","Validation Loss after Epoch 75: 0.04830488562583923\n","Epoch 76/250, Batch 1/2, Loss: 0.033644743263721466\n","Validation Loss after Epoch 76: 0.051111575216054916\n","Early stopping triggered at epoch 76\n","Subset size 50 - Test Loss: 0.0366, Test Accuracy: 98.77%, Average Dice Score: 0.9868\n","\n","Training on subset size: 125\n","Epoch 1/250, Batch 1/3, Loss: 1.0961244106292725\n","Validation Loss after Epoch 1: 1.0935769081115723\n","Epoch 2/250, Batch 1/3, Loss: 0.24462421238422394\n","Validation Loss after Epoch 2: 1.1176105737686157\n","Epoch 3/250, Batch 1/3, Loss: 0.16816769540309906\n","Validation Loss after Epoch 3: 1.1767990589141846\n","Epoch 4/250, Batch 1/3, Loss: 0.14195865392684937\n","Validation Loss after Epoch 4: 1.3332345485687256\n","Epoch 5/250, Batch 1/3, Loss: 0.12872320413589478\n","Validation Loss after Epoch 5: 1.125590205192566\n","Epoch 6/250, Batch 1/3, Loss: 0.10957151651382446\n","Validation Loss after Epoch 6: 0.9335709810256958\n","Epoch 7/250, Batch 1/3, Loss: 0.0998840481042862\n","Validation Loss after Epoch 7: 0.8905506134033203\n","Epoch 8/250, Batch 1/3, Loss: 0.08981560915708542\n","Validation Loss after Epoch 8: 0.8171641826629639\n","Epoch 9/250, Batch 1/3, Loss: 0.0891774371266365\n","Validation Loss after Epoch 9: 0.7966582179069519\n","Epoch 10/250, Batch 1/3, Loss: 0.0752599835395813\n","Validation Loss after Epoch 10: 0.8387280702590942\n","Epoch 11/250, Batch 1/3, Loss: 0.08044504374265671\n","Validation Loss after Epoch 11: 0.8797644972801208\n","Epoch 12/250, Batch 1/3, Loss: 0.06792223453521729\n","Validation Loss after Epoch 12: 0.8386015295982361\n","Epoch 13/250, Batch 1/3, Loss: 0.06707919389009476\n","Validation Loss after Epoch 13: 0.7476927042007446\n","Epoch 14/250, Batch 1/3, Loss: 0.0614604651927948\n","Validation Loss after Epoch 14: 0.7481135725975037\n","Epoch 15/250, Batch 1/3, Loss: 0.05638718232512474\n","Validation Loss after Epoch 15: 0.7440711855888367\n","Epoch 16/250, Batch 1/3, Loss: 0.056460704654455185\n","Validation Loss after Epoch 16: 0.7868110537528992\n","Epoch 17/250, Batch 1/3, Loss: 0.05437198281288147\n","Validation Loss after Epoch 17: 0.7225704789161682\n","Epoch 18/250, Batch 1/3, Loss: 0.05248668044805527\n","Validation Loss after Epoch 18: 0.6180171370506287\n","Epoch 19/250, Batch 1/3, Loss: 0.05031299218535423\n","Validation Loss after Epoch 19: 0.3196501135826111\n","Epoch 20/250, Batch 1/3, Loss: 0.04797430709004402\n","Validation Loss after Epoch 20: 0.19302654266357422\n","Epoch 21/250, Batch 1/3, Loss: 0.04395996406674385\n","Validation Loss after Epoch 21: 0.16187351942062378\n","Epoch 22/250, Batch 1/3, Loss: 0.04831219092011452\n","Validation Loss after Epoch 22: 0.0770246684551239\n","Epoch 23/250, Batch 1/3, Loss: 0.04580799117684364\n","Validation Loss after Epoch 23: 0.06330732256174088\n","Epoch 24/250, Batch 1/3, Loss: 0.044510867446660995\n","Validation Loss after Epoch 24: 0.06796527653932571\n","Epoch 25/250, Batch 1/3, Loss: 0.043134916573762894\n","Validation Loss after Epoch 25: 0.05999569222331047\n","Epoch 26/250, Batch 1/3, Loss: 0.043134063482284546\n","Validation Loss after Epoch 26: 0.05148167535662651\n","Epoch 27/250, Batch 1/3, Loss: 0.03939089551568031\n","Validation Loss after Epoch 27: 0.049103718250989914\n","Epoch 28/250, Batch 1/3, Loss: 0.03668622300028801\n","Validation Loss after Epoch 28: 0.04940159246325493\n","Epoch 29/250, Batch 1/3, Loss: 0.03820279613137245\n","Validation Loss after Epoch 29: 0.047798916697502136\n","Epoch 30/250, Batch 1/3, Loss: 0.045565757900476456\n","Validation Loss after Epoch 30: 0.041704628616571426\n","Epoch 31/250, Batch 1/3, Loss: 0.036347564309835434\n","Validation Loss after Epoch 31: 0.04573611542582512\n","Epoch 32/250, Batch 1/3, Loss: 0.034990157932043076\n","Validation Loss after Epoch 32: 0.0437551885843277\n","Epoch 33/250, Batch 1/3, Loss: 0.040026210248470306\n","Validation Loss after Epoch 33: 0.042632684111595154\n","Epoch 34/250, Batch 1/3, Loss: 0.035730257630348206\n","Validation Loss after Epoch 34: 0.03810393810272217\n","Epoch 35/250, Batch 1/3, Loss: 0.03696811571717262\n","Validation Loss after Epoch 35: 0.03787378594279289\n","Epoch 36/250, Batch 1/3, Loss: 0.03361348435282707\n","Validation Loss after Epoch 36: 0.043240997940301895\n","Epoch 37/250, Batch 1/3, Loss: 0.03274444490671158\n","Validation Loss after Epoch 37: 0.038757771253585815\n","Epoch 38/250, Batch 1/3, Loss: 0.03428336977958679\n","Validation Loss after Epoch 38: 0.04331425204873085\n","Epoch 39/250, Batch 1/3, Loss: 0.03277848660945892\n","Validation Loss after Epoch 39: 0.046373385936021805\n","Epoch 40/250, Batch 1/3, Loss: 0.03128661960363388\n","Validation Loss after Epoch 40: 0.04476410895586014\n","Epoch 41/250, Batch 1/3, Loss: 0.03433115780353546\n","Validation Loss after Epoch 41: 0.04350309073925018\n","Epoch 42/250, Batch 1/3, Loss: 0.031431954354047775\n","Validation Loss after Epoch 42: 0.03812714293599129\n","Epoch 43/250, Batch 1/3, Loss: 0.032076966017484665\n","Validation Loss after Epoch 43: 0.03969459980726242\n","Epoch 44/250, Batch 1/3, Loss: 0.031168680638074875\n","Validation Loss after Epoch 44: 0.03787510469555855\n","Early stopping triggered at epoch 44\n","Subset size 125 - Test Loss: 0.0362, Test Accuracy: 98.86%, Average Dice Score: 0.9881\n","\n","Training on subset size: 250\n","Epoch 1/250, Batch 1/6, Loss: 1.1547554731369019\n","Validation Loss after Epoch 1: 1.0732880234718323\n","Epoch 2/250, Batch 1/6, Loss: 0.24142016470432281\n","Validation Loss after Epoch 2: 1.2700371146202087\n","Epoch 3/250, Batch 1/6, Loss: 0.20119404792785645\n","Validation Loss after Epoch 3: 0.9467286467552185\n","Epoch 4/250, Batch 1/6, Loss: 0.1513611078262329\n","Validation Loss after Epoch 4: 0.8956794142723083\n","Epoch 5/250, Batch 1/6, Loss: 0.11860384792089462\n","Validation Loss after Epoch 5: 0.8819423019886017\n","Epoch 6/250, Batch 1/6, Loss: 0.09978951513767242\n","Validation Loss after Epoch 6: 0.874469667673111\n","Epoch 7/250, Batch 1/6, Loss: 0.09417304396629333\n","Validation Loss after Epoch 7: 0.8487744033336639\n","Epoch 8/250, Batch 1/6, Loss: 0.07264362275600433\n","Validation Loss after Epoch 8: 0.7341555953025818\n","Epoch 9/250, Batch 1/6, Loss: 0.0702282190322876\n","Validation Loss after Epoch 9: 0.5174355804920197\n","Epoch 10/250, Batch 1/6, Loss: 0.06713476032018661\n","Validation Loss after Epoch 10: 0.3321787714958191\n","Epoch 11/250, Batch 1/6, Loss: 0.05699240043759346\n","Validation Loss after Epoch 11: 0.12942104786634445\n","Epoch 12/250, Batch 1/6, Loss: 0.05807426571846008\n","Validation Loss after Epoch 12: 0.08323033899068832\n","Epoch 13/250, Batch 1/6, Loss: 0.05116717889904976\n","Validation Loss after Epoch 13: 0.056051772087812424\n","Epoch 14/250, Batch 1/6, Loss: 0.0499233640730381\n","Validation Loss after Epoch 14: 0.05704668164253235\n","Epoch 15/250, Batch 1/6, Loss: 0.0462779626250267\n","Validation Loss after Epoch 15: 0.045587753877043724\n","Epoch 16/250, Batch 1/6, Loss: 0.044467344880104065\n","Validation Loss after Epoch 16: 0.06380901299417019\n","Epoch 17/250, Batch 1/6, Loss: 0.04658808931708336\n","Validation Loss after Epoch 17: 0.043766336515545845\n","Epoch 18/250, Batch 1/6, Loss: 0.04288313165307045\n","Validation Loss after Epoch 18: 0.040513280779123306\n","Epoch 19/250, Batch 1/6, Loss: 0.03908037394285202\n","Validation Loss after Epoch 19: 0.04459019564092159\n","Epoch 20/250, Batch 1/6, Loss: 0.04067162796854973\n","Validation Loss after Epoch 20: 0.04105152375996113\n","Epoch 21/250, Batch 1/6, Loss: 0.04681643098592758\n","Validation Loss after Epoch 21: 0.04249241761863232\n","Epoch 22/250, Batch 1/6, Loss: 0.043397024273872375\n","Validation Loss after Epoch 22: 0.043303946033120155\n","Epoch 23/250, Batch 1/6, Loss: 0.035129107534885406\n","Validation Loss after Epoch 23: 0.04607514478266239\n","Epoch 24/250, Batch 1/6, Loss: 0.040618009865283966\n","Validation Loss after Epoch 24: 0.0451793409883976\n","Epoch 25/250, Batch 1/6, Loss: 0.034611284732818604\n","Validation Loss after Epoch 25: 0.041852645576000214\n","Epoch 26/250, Batch 1/6, Loss: 0.03536326438188553\n","Validation Loss after Epoch 26: 0.0417634304612875\n","Epoch 27/250, Batch 1/6, Loss: 0.03329458832740784\n","Validation Loss after Epoch 27: 0.03451437130570412\n","Epoch 28/250, Batch 1/6, Loss: 0.03632892668247223\n","Validation Loss after Epoch 28: 0.03689638338983059\n","Epoch 29/250, Batch 1/6, Loss: 0.03338557481765747\n","Validation Loss after Epoch 29: 0.035321786999702454\n","Epoch 30/250, Batch 1/6, Loss: 0.030384352430701256\n","Validation Loss after Epoch 30: 0.03151331190019846\n","Epoch 31/250, Batch 1/6, Loss: 0.031587231904268265\n","Validation Loss after Epoch 31: 0.03993871808052063\n","Epoch 32/250, Batch 1/6, Loss: 0.03436579182744026\n","Validation Loss after Epoch 32: 0.031073560938239098\n","Epoch 33/250, Batch 1/6, Loss: 0.03138638660311699\n","Validation Loss after Epoch 33: 0.03690350241959095\n","Epoch 34/250, Batch 1/6, Loss: 0.03086303174495697\n","Validation Loss after Epoch 34: 0.03494511544704437\n","Epoch 35/250, Batch 1/6, Loss: 0.0299320500344038\n","Validation Loss after Epoch 35: 0.046065863221883774\n","Epoch 36/250, Batch 1/6, Loss: 0.02848123572766781\n","Validation Loss after Epoch 36: 0.03175242431461811\n","Epoch 37/250, Batch 1/6, Loss: 0.03252379223704338\n","Validation Loss after Epoch 37: 0.03847595863044262\n","Epoch 38/250, Batch 1/6, Loss: 0.033664725720882416\n","Validation Loss after Epoch 38: 0.030473162420094013\n","Epoch 39/250, Batch 1/6, Loss: 0.03211497515439987\n","Validation Loss after Epoch 39: 0.03782540746033192\n","Epoch 40/250, Batch 1/6, Loss: 0.028130080550909042\n","Validation Loss after Epoch 40: 0.02920833695679903\n","Epoch 41/250, Batch 1/6, Loss: 0.027333296835422516\n","Validation Loss after Epoch 41: 0.03417879715561867\n","Epoch 42/250, Batch 1/6, Loss: 0.028726648539304733\n","Validation Loss after Epoch 42: 0.02855977974832058\n","Epoch 43/250, Batch 1/6, Loss: 0.033473316580057144\n","Validation Loss after Epoch 43: 0.032337295822799206\n","Epoch 44/250, Batch 1/6, Loss: 0.030138850212097168\n","Validation Loss after Epoch 44: 0.03217565268278122\n","Epoch 45/250, Batch 1/6, Loss: 0.02660970203578472\n","Validation Loss after Epoch 45: 0.029158063232898712\n","Epoch 46/250, Batch 1/6, Loss: 0.027117101475596428\n","Validation Loss after Epoch 46: 0.03244645334780216\n","Epoch 47/250, Batch 1/6, Loss: 0.027544232085347176\n","Validation Loss after Epoch 47: 0.027683647349476814\n","Epoch 48/250, Batch 1/6, Loss: 0.025015629827976227\n","Validation Loss after Epoch 48: 0.030038105323910713\n","Epoch 49/250, Batch 1/6, Loss: 0.025884684175252914\n","Validation Loss after Epoch 49: 0.02915783692151308\n","Epoch 50/250, Batch 1/6, Loss: 0.02340303547680378\n","Validation Loss after Epoch 50: 0.032191723585128784\n","Epoch 51/250, Batch 1/6, Loss: 0.02970738336443901\n","Validation Loss after Epoch 51: 0.029407992027699947\n","Epoch 52/250, Batch 1/6, Loss: 0.024524157866835594\n","Validation Loss after Epoch 52: 0.028100225143134594\n","Epoch 53/250, Batch 1/6, Loss: 0.025462770834565163\n","Validation Loss after Epoch 53: 0.030026482418179512\n","Epoch 54/250, Batch 1/6, Loss: 0.025971105322241783\n","Validation Loss after Epoch 54: 0.027406577952206135\n","Epoch 55/250, Batch 1/6, Loss: 0.02685687504708767\n","Validation Loss after Epoch 55: 0.03065716102719307\n","Epoch 56/250, Batch 1/6, Loss: 0.024318816140294075\n","Validation Loss after Epoch 56: 0.02861776389181614\n","Epoch 57/250, Batch 1/6, Loss: 0.02630355767905712\n","Validation Loss after Epoch 57: 0.02919277548789978\n","Early stopping triggered at epoch 57\n","Subset size 250 - Test Loss: 0.0296, Test Accuracy: 99.01%, Average Dice Score: 0.9888\n","\n","Training on subset size: 500\n","Epoch 1/250, Batch 1/11, Loss: 1.1929223537445068\n","Epoch 1/250, Batch 11/11, Loss: 0.15557001531124115\n","Validation Loss after Epoch 1: 1.2656514247258503\n","Epoch 2/250, Batch 1/11, Loss: 0.15531235933303833\n","Epoch 2/250, Batch 11/11, Loss: 0.1102895513176918\n","Validation Loss after Epoch 2: 1.359310547510783\n","Epoch 3/250, Batch 1/11, Loss: 0.11188160628080368\n","Epoch 3/250, Batch 11/11, Loss: 0.07599040120840073\n","Validation Loss after Epoch 3: 0.9507083694140116\n","Epoch 4/250, Batch 1/11, Loss: 0.07599970698356628\n","Epoch 4/250, Batch 11/11, Loss: 0.05958547815680504\n","Validation Loss after Epoch 4: 0.8815802534421285\n","Epoch 5/250, Batch 1/11, Loss: 0.07438766956329346\n","Epoch 5/250, Batch 11/11, Loss: 0.05225824564695358\n","Validation Loss after Epoch 5: 0.4152871072292328\n","Epoch 6/250, Batch 1/11, Loss: 0.06193677335977554\n","Epoch 6/250, Batch 11/11, Loss: 0.051115650683641434\n","Validation Loss after Epoch 6: 0.0702956145008405\n","Epoch 7/250, Batch 1/11, Loss: 0.05799690634012222\n","Epoch 7/250, Batch 11/11, Loss: 0.04629087448120117\n","Validation Loss after Epoch 7: 0.06486110637585323\n","Epoch 8/250, Batch 1/11, Loss: 0.04949892684817314\n","Epoch 8/250, Batch 11/11, Loss: 0.041901908814907074\n","Validation Loss after Epoch 8: 0.04700159405668577\n","Epoch 9/250, Batch 1/11, Loss: 0.04463798180222511\n","Epoch 9/250, Batch 11/11, Loss: 0.044979289174079895\n","Validation Loss after Epoch 9: 0.039216636369625725\n","Epoch 10/250, Batch 1/11, Loss: 0.04425562918186188\n","Epoch 10/250, Batch 11/11, Loss: 0.03978007286787033\n","Validation Loss after Epoch 10: 0.04210399091243744\n","Epoch 11/250, Batch 1/11, Loss: 0.03905375674366951\n","Epoch 11/250, Batch 11/11, Loss: 0.036504458636045456\n","Validation Loss after Epoch 11: 0.03446849808096886\n","Epoch 12/250, Batch 1/11, Loss: 0.035743359476327896\n","Epoch 12/250, Batch 11/11, Loss: 0.03239888697862625\n","Validation Loss after Epoch 12: 0.037167457242806755\n","Epoch 13/250, Batch 1/11, Loss: 0.04684384539723396\n","Epoch 13/250, Batch 11/11, Loss: 0.03203872963786125\n","Validation Loss after Epoch 13: 0.031237084418535233\n","Epoch 14/250, Batch 1/11, Loss: 0.03175605833530426\n","Epoch 14/250, Batch 11/11, Loss: 0.03296233341097832\n","Validation Loss after Epoch 14: 0.032797942558924355\n","Epoch 15/250, Batch 1/11, Loss: 0.03283853083848953\n","Epoch 15/250, Batch 11/11, Loss: 0.03214895725250244\n","Validation Loss after Epoch 15: 0.0330831923832496\n","Epoch 16/250, Batch 1/11, Loss: 0.033265918493270874\n","Epoch 16/250, Batch 11/11, Loss: 0.03749886900186539\n","Validation Loss after Epoch 16: 0.030621012672781944\n","Epoch 17/250, Batch 1/11, Loss: 0.031208066269755363\n","Epoch 17/250, Batch 11/11, Loss: 0.031983260065317154\n","Validation Loss after Epoch 17: 0.03359138717254003\n","Epoch 18/250, Batch 1/11, Loss: 0.03036436066031456\n","Epoch 18/250, Batch 11/11, Loss: 0.03708205744624138\n","Validation Loss after Epoch 18: 0.0350550984342893\n","Epoch 19/250, Batch 1/11, Loss: 0.03243763744831085\n","Epoch 19/250, Batch 11/11, Loss: 0.03347601369023323\n","Validation Loss after Epoch 19: 0.031444864347577095\n","Epoch 20/250, Batch 1/11, Loss: 0.02848013862967491\n","Epoch 20/250, Batch 11/11, Loss: 0.03558794781565666\n","Validation Loss after Epoch 20: 0.029130579282840092\n","Epoch 21/250, Batch 1/11, Loss: 0.030527204275131226\n","Epoch 21/250, Batch 11/11, Loss: 0.05041368678212166\n","Validation Loss after Epoch 21: 0.02984628515938918\n","Epoch 22/250, Batch 1/11, Loss: 0.030189283192157745\n","Epoch 22/250, Batch 11/11, Loss: 0.029463617131114006\n","Validation Loss after Epoch 22: 0.029683703556656837\n","Epoch 23/250, Batch 1/11, Loss: 0.029615627601742744\n","Epoch 23/250, Batch 11/11, Loss: 0.026638230308890343\n","Validation Loss after Epoch 23: 0.03145704294244448\n","Epoch 24/250, Batch 1/11, Loss: 0.02703935280442238\n","Epoch 24/250, Batch 11/11, Loss: 0.03945460543036461\n","Validation Loss after Epoch 24: 0.029052807639042538\n","Epoch 25/250, Batch 1/11, Loss: 0.029866980388760567\n","Epoch 25/250, Batch 11/11, Loss: 0.02450447715818882\n","Validation Loss after Epoch 25: 0.027091948315501213\n","Epoch 26/250, Batch 1/11, Loss: 0.025928214192390442\n","Epoch 26/250, Batch 11/11, Loss: 0.028717445209622383\n","Validation Loss after Epoch 26: 0.031395340959231056\n","Epoch 27/250, Batch 1/11, Loss: 0.023933732882142067\n","Epoch 27/250, Batch 11/11, Loss: 0.028519639745354652\n","Validation Loss after Epoch 27: 0.02828482910990715\n","Epoch 28/250, Batch 1/11, Loss: 0.023276252672076225\n","Epoch 28/250, Batch 11/11, Loss: 0.027282316237688065\n","Validation Loss after Epoch 28: 0.030837299923102062\n","Epoch 29/250, Batch 1/11, Loss: 0.023251548409461975\n","Epoch 29/250, Batch 11/11, Loss: 0.024551769718527794\n","Validation Loss after Epoch 29: 0.027648425350586574\n","Epoch 30/250, Batch 1/11, Loss: 0.03029298037290573\n","Epoch 30/250, Batch 11/11, Loss: 0.027335381135344505\n","Validation Loss after Epoch 30: 0.032190155858794846\n","Epoch 31/250, Batch 1/11, Loss: 0.026416564360260963\n","Epoch 31/250, Batch 11/11, Loss: 0.03180594742298126\n","Validation Loss after Epoch 31: 0.03916318093736967\n","Epoch 32/250, Batch 1/11, Loss: 0.022415615618228912\n","Epoch 32/250, Batch 11/11, Loss: 0.02420857734978199\n","Validation Loss after Epoch 32: 0.04582716152071953\n","Epoch 33/250, Batch 1/11, Loss: 0.02426922507584095\n","Epoch 33/250, Batch 11/11, Loss: 0.02677239291369915\n","Validation Loss after Epoch 33: 0.03387712190548579\n","Epoch 34/250, Batch 1/11, Loss: 0.02511717565357685\n","Epoch 34/250, Batch 11/11, Loss: 0.023545313626527786\n","Validation Loss after Epoch 34: 0.029503239939610165\n","Epoch 35/250, Batch 1/11, Loss: 0.025128383189439774\n","Epoch 35/250, Batch 11/11, Loss: 0.026213068515062332\n","Validation Loss after Epoch 35: 0.03117828567822774\n","Early stopping triggered at epoch 35\n","Subset size 500 - Test Loss: 0.0285, Test Accuracy: 98.99%, Average Dice Score: 0.9890\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '128x128smaller_subsets_with_Early_Stoppage.pth')"],"metadata":{"id":"OiQm06_ieLyq","executionInfo":{"status":"ok","timestamp":1701184080758,"user_tz":-60,"elapsed":663,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}}},"execution_count":6,"outputs":[]}]}