{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1jcBIc2beLBWq6OpNIeiu0Q92OSmEKN4X","timestamp":1701180284370}],"gpuType":"T4","authorship_tag":"ABX9TyNw7IKQ7uerzZWdKFQAbhZI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z_shzULYCMNO","executionInfo":{"status":"ok","timestamp":1701184189752,"user_tz":-60,"elapsed":29523,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}},"outputId":"6be55cdb-e225-4d09-8a90-6b712de2bd8c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","from torch.nn.functional import relu\n","import torch.nn.functional as F\n","\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import numpy as np\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, image_paths, label_paths):\n","        self.image_paths = image_paths\n","        self.label_paths = label_paths\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        # Load the image\n","        image = Image.open(self.image_paths[idx])\n","        np_image = np.array(image, dtype=np.float32)\n","\n","        # Normalize the image\n","        normalized_image = np_image / 65535.0  # For 16-bit images\n","\n","        # Load and process the label data\n","        label_image = Image.open(self.label_paths[idx])\n","        label_array = np.array(label_image, dtype=np.float32)\n","\n","        grayscale_to_class_mapping = {0: 0, 128: 1, 255: 2} # a set that maps gray-levels to a class\n","\n","        # Map grayscale values to class labels\n","        mapped_labels = np.copy(label_array)\n","        for grayscale_value, class_id in grayscale_to_class_mapping.items():\n","            mapped_labels[label_array == grayscale_value] = class_id\n","\n","        # Convert to PyTorch tensors\n","        image_tensor = torch.from_numpy(normalized_image).unsqueeze(0) # unsqueeze to enable channel dimension, was gone due to being a grayscale image\n","        label_tensor = torch.from_numpy(mapped_labels)\n","\n","        return image_tensor, label_tensor\n"],"metadata":{"id":"SsaNvjjhCMLH","executionInfo":{"status":"ok","timestamp":1701184197116,"user_tz":-60,"elapsed":7368,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["### Label images ###\n","# white class - 255 nickel\n","# gray class - 128 ysz\n","# black class - 0 pores\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_class):\n","        super().__init__()\n","\n","        # Define a helper function for creating a block\n","        def conv_block(in_channels, out_channels):\n","            return nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels),\n","                nn.ReLU(),\n","                nn.Dropout(p=0.1)\n","            )\n","\n","        # Encoder\n","        self.e11 = conv_block(1, 64)\n","        self.e12 = conv_block(64, 64)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.e21 = conv_block(64, 128)\n","        self.e22 = conv_block(128, 128)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.e31 = conv_block(128, 256)\n","        self.e32 = conv_block(256, 256)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.e41 = conv_block(256, 512)\n","        self.e42 = conv_block(512, 512)\n","        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.e51 = conv_block(512, 1024)\n","        self.e52 = conv_block(1024, 1024)\n","\n","        # Decoder\n","        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n","        self.d11 = conv_block(1024, 512)\n","        self.d12 = conv_block(512, 512)\n","\n","        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","        self.d21 = conv_block(512, 256)\n","        self.d22 = conv_block(256, 256)\n","\n","        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.d31 = conv_block(256, 128)\n","        self.d32 = conv_block(128, 128)\n","\n","        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.d41 = conv_block(128, 64)\n","        self.d42 = conv_block(64, 64)\n","\n","        # Output layer\n","        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n","\n","    def forward(self, x):\n","        # Encoder\n","        xe11 = self.e11(x)\n","        xe12 = self.e12(xe11)\n","        xp1 = self.pool1(xe12)\n","\n","        xe21 = self.e21(xp1)\n","        xe22 = self.e22(xe21)\n","        xp2 = self.pool2(xe22)\n","\n","        xe31 = self.e31(xp2)\n","        xe32 = self.e32(xe31)\n","        xp3 = self.pool3(xe32)\n","\n","        xe41 = self.e41(xp3)\n","        xe42 = self.e42(xe41)\n","        xp4 = self.pool4(xe42)\n","\n","        xe51 = self.e51(xp4)\n","        xe52 = self.e52(xe51)\n","\n","        # Decoder\n","        xu1 = self.upconv1(xe52)\n","        xu11 = torch.cat([xu1, xe42], dim=1)\n","        xd11 = self.d11(xu11)\n","        xd12 = self.d12(xd11)\n","\n","        xu2 = self.upconv2(xd12)\n","        xu22 = torch.cat([xu2, xe32], dim=1)\n","        xd21 = self.d21(xu22)\n","        xd22 = self.d22(xd21)\n","\n","        xu3 = self.upconv3(xd22)\n","        xu33 = torch.cat([xu3, xe22], dim=1)\n","        xd31 = self.d31(xu33)\n","        xd32 = self.d32(xd31)\n","\n","        xu4 = self.upconv4(xd32)\n","        xu44 = torch.cat([xu4, xe12], dim=1)\n","        xd41 = self.d41(xu44)\n","        xd42 = self.d42(xd41)\n","\n","        # Output layer\n","        out = self.outconv(xd42)\n","\n","        return out"],"metadata":{"id":"28Q8Ds-fCMIt","executionInfo":{"status":"ok","timestamp":1701184197935,"user_tz":-60,"elapsed":10,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, random_split\n","from torch import optim\n","import torch\n","import os\n","import matplotlib.pyplot as plt\n","\n","def dice_coefficient(predicted, target, num_classes):\n","    dice_scores = []  # To store dice coefficient for each class\n","\n","    # Convert predictions and targets to one-hot encoded form\n","    predicted_one_hot = F.one_hot(predicted, num_classes).permute(0, 3, 1, 2).float()\n","    target_one_hot = F.one_hot(target, num_classes).permute(0, 3, 1, 2).float()\n","\n","    # Calculate Dice coefficient for each class\n","    for class_index in range(num_classes):\n","        intersection = (predicted_one_hot[:, class_index, :, :] * target_one_hot[:, class_index, :, :]).sum()\n","        union = predicted_one_hot[:, class_index, :, :].sum() + target_one_hot[:, class_index, :, :].sum()\n","        dice_score = (2 * intersection + 1e-6) / (union + 1e-6)  # Adding a small epsilon to avoid division by zero\n","        dice_scores.append(dice_score)\n","\n","    # Average Dice score across all classes\n","    avg_dice_score = sum(dice_scores) / len(dice_scores)\n","    return avg_dice_score.item()  # Return the value as a Python scalar\n","\n","def get_image_paths(data_dir, label_dir):\n","    data_paths = [os.path.join(data_dir, img) for img in sorted(os.listdir(data_dir))]\n","    label_paths = [os.path.join(label_dir, lbl) for lbl in sorted(os.listdir(label_dir))]\n","    return data_paths, label_paths\n","\n","def create_subsets(dataset, subset_sizes):\n","    subsets = {}\n","    for size in subset_sizes:\n","        if size == len(dataset):\n","            subsets[size] = dataset  # Use the full dataset\n","        else:\n","            subset, _ = random_split(dataset, [size, len(dataset) - size])\n","            subsets[size] = subset\n","    return subsets\n"],"metadata":{"id":"Qu4BYsfPCMGU","executionInfo":{"status":"ok","timestamp":1701184197936,"user_tz":-60,"elapsed":8,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Define your dataset paths\n","data_dir = '/content/gdrive/MyDrive/training_dataset/data_crop64/'\n","label_dir = '/content/gdrive/MyDrive/training_dataset/label_crop64/'\n","\n","# Get image paths and create the full dataset\n","image_paths, label_paths = get_image_paths(data_dir, label_dir)\n","dataset = CustomDataset(image_paths=image_paths, label_paths=label_paths)\n","\n","# Define subset sizes including the full dataset size\n","subset_sizes = [50, 125, 250, len(dataset)]  # Add the full dataset size\n","\n","# Create subsets\n","dataset_subsets = create_subsets(dataset, subset_sizes)\n","\n","# Device setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using {device}\")\n","\n","# Training configurations\n","learning_rate = 0.001\n","num_epochs = 250  # Adjust as needed\n","\n","# Loop over subsets and train the model\n","for size, subset in dataset_subsets.items():\n","    print(f\"\\nTraining on subset size: {size}\")\n","\n","    # Split the subset into training, validation, and test datasets\n","    train_size = int(0.70 * len(subset))\n","    val_size = int(0.15 * len(subset))\n","    test_size = len(subset) - train_size - val_size\n","    train_dataset, val_dataset, test_dataset = random_split(subset, [train_size, val_size, test_size])\n","\n","    # DataLoader setup\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","    # Model, loss function, and optimizer setup\n","    model = UNet(n_class=3).to(device)\n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for batch_idx, (images, labels) in enumerate(train_loader):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            labels = labels.squeeze(1).long()\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            if batch_idx % 10 == 0:\n","                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item()}\")\n","\n","        # Validation phase\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = 0\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                labels = labels.squeeze(1).long()\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","            val_loss /= len(val_loader)\n","            print(f\"Validation Loss after Epoch {epoch+1}: {val_loss}\")\n","\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        test_loss = 0\n","        correct = 0\n","        total = 0\n","        dice_scores = []\n","\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            probabilities = F.softmax(outputs, dim=1)\n","            _, predicted = torch.max(probabilities, 1)\n","            labels = labels.squeeze(1).long()\n","\n","            loss = criterion(outputs, labels)\n","            test_loss += loss.item()\n","            total += labels.numel()\n","            correct += (predicted == labels).sum().item()\n","\n","            dice_score = dice_coefficient(predicted, labels, num_classes=3)\n","            dice_scores.append(dice_score)\n","\n","        test_loss /= len(test_loader)\n","        test_accuracy = 100 * correct / total\n","        average_dice_score = sum(dice_scores) / len(dice_scores)\n","\n","        print(f\"Subset size {size} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Average Dice Score: {average_dice_score:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4lezj3KQCMED","executionInfo":{"status":"ok","timestamp":1701185287881,"user_tz":-60,"elapsed":1089952,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}},"outputId":"19ffc143-a435-4862-d11e-e96ee47c6afb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda\n","\n","Training on subset size: 50\n","Epoch 1/250, Batch 1/2, Loss: 1.0780671834945679\n","Validation Loss after Epoch 1: 1.0825315713882446\n","Epoch 2/250, Batch 1/2, Loss: 0.42933645844459534\n","Validation Loss after Epoch 2: 1.074283480644226\n","Epoch 3/250, Batch 1/2, Loss: 0.2991139888763428\n","Validation Loss after Epoch 3: 1.0712789297103882\n","Epoch 4/250, Batch 1/2, Loss: 0.2659841477870941\n","Validation Loss after Epoch 4: 1.0289161205291748\n","Epoch 5/250, Batch 1/2, Loss: 0.24353034794330597\n","Validation Loss after Epoch 5: 0.9424404501914978\n","Epoch 6/250, Batch 1/2, Loss: 0.21740645170211792\n","Validation Loss after Epoch 6: 0.82953941822052\n","Epoch 7/250, Batch 1/2, Loss: 0.19537435472011566\n","Validation Loss after Epoch 7: 0.7662394642829895\n","Epoch 8/250, Batch 1/2, Loss: 0.18010041117668152\n","Validation Loss after Epoch 8: 0.726604163646698\n","Epoch 9/250, Batch 1/2, Loss: 0.1640089899301529\n","Validation Loss after Epoch 9: 0.7192140817642212\n","Epoch 10/250, Batch 1/2, Loss: 0.1514086127281189\n","Validation Loss after Epoch 10: 0.7324854135513306\n","Epoch 11/250, Batch 1/2, Loss: 0.14428144693374634\n","Validation Loss after Epoch 11: 0.7412487864494324\n","Epoch 12/250, Batch 1/2, Loss: 0.1372925341129303\n","Validation Loss after Epoch 12: 0.7570119500160217\n","Epoch 13/250, Batch 1/2, Loss: 0.1287257969379425\n","Validation Loss after Epoch 13: 0.7923936247825623\n","Epoch 14/250, Batch 1/2, Loss: 0.12101743370294571\n","Validation Loss after Epoch 14: 0.8187718987464905\n","Epoch 15/250, Batch 1/2, Loss: 0.11212161183357239\n","Validation Loss after Epoch 15: 0.8092716932296753\n","Epoch 16/250, Batch 1/2, Loss: 0.10451462119817734\n","Validation Loss after Epoch 16: 0.7921361923217773\n","Epoch 17/250, Batch 1/2, Loss: 0.09935565292835236\n","Validation Loss after Epoch 17: 0.8344413638114929\n","Epoch 18/250, Batch 1/2, Loss: 0.09575488418340683\n","Validation Loss after Epoch 18: 0.8599920272827148\n","Epoch 19/250, Batch 1/2, Loss: 0.09580249339342117\n","Validation Loss after Epoch 19: 0.8626059293746948\n","Epoch 20/250, Batch 1/2, Loss: 0.09072193503379822\n","Validation Loss after Epoch 20: 0.8995275497436523\n","Epoch 21/250, Batch 1/2, Loss: 0.09077902883291245\n","Validation Loss after Epoch 21: 0.8610649704933167\n","Epoch 22/250, Batch 1/2, Loss: 0.08934542536735535\n","Validation Loss after Epoch 22: 0.7499094605445862\n","Epoch 23/250, Batch 1/2, Loss: 0.08855603635311127\n","Validation Loss after Epoch 23: 0.5042189955711365\n","Epoch 24/250, Batch 1/2, Loss: 0.08438146114349365\n","Validation Loss after Epoch 24: 0.33343082666397095\n","Epoch 25/250, Batch 1/2, Loss: 0.08575034141540527\n","Validation Loss after Epoch 25: 0.2759387493133545\n","Epoch 26/250, Batch 1/2, Loss: 0.0823618546128273\n","Validation Loss after Epoch 26: 0.3537619411945343\n","Epoch 27/250, Batch 1/2, Loss: 0.08105270564556122\n","Validation Loss after Epoch 27: 0.4688883125782013\n","Epoch 28/250, Batch 1/2, Loss: 0.08261819928884506\n","Validation Loss after Epoch 28: 0.5194944143295288\n","Epoch 29/250, Batch 1/2, Loss: 0.08316178619861603\n","Validation Loss after Epoch 29: 0.5245233178138733\n","Epoch 30/250, Batch 1/2, Loss: 0.0808011144399643\n","Validation Loss after Epoch 30: 0.4732724726200104\n","Epoch 31/250, Batch 1/2, Loss: 0.07932641357183456\n","Validation Loss after Epoch 31: 0.33535289764404297\n","Epoch 32/250, Batch 1/2, Loss: 0.07510114461183548\n","Validation Loss after Epoch 32: 0.23353493213653564\n","Epoch 33/250, Batch 1/2, Loss: 0.0741044282913208\n","Validation Loss after Epoch 33: 0.17689989507198334\n","Epoch 34/250, Batch 1/2, Loss: 0.07465613633394241\n","Validation Loss after Epoch 34: 0.14617247879505157\n","Epoch 35/250, Batch 1/2, Loss: 0.07690944522619247\n","Validation Loss after Epoch 35: 0.12270420044660568\n","Epoch 36/250, Batch 1/2, Loss: 0.07364874333143234\n","Validation Loss after Epoch 36: 0.09897831827402115\n","Epoch 37/250, Batch 1/2, Loss: 0.07230454683303833\n","Validation Loss after Epoch 37: 0.09256469458341599\n","Epoch 38/250, Batch 1/2, Loss: 0.07154925912618637\n","Validation Loss after Epoch 38: 0.08819681406021118\n","Epoch 39/250, Batch 1/2, Loss: 0.06439659744501114\n","Validation Loss after Epoch 39: 0.092156782746315\n","Epoch 40/250, Batch 1/2, Loss: 0.06596522033214569\n","Validation Loss after Epoch 40: 0.10658297687768936\n","Epoch 41/250, Batch 1/2, Loss: 0.06333627551794052\n","Validation Loss after Epoch 41: 0.1392676830291748\n","Epoch 42/250, Batch 1/2, Loss: 0.06302458792924881\n","Validation Loss after Epoch 42: 0.25973740220069885\n","Epoch 43/250, Batch 1/2, Loss: 0.06173821538686752\n","Validation Loss after Epoch 43: 0.2591579854488373\n","Epoch 44/250, Batch 1/2, Loss: 0.06904231011867523\n","Validation Loss after Epoch 44: 0.1577916443347931\n","Epoch 45/250, Batch 1/2, Loss: 0.06207652390003204\n","Validation Loss after Epoch 45: 0.11156414449214935\n","Epoch 46/250, Batch 1/2, Loss: 0.058447543531656265\n","Validation Loss after Epoch 46: 0.10063240677118301\n","Epoch 47/250, Batch 1/2, Loss: 0.060370396822690964\n","Validation Loss after Epoch 47: 0.0980052798986435\n","Epoch 48/250, Batch 1/2, Loss: 0.0641445443034172\n","Validation Loss after Epoch 48: 0.10217197239398956\n","Epoch 49/250, Batch 1/2, Loss: 0.0662980005145073\n","Validation Loss after Epoch 49: 0.10874064266681671\n","Epoch 50/250, Batch 1/2, Loss: 0.07118041813373566\n","Validation Loss after Epoch 50: 0.10914351046085358\n","Epoch 51/250, Batch 1/2, Loss: 0.07082320749759674\n","Validation Loss after Epoch 51: 0.09824157506227493\n","Epoch 52/250, Batch 1/2, Loss: 0.06634311378002167\n","Validation Loss after Epoch 52: 0.09289718419313431\n","Epoch 53/250, Batch 1/2, Loss: 0.0593615397810936\n","Validation Loss after Epoch 53: 0.08974744379520416\n","Epoch 54/250, Batch 1/2, Loss: 0.05896729975938797\n","Validation Loss after Epoch 54: 0.09820134192705154\n","Epoch 55/250, Batch 1/2, Loss: 0.05624435469508171\n","Validation Loss after Epoch 55: 0.11016043275594711\n","Epoch 56/250, Batch 1/2, Loss: 0.05786410719156265\n","Validation Loss after Epoch 56: 0.10690911114215851\n","Epoch 57/250, Batch 1/2, Loss: 0.057008303701877594\n","Validation Loss after Epoch 57: 0.09525734931230545\n","Epoch 58/250, Batch 1/2, Loss: 0.05873600021004677\n","Validation Loss after Epoch 58: 0.09118159860372543\n","Epoch 59/250, Batch 1/2, Loss: 0.05444655567407608\n","Validation Loss after Epoch 59: 0.10248544067144394\n","Epoch 60/250, Batch 1/2, Loss: 0.050163835287094116\n","Validation Loss after Epoch 60: 0.10655900090932846\n","Epoch 61/250, Batch 1/2, Loss: 0.04979353025555611\n","Validation Loss after Epoch 61: 0.0935916155576706\n","Epoch 62/250, Batch 1/2, Loss: 0.05239289999008179\n","Validation Loss after Epoch 62: 0.0962255597114563\n","Epoch 63/250, Batch 1/2, Loss: 0.057996392250061035\n","Validation Loss after Epoch 63: 0.08886580169200897\n","Epoch 64/250, Batch 1/2, Loss: 0.05376347526907921\n","Validation Loss after Epoch 64: 0.08881308138370514\n","Epoch 65/250, Batch 1/2, Loss: 0.05215819552540779\n","Validation Loss after Epoch 65: 0.09402160346508026\n","Epoch 66/250, Batch 1/2, Loss: 0.053043436259031296\n","Validation Loss after Epoch 66: 0.09351776540279388\n","Epoch 67/250, Batch 1/2, Loss: 0.050349511206150055\n","Validation Loss after Epoch 67: 0.10272176563739777\n","Epoch 68/250, Batch 1/2, Loss: 0.048631805926561356\n","Validation Loss after Epoch 68: 0.11157441884279251\n","Epoch 69/250, Batch 1/2, Loss: 0.05467655509710312\n","Validation Loss after Epoch 69: 0.10538119077682495\n","Epoch 70/250, Batch 1/2, Loss: 0.05309106782078743\n","Validation Loss after Epoch 70: 0.09813090413808823\n","Epoch 71/250, Batch 1/2, Loss: 0.0448414646089077\n","Validation Loss after Epoch 71: 0.09535318613052368\n","Epoch 72/250, Batch 1/2, Loss: 0.04968519136309624\n","Validation Loss after Epoch 72: 0.09029744565486908\n","Epoch 73/250, Batch 1/2, Loss: 0.046301405876874924\n","Validation Loss after Epoch 73: 0.09258777648210526\n","Epoch 74/250, Batch 1/2, Loss: 0.045340266078710556\n","Validation Loss after Epoch 74: 0.10132282972335815\n","Epoch 75/250, Batch 1/2, Loss: 0.052381474524736404\n","Validation Loss after Epoch 75: 0.10595589876174927\n","Epoch 76/250, Batch 1/2, Loss: 0.04951424151659012\n","Validation Loss after Epoch 76: 0.10557619482278824\n","Epoch 77/250, Batch 1/2, Loss: 0.041036080569028854\n","Validation Loss after Epoch 77: 0.10581416636705399\n","Epoch 78/250, Batch 1/2, Loss: 0.04330255836248398\n","Validation Loss after Epoch 78: 0.11016233265399933\n","Epoch 79/250, Batch 1/2, Loss: 0.044868774712085724\n","Validation Loss after Epoch 79: 0.10951309651136398\n","Epoch 80/250, Batch 1/2, Loss: 0.042190439999103546\n","Validation Loss after Epoch 80: 0.09826964884996414\n","Epoch 81/250, Batch 1/2, Loss: 0.046130817383527756\n","Validation Loss after Epoch 81: 0.09854225069284439\n","Epoch 82/250, Batch 1/2, Loss: 0.04593430832028389\n","Validation Loss after Epoch 82: 0.12180308997631073\n","Epoch 83/250, Batch 1/2, Loss: 0.042830318212509155\n","Validation Loss after Epoch 83: 0.11166099458932877\n","Epoch 84/250, Batch 1/2, Loss: 0.04489365965127945\n","Validation Loss after Epoch 84: 0.09321338683366776\n","Epoch 85/250, Batch 1/2, Loss: 0.05451590567827225\n","Validation Loss after Epoch 85: 0.0814976692199707\n","Epoch 86/250, Batch 1/2, Loss: 0.0565992146730423\n","Validation Loss after Epoch 86: 0.09511853009462357\n","Epoch 87/250, Batch 1/2, Loss: 0.06034747511148453\n","Validation Loss after Epoch 87: 0.13168655335903168\n","Epoch 88/250, Batch 1/2, Loss: 0.062289465218782425\n","Validation Loss after Epoch 88: 0.10518570989370346\n","Epoch 89/250, Batch 1/2, Loss: 0.05923357233405113\n","Validation Loss after Epoch 89: 0.08155292272567749\n","Epoch 90/250, Batch 1/2, Loss: 0.04648023098707199\n","Validation Loss after Epoch 90: 0.08203594386577606\n","Epoch 91/250, Batch 1/2, Loss: 0.04709048196673393\n","Validation Loss after Epoch 91: 0.07934297621250153\n","Epoch 92/250, Batch 1/2, Loss: 0.043474841862916946\n","Validation Loss after Epoch 92: 0.07934590429067612\n","Epoch 93/250, Batch 1/2, Loss: 0.04262014478445053\n","Validation Loss after Epoch 93: 0.08935634046792984\n","Epoch 94/250, Batch 1/2, Loss: 0.03935801610350609\n","Validation Loss after Epoch 94: 0.09847129881381989\n","Epoch 95/250, Batch 1/2, Loss: 0.04175662249326706\n","Validation Loss after Epoch 95: 0.09616092592477798\n","Epoch 96/250, Batch 1/2, Loss: 0.04093310981988907\n","Validation Loss after Epoch 96: 0.09527315199375153\n","Epoch 97/250, Batch 1/2, Loss: 0.04179826006293297\n","Validation Loss after Epoch 97: 0.08627431839704514\n","Epoch 98/250, Batch 1/2, Loss: 0.04368530958890915\n","Validation Loss after Epoch 98: 0.08280831575393677\n","Epoch 99/250, Batch 1/2, Loss: 0.042208608239889145\n","Validation Loss after Epoch 99: 0.08300387859344482\n","Epoch 100/250, Batch 1/2, Loss: 0.04045616835355759\n","Validation Loss after Epoch 100: 0.09817949682474136\n","Epoch 101/250, Batch 1/2, Loss: 0.0399802066385746\n","Validation Loss after Epoch 101: 0.11464746296405792\n","Epoch 102/250, Batch 1/2, Loss: 0.04836166650056839\n","Validation Loss after Epoch 102: 0.1119951382279396\n","Epoch 103/250, Batch 1/2, Loss: 0.0496523454785347\n","Validation Loss after Epoch 103: 0.10267603397369385\n","Epoch 104/250, Batch 1/2, Loss: 0.04573372006416321\n","Validation Loss after Epoch 104: 0.09012918919324875\n","Epoch 105/250, Batch 1/2, Loss: 0.04371173679828644\n","Validation Loss after Epoch 105: 0.08018628507852554\n","Epoch 106/250, Batch 1/2, Loss: 0.0424131378531456\n","Validation Loss after Epoch 106: 0.07887423783540726\n","Epoch 107/250, Batch 1/2, Loss: 0.04170754551887512\n","Validation Loss after Epoch 107: 0.08657333999872208\n","Epoch 108/250, Batch 1/2, Loss: 0.042166948318481445\n","Validation Loss after Epoch 108: 0.10549138486385345\n","Epoch 109/250, Batch 1/2, Loss: 0.0391664020717144\n","Validation Loss after Epoch 109: 0.20120950043201447\n","Epoch 110/250, Batch 1/2, Loss: 0.041664525866508484\n","Validation Loss after Epoch 110: 0.22401311993598938\n","Epoch 111/250, Batch 1/2, Loss: 0.04129877686500549\n","Validation Loss after Epoch 111: 0.12875862419605255\n","Epoch 112/250, Batch 1/2, Loss: 0.04031776636838913\n","Validation Loss after Epoch 112: 0.09353768825531006\n","Epoch 113/250, Batch 1/2, Loss: 0.04052530601620674\n","Validation Loss after Epoch 113: 0.08221280574798584\n","Epoch 114/250, Batch 1/2, Loss: 0.04324205592274666\n","Validation Loss after Epoch 114: 0.08185124397277832\n","Epoch 115/250, Batch 1/2, Loss: 0.04166686534881592\n","Validation Loss after Epoch 115: 0.08338747173547745\n","Epoch 116/250, Batch 1/2, Loss: 0.04116640239953995\n","Validation Loss after Epoch 116: 0.09255713224411011\n","Epoch 117/250, Batch 1/2, Loss: 0.042446307837963104\n","Validation Loss after Epoch 117: 0.10188641399145126\n","Epoch 118/250, Batch 1/2, Loss: 0.0388309620320797\n","Validation Loss after Epoch 118: 0.10094957053661346\n","Epoch 119/250, Batch 1/2, Loss: 0.040927909314632416\n","Validation Loss after Epoch 119: 0.10095684230327606\n","Epoch 120/250, Batch 1/2, Loss: 0.04134057089686394\n","Validation Loss after Epoch 120: 0.09718150645494461\n","Epoch 121/250, Batch 1/2, Loss: 0.037302471697330475\n","Validation Loss after Epoch 121: 0.0930500477552414\n","Epoch 122/250, Batch 1/2, Loss: 0.03711378201842308\n","Validation Loss after Epoch 122: 0.09141307324171066\n","Epoch 123/250, Batch 1/2, Loss: 0.036470700055360794\n","Validation Loss after Epoch 123: 0.0992589220404625\n","Epoch 124/250, Batch 1/2, Loss: 0.036939144134521484\n","Validation Loss after Epoch 124: 0.09327325969934464\n","Epoch 125/250, Batch 1/2, Loss: 0.04154902324080467\n","Validation Loss after Epoch 125: 0.10055024921894073\n","Epoch 126/250, Batch 1/2, Loss: 0.04081650823354721\n","Validation Loss after Epoch 126: 0.09866021573543549\n","Epoch 127/250, Batch 1/2, Loss: 0.039735473692417145\n","Validation Loss after Epoch 127: 0.0900116041302681\n","Epoch 128/250, Batch 1/2, Loss: 0.03531994670629501\n","Validation Loss after Epoch 128: 0.08356443792581558\n","Epoch 129/250, Batch 1/2, Loss: 0.035098012536764145\n","Validation Loss after Epoch 129: 0.08461227267980576\n","Epoch 130/250, Batch 1/2, Loss: 0.03530256822705269\n","Validation Loss after Epoch 130: 0.081886887550354\n","Epoch 131/250, Batch 1/2, Loss: 0.03538999706506729\n","Validation Loss after Epoch 131: 0.08048105239868164\n","Epoch 132/250, Batch 1/2, Loss: 0.03525761142373085\n","Validation Loss after Epoch 132: 0.082305409014225\n","Epoch 133/250, Batch 1/2, Loss: 0.038138072937726974\n","Validation Loss after Epoch 133: 0.08468101918697357\n","Epoch 134/250, Batch 1/2, Loss: 0.033313170075416565\n","Validation Loss after Epoch 134: 0.0884372815489769\n","Epoch 135/250, Batch 1/2, Loss: 0.03328736871480942\n","Validation Loss after Epoch 135: 0.09448698908090591\n","Epoch 136/250, Batch 1/2, Loss: 0.03177791088819504\n","Validation Loss after Epoch 136: 0.10655460506677628\n","Epoch 137/250, Batch 1/2, Loss: 0.035770904272794724\n","Validation Loss after Epoch 137: 0.11826294660568237\n","Epoch 138/250, Batch 1/2, Loss: 0.03823509067296982\n","Validation Loss after Epoch 138: 0.11220517009496689\n","Epoch 139/250, Batch 1/2, Loss: 0.04049242287874222\n","Validation Loss after Epoch 139: 0.09101302921772003\n","Epoch 140/250, Batch 1/2, Loss: 0.04185556247830391\n","Validation Loss after Epoch 140: 0.09086821228265762\n","Epoch 141/250, Batch 1/2, Loss: 0.03744630888104439\n","Validation Loss after Epoch 141: 0.09340456873178482\n","Epoch 142/250, Batch 1/2, Loss: 0.034812141209840775\n","Validation Loss after Epoch 142: 0.09817308187484741\n","Epoch 143/250, Batch 1/2, Loss: 0.037685997784137726\n","Validation Loss after Epoch 143: 0.10710717737674713\n","Epoch 144/250, Batch 1/2, Loss: 0.03226591274142265\n","Validation Loss after Epoch 144: 0.09895087778568268\n","Epoch 145/250, Batch 1/2, Loss: 0.03606128320097923\n","Validation Loss after Epoch 145: 0.0928555577993393\n","Epoch 146/250, Batch 1/2, Loss: 0.037905316799879074\n","Validation Loss after Epoch 146: 0.08775455504655838\n","Epoch 147/250, Batch 1/2, Loss: 0.03818749636411667\n","Validation Loss after Epoch 147: 0.09081287682056427\n","Epoch 148/250, Batch 1/2, Loss: 0.03964440897107124\n","Validation Loss after Epoch 148: 0.10374852269887924\n","Epoch 149/250, Batch 1/2, Loss: 0.047561176121234894\n","Validation Loss after Epoch 149: 0.10961530357599258\n","Epoch 150/250, Batch 1/2, Loss: 0.05313993990421295\n","Validation Loss after Epoch 150: 0.10683756321668625\n","Epoch 151/250, Batch 1/2, Loss: 0.04754771292209625\n","Validation Loss after Epoch 151: 0.0989154577255249\n","Epoch 152/250, Batch 1/2, Loss: 0.0425211563706398\n","Validation Loss after Epoch 152: 0.10747361183166504\n","Epoch 153/250, Batch 1/2, Loss: 0.04243990778923035\n","Validation Loss after Epoch 153: 0.11534646898508072\n","Epoch 154/250, Batch 1/2, Loss: 0.04182680323719978\n","Validation Loss after Epoch 154: 0.1176648661494255\n","Epoch 155/250, Batch 1/2, Loss: 0.042309802025556564\n","Validation Loss after Epoch 155: 0.1298404186964035\n","Epoch 156/250, Batch 1/2, Loss: 0.04191410914063454\n","Validation Loss after Epoch 156: 0.13037633895874023\n","Epoch 157/250, Batch 1/2, Loss: 0.03838884085416794\n","Validation Loss after Epoch 157: 0.08250529319047928\n","Epoch 158/250, Batch 1/2, Loss: 0.042907726019620895\n","Validation Loss after Epoch 158: 0.08439229428768158\n","Epoch 159/250, Batch 1/2, Loss: 0.04825292527675629\n","Validation Loss after Epoch 159: 0.08579673618078232\n","Epoch 160/250, Batch 1/2, Loss: 0.0492231510579586\n","Validation Loss after Epoch 160: 0.0837809219956398\n","Epoch 161/250, Batch 1/2, Loss: 0.048258911818265915\n","Validation Loss after Epoch 161: 0.08387253433465958\n","Epoch 162/250, Batch 1/2, Loss: 0.0490272231400013\n","Validation Loss after Epoch 162: 0.0824369266629219\n","Epoch 163/250, Batch 1/2, Loss: 0.04461948573589325\n","Validation Loss after Epoch 163: 0.07992313802242279\n","Epoch 164/250, Batch 1/2, Loss: 0.04058801755309105\n","Validation Loss after Epoch 164: 0.07783060520887375\n","Epoch 165/250, Batch 1/2, Loss: 0.039472538977861404\n","Validation Loss after Epoch 165: 0.07813625037670135\n","Epoch 166/250, Batch 1/2, Loss: 0.04050251096487045\n","Validation Loss after Epoch 166: 0.08052569627761841\n","Epoch 167/250, Batch 1/2, Loss: 0.04557203873991966\n","Validation Loss after Epoch 167: 0.085352823138237\n","Epoch 168/250, Batch 1/2, Loss: 0.046128176152706146\n","Validation Loss after Epoch 168: 0.08699752390384674\n","Epoch 169/250, Batch 1/2, Loss: 0.04393002390861511\n","Validation Loss after Epoch 169: 0.0854104533791542\n","Epoch 170/250, Batch 1/2, Loss: 0.047505542635917664\n","Validation Loss after Epoch 170: 0.07577220350503922\n","Epoch 171/250, Batch 1/2, Loss: 0.042882319539785385\n","Validation Loss after Epoch 171: 0.08360294252634048\n","Epoch 172/250, Batch 1/2, Loss: 0.03691679239273071\n","Validation Loss after Epoch 172: 0.09954853355884552\n","Epoch 173/250, Batch 1/2, Loss: 0.04271973296999931\n","Validation Loss after Epoch 173: 0.09107762575149536\n","Epoch 174/250, Batch 1/2, Loss: 0.04465600103139877\n","Validation Loss after Epoch 174: 0.08088191598653793\n","Epoch 175/250, Batch 1/2, Loss: 0.040859825909137726\n","Validation Loss after Epoch 175: 0.0769319087266922\n","Epoch 176/250, Batch 1/2, Loss: 0.03847965970635414\n","Validation Loss after Epoch 176: 0.07646506279706955\n","Epoch 177/250, Batch 1/2, Loss: 0.03474791720509529\n","Validation Loss after Epoch 177: 0.08065150678157806\n","Epoch 178/250, Batch 1/2, Loss: 0.03584916144609451\n","Validation Loss after Epoch 178: 0.08670593053102493\n","Epoch 179/250, Batch 1/2, Loss: 0.0436822734773159\n","Validation Loss after Epoch 179: 0.11839304119348526\n","Epoch 180/250, Batch 1/2, Loss: 0.03699738159775734\n","Validation Loss after Epoch 180: 0.13027915358543396\n","Epoch 181/250, Batch 1/2, Loss: 0.035721298307180405\n","Validation Loss after Epoch 181: 0.12326087057590485\n","Epoch 182/250, Batch 1/2, Loss: 0.042700182646512985\n","Validation Loss after Epoch 182: 0.10488592833280563\n","Epoch 183/250, Batch 1/2, Loss: 0.04630640149116516\n","Validation Loss after Epoch 183: 0.08529500663280487\n","Epoch 184/250, Batch 1/2, Loss: 0.04933595657348633\n","Validation Loss after Epoch 184: 0.08091117441654205\n","Epoch 185/250, Batch 1/2, Loss: 0.05439654365181923\n","Validation Loss after Epoch 185: 0.08580629527568817\n","Epoch 186/250, Batch 1/2, Loss: 0.0533207431435585\n","Validation Loss after Epoch 186: 0.09770049154758453\n","Epoch 187/250, Batch 1/2, Loss: 0.04793117940425873\n","Validation Loss after Epoch 187: 0.11521414667367935\n","Epoch 188/250, Batch 1/2, Loss: 0.04345710575580597\n","Validation Loss after Epoch 188: 0.14891400933265686\n","Epoch 189/250, Batch 1/2, Loss: 0.04437227174639702\n","Validation Loss after Epoch 189: 0.15820495784282684\n","Epoch 190/250, Batch 1/2, Loss: 0.041780125349760056\n","Validation Loss after Epoch 190: 0.1532537043094635\n","Epoch 191/250, Batch 1/2, Loss: 0.045315321534872055\n","Validation Loss after Epoch 191: 0.13844288885593414\n","Epoch 192/250, Batch 1/2, Loss: 0.04165055230259895\n","Validation Loss after Epoch 192: 0.12582117319107056\n","Epoch 193/250, Batch 1/2, Loss: 0.04111618921160698\n","Validation Loss after Epoch 193: 0.1103716567158699\n","Epoch 194/250, Batch 1/2, Loss: 0.043222542852163315\n","Validation Loss after Epoch 194: 0.09540926665067673\n","Epoch 195/250, Batch 1/2, Loss: 0.04418081417679787\n","Validation Loss after Epoch 195: 0.08755584806203842\n","Epoch 196/250, Batch 1/2, Loss: 0.04219741001725197\n","Validation Loss after Epoch 196: 0.08719468116760254\n","Epoch 197/250, Batch 1/2, Loss: 0.04404139891266823\n","Validation Loss after Epoch 197: 0.08303527534008026\n","Epoch 198/250, Batch 1/2, Loss: 0.04147378355264664\n","Validation Loss after Epoch 198: 0.12868750095367432\n","Epoch 199/250, Batch 1/2, Loss: 0.2592706084251404\n","Validation Loss after Epoch 199: 0.11972168833017349\n","Epoch 200/250, Batch 1/2, Loss: 0.08565159887075424\n","Validation Loss after Epoch 200: 0.43134552240371704\n","Epoch 201/250, Batch 1/2, Loss: 0.06859587132930756\n","Validation Loss after Epoch 201: 0.8998412489891052\n","Epoch 202/250, Batch 1/2, Loss: 0.0613391250371933\n","Validation Loss after Epoch 202: 0.7981061935424805\n","Epoch 203/250, Batch 1/2, Loss: 0.06012852489948273\n","Validation Loss after Epoch 203: 0.5904034972190857\n","Epoch 204/250, Batch 1/2, Loss: 0.06361350417137146\n","Validation Loss after Epoch 204: 0.29449382424354553\n","Epoch 205/250, Batch 1/2, Loss: 0.06017684191465378\n","Validation Loss after Epoch 205: 0.16520142555236816\n","Epoch 206/250, Batch 1/2, Loss: 0.05375893414020538\n","Validation Loss after Epoch 206: 0.1248374804854393\n","Epoch 207/250, Batch 1/2, Loss: 0.04967127740383148\n","Validation Loss after Epoch 207: 0.09777313470840454\n","Epoch 208/250, Batch 1/2, Loss: 0.051506757736206055\n","Validation Loss after Epoch 208: 0.08961284905672073\n","Epoch 209/250, Batch 1/2, Loss: 0.048237256705760956\n","Validation Loss after Epoch 209: 0.07929577678442001\n","Epoch 210/250, Batch 1/2, Loss: 0.05196510627865791\n","Validation Loss after Epoch 210: 0.086222343146801\n","Epoch 211/250, Batch 1/2, Loss: 0.041352007538080215\n","Validation Loss after Epoch 211: 0.0985313281416893\n","Epoch 212/250, Batch 1/2, Loss: 0.04877200350165367\n","Validation Loss after Epoch 212: 0.09872598201036453\n","Epoch 213/250, Batch 1/2, Loss: 0.04570384696125984\n","Validation Loss after Epoch 213: 0.08941540867090225\n","Epoch 214/250, Batch 1/2, Loss: 0.04276808351278305\n","Validation Loss after Epoch 214: 0.08367465436458588\n","Epoch 215/250, Batch 1/2, Loss: 0.04568047448992729\n","Validation Loss after Epoch 215: 0.08233876526355743\n","Epoch 216/250, Batch 1/2, Loss: 0.043801456689834595\n","Validation Loss after Epoch 216: 0.08785127103328705\n","Epoch 217/250, Batch 1/2, Loss: 0.0397786945104599\n","Validation Loss after Epoch 217: 0.09202749282121658\n","Epoch 218/250, Batch 1/2, Loss: 0.039662379771471024\n","Validation Loss after Epoch 218: 0.09308276325464249\n","Epoch 219/250, Batch 1/2, Loss: 0.039117809385061264\n","Validation Loss after Epoch 219: 0.09696119278669357\n","Epoch 220/250, Batch 1/2, Loss: 0.03941521421074867\n","Validation Loss after Epoch 220: 0.10755787789821625\n","Epoch 221/250, Batch 1/2, Loss: 0.03810707852244377\n","Validation Loss after Epoch 221: 0.10997748374938965\n","Epoch 222/250, Batch 1/2, Loss: 0.03766113147139549\n","Validation Loss after Epoch 222: 0.10314613580703735\n","Epoch 223/250, Batch 1/2, Loss: 0.035124704241752625\n","Validation Loss after Epoch 223: 0.09938671439886093\n","Epoch 224/250, Batch 1/2, Loss: 0.036381933838129044\n","Validation Loss after Epoch 224: 0.0998743399977684\n","Epoch 225/250, Batch 1/2, Loss: 0.042004745453596115\n","Validation Loss after Epoch 225: 0.09837736189365387\n","Epoch 226/250, Batch 1/2, Loss: 0.047348752617836\n","Validation Loss after Epoch 226: 0.10460914671421051\n","Epoch 227/250, Batch 1/2, Loss: 0.055198486894369125\n","Validation Loss after Epoch 227: 0.10764607042074203\n","Epoch 228/250, Batch 1/2, Loss: 0.09693339467048645\n","Validation Loss after Epoch 228: 0.08172411471605301\n","Epoch 229/250, Batch 1/2, Loss: 0.05324628949165344\n","Validation Loss after Epoch 229: 0.07496749609708786\n","Epoch 230/250, Batch 1/2, Loss: 0.04704144969582558\n","Validation Loss after Epoch 230: 0.08188939094543457\n","Epoch 231/250, Batch 1/2, Loss: 0.04093432426452637\n","Validation Loss after Epoch 231: 0.08842068165540695\n","Epoch 232/250, Batch 1/2, Loss: 0.039182037115097046\n","Validation Loss after Epoch 232: 0.09143394231796265\n","Epoch 233/250, Batch 1/2, Loss: 0.04005284234881401\n","Validation Loss after Epoch 233: 0.09945617616176605\n","Epoch 234/250, Batch 1/2, Loss: 0.03984251990914345\n","Validation Loss after Epoch 234: 0.09543608129024506\n","Epoch 235/250, Batch 1/2, Loss: 0.038909029215574265\n","Validation Loss after Epoch 235: 0.09287279099225998\n","Epoch 236/250, Batch 1/2, Loss: 0.04314927011728287\n","Validation Loss after Epoch 236: 0.09693784266710281\n","Epoch 237/250, Batch 1/2, Loss: 0.042541541159152985\n","Validation Loss after Epoch 237: 0.10582609474658966\n","Epoch 238/250, Batch 1/2, Loss: 0.03929842635989189\n","Validation Loss after Epoch 238: 0.1130056381225586\n","Epoch 239/250, Batch 1/2, Loss: 0.0377783328294754\n","Validation Loss after Epoch 239: 0.11371968686580658\n","Epoch 240/250, Batch 1/2, Loss: 0.037223659455776215\n","Validation Loss after Epoch 240: 0.107061468064785\n","Epoch 241/250, Batch 1/2, Loss: 0.03895900398492813\n","Validation Loss after Epoch 241: 0.09442643821239471\n","Epoch 242/250, Batch 1/2, Loss: 0.03558001294732094\n","Validation Loss after Epoch 242: 0.08437337726354599\n","Epoch 243/250, Batch 1/2, Loss: 0.03518040478229523\n","Validation Loss after Epoch 243: 0.08159182965755463\n","Epoch 244/250, Batch 1/2, Loss: 0.03483984246850014\n","Validation Loss after Epoch 244: 0.08202511072158813\n","Epoch 245/250, Batch 1/2, Loss: 0.03487831726670265\n","Validation Loss after Epoch 245: 0.08443151414394379\n","Epoch 246/250, Batch 1/2, Loss: 0.034855734556913376\n","Validation Loss after Epoch 246: 0.08156976848840714\n","Epoch 247/250, Batch 1/2, Loss: 0.0373016856610775\n","Validation Loss after Epoch 247: 0.080415740609169\n","Epoch 248/250, Batch 1/2, Loss: 0.03557094559073448\n","Validation Loss after Epoch 248: 0.0815325528383255\n","Epoch 249/250, Batch 1/2, Loss: 0.041955847293138504\n","Validation Loss after Epoch 249: 0.07861215621232986\n","Epoch 250/250, Batch 1/2, Loss: 0.04238000139594078\n","Validation Loss after Epoch 250: 0.08101101219654083\n","Subset size 50 - Test Loss: 0.0446, Test Accuracy: 98.26%, Average Dice Score: 0.9795\n","\n","Training on subset size: 125\n","Epoch 1/250, Batch 1/3, Loss: 1.1599198579788208\n","Validation Loss after Epoch 1: 1.1234558820724487\n","Epoch 2/250, Batch 1/3, Loss: 0.29333433508872986\n","Validation Loss after Epoch 2: 1.085344672203064\n","Epoch 3/250, Batch 1/3, Loss: 0.2000163495540619\n","Validation Loss after Epoch 3: 1.1209635734558105\n","Epoch 4/250, Batch 1/3, Loss: 0.18397410213947296\n","Validation Loss after Epoch 4: 1.2207157611846924\n","Epoch 5/250, Batch 1/3, Loss: 0.1433626413345337\n","Validation Loss after Epoch 5: 1.2580699920654297\n","Epoch 6/250, Batch 1/3, Loss: 0.13156774640083313\n","Validation Loss after Epoch 6: 1.1099224090576172\n","Epoch 7/250, Batch 1/3, Loss: 0.11177846789360046\n","Validation Loss after Epoch 7: 1.0501387119293213\n","Epoch 8/250, Batch 1/3, Loss: 0.10235092788934708\n","Validation Loss after Epoch 8: 0.9773619771003723\n","Epoch 9/250, Batch 1/3, Loss: 0.0982576534152031\n","Validation Loss after Epoch 9: 0.9324477314949036\n","Epoch 10/250, Batch 1/3, Loss: 0.08894166350364685\n","Validation Loss after Epoch 10: 0.8593113422393799\n","Epoch 11/250, Batch 1/3, Loss: 0.07937098294496536\n","Validation Loss after Epoch 11: 0.8276637196540833\n","Epoch 12/250, Batch 1/3, Loss: 0.08339768648147583\n","Validation Loss after Epoch 12: 0.8737450242042542\n","Epoch 13/250, Batch 1/3, Loss: 0.06812608987092972\n","Validation Loss after Epoch 13: 0.8693455457687378\n","Epoch 14/250, Batch 1/3, Loss: 0.07178207486867905\n","Validation Loss after Epoch 14: 0.8267805576324463\n","Epoch 15/250, Batch 1/3, Loss: 0.06409536302089691\n","Validation Loss after Epoch 15: 0.7717682719230652\n","Epoch 16/250, Batch 1/3, Loss: 0.06933724135160446\n","Validation Loss after Epoch 16: 0.7351782321929932\n","Epoch 17/250, Batch 1/3, Loss: 0.05795083940029144\n","Validation Loss after Epoch 17: 0.6726817488670349\n","Epoch 18/250, Batch 1/3, Loss: 0.0585966557264328\n","Validation Loss after Epoch 18: 0.46810096502304077\n","Epoch 19/250, Batch 1/3, Loss: 0.06321555376052856\n","Validation Loss after Epoch 19: 0.2772788405418396\n","Epoch 20/250, Batch 1/3, Loss: 0.05360687896609306\n","Validation Loss after Epoch 20: 0.3375999927520752\n","Epoch 21/250, Batch 1/3, Loss: 0.06978882849216461\n","Validation Loss after Epoch 21: 0.2514864504337311\n","Epoch 22/250, Batch 1/3, Loss: 0.05199200287461281\n","Validation Loss after Epoch 22: 0.13606248795986176\n","Epoch 23/250, Batch 1/3, Loss: 0.049689020961523056\n","Validation Loss after Epoch 23: 0.1066303551197052\n","Epoch 24/250, Batch 1/3, Loss: 0.055862076580524445\n","Validation Loss after Epoch 24: 0.06144741550087929\n","Epoch 25/250, Batch 1/3, Loss: 0.046198442578315735\n","Validation Loss after Epoch 25: 0.053856924176216125\n","Epoch 26/250, Batch 1/3, Loss: 0.04819847643375397\n","Validation Loss after Epoch 26: 0.0898810476064682\n","Epoch 27/250, Batch 1/3, Loss: 0.04468375816941261\n","Validation Loss after Epoch 27: 0.07016919553279877\n","Epoch 28/250, Batch 1/3, Loss: 0.05043502524495125\n","Validation Loss after Epoch 28: 0.04413431137800217\n","Epoch 29/250, Batch 1/3, Loss: 0.0421486496925354\n","Validation Loss after Epoch 29: 0.03980740159749985\n","Epoch 30/250, Batch 1/3, Loss: 0.04701749235391617\n","Validation Loss after Epoch 30: 0.04907228425145149\n","Epoch 31/250, Batch 1/3, Loss: 0.042203351855278015\n","Validation Loss after Epoch 31: 0.04517678543925285\n","Epoch 32/250, Batch 1/3, Loss: 0.03989901766180992\n","Validation Loss after Epoch 32: 0.04093268886208534\n","Epoch 33/250, Batch 1/3, Loss: 0.03982916846871376\n","Validation Loss after Epoch 33: 0.04547235742211342\n","Epoch 34/250, Batch 1/3, Loss: 0.036189015954732895\n","Validation Loss after Epoch 34: 0.05683128535747528\n","Epoch 35/250, Batch 1/3, Loss: 0.034468717873096466\n","Validation Loss after Epoch 35: 0.04549795761704445\n","Epoch 36/250, Batch 1/3, Loss: 0.037919338792562485\n","Validation Loss after Epoch 36: 0.045962437987327576\n","Epoch 37/250, Batch 1/3, Loss: 0.037393324077129364\n","Validation Loss after Epoch 37: 0.05691337585449219\n","Epoch 38/250, Batch 1/3, Loss: 0.03869485855102539\n","Validation Loss after Epoch 38: 0.04579904302954674\n","Epoch 39/250, Batch 1/3, Loss: 0.0391109362244606\n","Validation Loss after Epoch 39: 0.04190610349178314\n","Epoch 40/250, Batch 1/3, Loss: 0.03471275791525841\n","Validation Loss after Epoch 40: 0.04321672394871712\n","Epoch 41/250, Batch 1/3, Loss: 0.03902839496731758\n","Validation Loss after Epoch 41: 0.04257597029209137\n","Epoch 42/250, Batch 1/3, Loss: 0.03704463690519333\n","Validation Loss after Epoch 42: 0.04141485318541527\n","Epoch 43/250, Batch 1/3, Loss: 0.03414136543869972\n","Validation Loss after Epoch 43: 0.040653400123119354\n","Epoch 44/250, Batch 1/3, Loss: 0.03198935464024544\n","Validation Loss after Epoch 44: 0.0412667915225029\n","Epoch 45/250, Batch 1/3, Loss: 0.029317716136574745\n","Validation Loss after Epoch 45: 0.03887178376317024\n","Epoch 46/250, Batch 1/3, Loss: 0.039666786789894104\n","Validation Loss after Epoch 46: 0.03679768368601799\n","Epoch 47/250, Batch 1/3, Loss: 0.03294174745678902\n","Validation Loss after Epoch 47: 0.03830824792385101\n","Epoch 48/250, Batch 1/3, Loss: 0.031920433044433594\n","Validation Loss after Epoch 48: 0.03873617202043533\n","Epoch 49/250, Batch 1/3, Loss: 0.03359037637710571\n","Validation Loss after Epoch 49: 0.04125768318772316\n","Epoch 50/250, Batch 1/3, Loss: 0.03150530904531479\n","Validation Loss after Epoch 50: 0.03623872250318527\n","Epoch 51/250, Batch 1/3, Loss: 0.03341498598456383\n","Validation Loss after Epoch 51: 0.04342304542660713\n","Epoch 52/250, Batch 1/3, Loss: 0.033735014498233795\n","Validation Loss after Epoch 52: 0.03289220854640007\n","Epoch 53/250, Batch 1/3, Loss: 0.032132938504219055\n","Validation Loss after Epoch 53: 0.053055278956890106\n","Epoch 54/250, Batch 1/3, Loss: 0.029559839516878128\n","Validation Loss after Epoch 54: 0.05513039603829384\n","Epoch 55/250, Batch 1/3, Loss: 0.03621520847082138\n","Validation Loss after Epoch 55: 0.05358373746275902\n","Epoch 56/250, Batch 1/3, Loss: 0.03429577872157097\n","Validation Loss after Epoch 56: 0.038032129406929016\n","Epoch 57/250, Batch 1/3, Loss: 0.03206322342157364\n","Validation Loss after Epoch 57: 0.04133649542927742\n","Epoch 58/250, Batch 1/3, Loss: 0.02842494286596775\n","Validation Loss after Epoch 58: 0.03669270873069763\n","Epoch 59/250, Batch 1/3, Loss: 0.028710253536701202\n","Validation Loss after Epoch 59: 0.03815852478146553\n","Epoch 60/250, Batch 1/3, Loss: 0.027361949905753136\n","Validation Loss after Epoch 60: 0.03892496973276138\n","Epoch 61/250, Batch 1/3, Loss: 0.030159976333379745\n","Validation Loss after Epoch 61: 0.03827734291553497\n","Epoch 62/250, Batch 1/3, Loss: 0.02704685926437378\n","Validation Loss after Epoch 62: 0.03355492651462555\n","Epoch 63/250, Batch 1/3, Loss: 0.037735987454652786\n","Validation Loss after Epoch 63: 0.034045830368995667\n","Epoch 64/250, Batch 1/3, Loss: 0.02622929960489273\n","Validation Loss after Epoch 64: 0.03753144294023514\n","Epoch 65/250, Batch 1/3, Loss: 0.03145114332437515\n","Validation Loss after Epoch 65: 0.033342670649290085\n","Epoch 66/250, Batch 1/3, Loss: 0.031710632145404816\n","Validation Loss after Epoch 66: 0.03706347569823265\n","Epoch 67/250, Batch 1/3, Loss: 0.025850029662251472\n","Validation Loss after Epoch 67: 0.03281151503324509\n","Epoch 68/250, Batch 1/3, Loss: 0.026116976514458656\n","Validation Loss after Epoch 68: 0.03496105968952179\n","Epoch 69/250, Batch 1/3, Loss: 0.025139618664979935\n","Validation Loss after Epoch 69: 0.04061288386583328\n","Epoch 70/250, Batch 1/3, Loss: 0.026591019704937935\n","Validation Loss after Epoch 70: 0.03706609085202217\n","Epoch 71/250, Batch 1/3, Loss: 0.024811919778585434\n","Validation Loss after Epoch 71: 0.038650304079055786\n","Epoch 72/250, Batch 1/3, Loss: 0.025739001110196114\n","Validation Loss after Epoch 72: 0.04366140067577362\n","Epoch 73/250, Batch 1/3, Loss: 0.024645164608955383\n","Validation Loss after Epoch 73: 0.04074949026107788\n","Epoch 74/250, Batch 1/3, Loss: 0.023086009547114372\n","Validation Loss after Epoch 74: 0.05369763821363449\n","Epoch 75/250, Batch 1/3, Loss: 0.02660243585705757\n","Validation Loss after Epoch 75: 0.04226964712142944\n","Epoch 76/250, Batch 1/3, Loss: 0.0275866761803627\n","Validation Loss after Epoch 76: 0.04243067279458046\n","Epoch 77/250, Batch 1/3, Loss: 0.03032592497766018\n","Validation Loss after Epoch 77: 0.036613188683986664\n","Epoch 78/250, Batch 1/3, Loss: 0.03342469409108162\n","Validation Loss after Epoch 78: 0.03405378758907318\n","Epoch 79/250, Batch 1/3, Loss: 0.02803559973835945\n","Validation Loss after Epoch 79: 0.031619224697351456\n","Epoch 80/250, Batch 1/3, Loss: 0.03136146813631058\n","Validation Loss after Epoch 80: 0.02953083999454975\n","Epoch 81/250, Batch 1/3, Loss: 0.028423842042684555\n","Validation Loss after Epoch 81: 0.03432314842939377\n","Epoch 82/250, Batch 1/3, Loss: 0.0250344667583704\n","Validation Loss after Epoch 82: 0.0350235290825367\n","Epoch 83/250, Batch 1/3, Loss: 0.024162927642464638\n","Validation Loss after Epoch 83: 0.033344611525535583\n","Epoch 84/250, Batch 1/3, Loss: 0.023141538724303246\n","Validation Loss after Epoch 84: 0.030461689457297325\n","Epoch 85/250, Batch 1/3, Loss: 0.02235126867890358\n","Validation Loss after Epoch 85: 0.03336666524410248\n","Epoch 86/250, Batch 1/3, Loss: 0.02618613839149475\n","Validation Loss after Epoch 86: 0.04447526857256889\n","Epoch 87/250, Batch 1/3, Loss: 0.031738605350255966\n","Validation Loss after Epoch 87: 0.04124728590250015\n","Epoch 88/250, Batch 1/3, Loss: 0.026314115151762962\n","Validation Loss after Epoch 88: 0.05247534438967705\n","Epoch 89/250, Batch 1/3, Loss: 0.02331981249153614\n","Validation Loss after Epoch 89: 0.03245227038860321\n","Epoch 90/250, Batch 1/3, Loss: 0.022575831040740013\n","Validation Loss after Epoch 90: 0.02774651162326336\n","Epoch 91/250, Batch 1/3, Loss: 0.02827737107872963\n","Validation Loss after Epoch 91: 0.039818037301301956\n","Epoch 92/250, Batch 1/3, Loss: 0.02357863076031208\n","Validation Loss after Epoch 92: 0.02972644567489624\n","Epoch 93/250, Batch 1/3, Loss: 0.022993043065071106\n","Validation Loss after Epoch 93: 0.03205391392111778\n","Epoch 94/250, Batch 1/3, Loss: 0.019715510308742523\n","Validation Loss after Epoch 94: 0.03414234146475792\n","Epoch 95/250, Batch 1/3, Loss: 0.020953647792339325\n","Validation Loss after Epoch 95: 0.03311757743358612\n","Epoch 96/250, Batch 1/3, Loss: 0.02224535495042801\n","Validation Loss after Epoch 96: 0.03246653825044632\n","Epoch 97/250, Batch 1/3, Loss: 0.02332141064107418\n","Validation Loss after Epoch 97: 0.03204184025526047\n","Epoch 98/250, Batch 1/3, Loss: 0.019199779257178307\n","Validation Loss after Epoch 98: 0.03459033742547035\n","Epoch 99/250, Batch 1/3, Loss: 0.021451497450470924\n","Validation Loss after Epoch 99: 0.03708311915397644\n","Epoch 100/250, Batch 1/3, Loss: 0.020860573276877403\n","Validation Loss after Epoch 100: 0.03699691593647003\n","Epoch 101/250, Batch 1/3, Loss: 0.0200063306838274\n","Validation Loss after Epoch 101: 0.037347059696912766\n","Epoch 102/250, Batch 1/3, Loss: 0.01973927579820156\n","Validation Loss after Epoch 102: 0.03969727084040642\n","Epoch 103/250, Batch 1/3, Loss: 0.020255090668797493\n","Validation Loss after Epoch 103: 0.03598802164196968\n","Epoch 104/250, Batch 1/3, Loss: 0.01765356957912445\n","Validation Loss after Epoch 104: 0.035271916538476944\n","Epoch 105/250, Batch 1/3, Loss: 0.019990993663668633\n","Validation Loss after Epoch 105: 0.03664413467049599\n","Epoch 106/250, Batch 1/3, Loss: 0.018843401223421097\n","Validation Loss after Epoch 106: 0.033905141055583954\n","Epoch 107/250, Batch 1/3, Loss: 0.0209934301674366\n","Validation Loss after Epoch 107: 0.04012756049633026\n","Epoch 108/250, Batch 1/3, Loss: 0.017820701003074646\n","Validation Loss after Epoch 108: 0.03502856194972992\n","Epoch 109/250, Batch 1/3, Loss: 0.015630004927515984\n","Validation Loss after Epoch 109: 0.0352276936173439\n","Epoch 110/250, Batch 1/3, Loss: 0.01912698522210121\n","Validation Loss after Epoch 110: 0.03154979646205902\n","Epoch 111/250, Batch 1/3, Loss: 0.017426487058401108\n","Validation Loss after Epoch 111: 0.033931389451026917\n","Epoch 112/250, Batch 1/3, Loss: 0.016746537759900093\n","Validation Loss after Epoch 112: 0.03587542474269867\n","Epoch 113/250, Batch 1/3, Loss: 0.016118241474032402\n","Validation Loss after Epoch 113: 0.035594090819358826\n","Epoch 114/250, Batch 1/3, Loss: 0.015803975984454155\n","Validation Loss after Epoch 114: 0.03618846461176872\n","Epoch 115/250, Batch 1/3, Loss: 0.023144220933318138\n","Validation Loss after Epoch 115: 0.03929150849580765\n","Epoch 116/250, Batch 1/3, Loss: 0.015487957745790482\n","Validation Loss after Epoch 116: 0.05036652460694313\n","Epoch 117/250, Batch 1/3, Loss: 0.03286540508270264\n","Validation Loss after Epoch 117: 0.03213958442211151\n","Epoch 118/250, Batch 1/3, Loss: 0.023527970537543297\n","Validation Loss after Epoch 118: 0.05473777651786804\n","Epoch 119/250, Batch 1/3, Loss: 0.022128475829958916\n","Validation Loss after Epoch 119: 0.047099519520998\n","Epoch 120/250, Batch 1/3, Loss: 0.027049802243709564\n","Validation Loss after Epoch 120: 0.04699145630002022\n","Epoch 121/250, Batch 1/3, Loss: 0.024685295298695564\n","Validation Loss after Epoch 121: 0.033502478152513504\n","Epoch 122/250, Batch 1/3, Loss: 0.017296649515628815\n","Validation Loss after Epoch 122: 0.0312662273645401\n","Epoch 123/250, Batch 1/3, Loss: 0.019934913143515587\n","Validation Loss after Epoch 123: 0.03112083673477173\n","Epoch 124/250, Batch 1/3, Loss: 0.020467611029744148\n","Validation Loss after Epoch 124: 0.030216313898563385\n","Epoch 125/250, Batch 1/3, Loss: 0.021694250404834747\n","Validation Loss after Epoch 125: 0.03299248591065407\n","Epoch 126/250, Batch 1/3, Loss: 0.021393170580267906\n","Validation Loss after Epoch 126: 0.03616674989461899\n","Epoch 127/250, Batch 1/3, Loss: 0.018579933792352676\n","Validation Loss after Epoch 127: 0.036239661276340485\n","Epoch 128/250, Batch 1/3, Loss: 0.018809396773576736\n","Validation Loss after Epoch 128: 0.027954943478107452\n","Epoch 129/250, Batch 1/3, Loss: 0.01945439726114273\n","Validation Loss after Epoch 129: 0.02867455594241619\n","Epoch 130/250, Batch 1/3, Loss: 0.015549805946648121\n","Validation Loss after Epoch 130: 0.03033185377717018\n","Epoch 131/250, Batch 1/3, Loss: 0.01667521893978119\n","Validation Loss after Epoch 131: 0.04911598563194275\n","Epoch 132/250, Batch 1/3, Loss: 0.02126336470246315\n","Validation Loss after Epoch 132: 0.035991016775369644\n","Epoch 133/250, Batch 1/3, Loss: 0.018995337188243866\n","Validation Loss after Epoch 133: 0.0355696864426136\n","Epoch 134/250, Batch 1/3, Loss: 0.016583280637860298\n","Validation Loss after Epoch 134: 0.03519465774297714\n","Epoch 135/250, Batch 1/3, Loss: 0.01599258929491043\n","Validation Loss after Epoch 135: 0.04611024633049965\n","Epoch 136/250, Batch 1/3, Loss: 0.01796349510550499\n","Validation Loss after Epoch 136: 0.03620268031954765\n","Epoch 137/250, Batch 1/3, Loss: 0.01538644079118967\n","Validation Loss after Epoch 137: 0.03536417335271835\n","Epoch 138/250, Batch 1/3, Loss: 0.014454415999352932\n","Validation Loss after Epoch 138: 0.037754882127046585\n","Epoch 139/250, Batch 1/3, Loss: 0.014607388526201248\n","Validation Loss after Epoch 139: 0.030697444453835487\n","Epoch 140/250, Batch 1/3, Loss: 0.015503705479204655\n","Validation Loss after Epoch 140: 0.03546767681837082\n","Epoch 141/250, Batch 1/3, Loss: 0.014505347236990929\n","Validation Loss after Epoch 141: 0.031059563159942627\n","Epoch 142/250, Batch 1/3, Loss: 0.0204450786113739\n","Validation Loss after Epoch 142: 0.03437083214521408\n","Epoch 143/250, Batch 1/3, Loss: 0.014197863638401031\n","Validation Loss after Epoch 143: 0.03787805140018463\n","Epoch 144/250, Batch 1/3, Loss: 0.014427302405238152\n","Validation Loss after Epoch 144: 0.03846806287765503\n","Epoch 145/250, Batch 1/3, Loss: 0.015463829971849918\n","Validation Loss after Epoch 145: 0.03072889894247055\n","Epoch 146/250, Batch 1/3, Loss: 0.012978700920939445\n","Validation Loss after Epoch 146: 0.03735128417611122\n","Epoch 147/250, Batch 1/3, Loss: 0.01390375941991806\n","Validation Loss after Epoch 147: 0.03379251807928085\n","Epoch 148/250, Batch 1/3, Loss: 0.016027217730879784\n","Validation Loss after Epoch 148: 0.03621424734592438\n","Epoch 149/250, Batch 1/3, Loss: 0.014853403903543949\n","Validation Loss after Epoch 149: 0.03386387974023819\n","Epoch 150/250, Batch 1/3, Loss: 0.014253766275942326\n","Validation Loss after Epoch 150: 0.03378504514694214\n","Epoch 151/250, Batch 1/3, Loss: 0.01192631945014\n","Validation Loss after Epoch 151: 0.03384401276707649\n","Epoch 152/250, Batch 1/3, Loss: 0.01236747670918703\n","Validation Loss after Epoch 152: 0.03576071932911873\n","Epoch 153/250, Batch 1/3, Loss: 0.01172545924782753\n","Validation Loss after Epoch 153: 0.03605400025844574\n","Epoch 154/250, Batch 1/3, Loss: 0.013366865925490856\n","Validation Loss after Epoch 154: 0.04733840748667717\n","Epoch 155/250, Batch 1/3, Loss: 0.010506413877010345\n","Validation Loss after Epoch 155: 0.03569861873984337\n","Epoch 156/250, Batch 1/3, Loss: 0.018808968365192413\n","Validation Loss after Epoch 156: 0.03538568690419197\n","Epoch 157/250, Batch 1/3, Loss: 0.014112095348536968\n","Validation Loss after Epoch 157: 0.03010500781238079\n","Epoch 158/250, Batch 1/3, Loss: 0.012196194380521774\n","Validation Loss after Epoch 158: 0.03160768747329712\n","Epoch 159/250, Batch 1/3, Loss: 0.013581963256001472\n","Validation Loss after Epoch 159: 0.0330195426940918\n","Epoch 160/250, Batch 1/3, Loss: 0.013745332136750221\n","Validation Loss after Epoch 160: 0.034793201833963394\n","Epoch 161/250, Batch 1/3, Loss: 0.013438004069030285\n","Validation Loss after Epoch 161: 0.03472154214978218\n","Epoch 162/250, Batch 1/3, Loss: 0.013845019042491913\n","Validation Loss after Epoch 162: 0.03646591305732727\n","Epoch 163/250, Batch 1/3, Loss: 0.01145625114440918\n","Validation Loss after Epoch 163: 0.03638960421085358\n","Epoch 164/250, Batch 1/3, Loss: 0.012868281453847885\n","Validation Loss after Epoch 164: 0.03387788310647011\n","Epoch 165/250, Batch 1/3, Loss: 0.013084638863801956\n","Validation Loss after Epoch 165: 0.032654453068971634\n","Epoch 166/250, Batch 1/3, Loss: 0.010580088011920452\n","Validation Loss after Epoch 166: 0.038577593863010406\n","Epoch 167/250, Batch 1/3, Loss: 0.011746547184884548\n","Validation Loss after Epoch 167: 0.03529544919729233\n","Epoch 168/250, Batch 1/3, Loss: 0.01024808082729578\n","Validation Loss after Epoch 168: 0.04027111455798149\n","Epoch 169/250, Batch 1/3, Loss: 0.011991193518042564\n","Validation Loss after Epoch 169: 0.033133942633867264\n","Epoch 170/250, Batch 1/3, Loss: 0.010183697566390038\n","Validation Loss after Epoch 170: 0.034493040293455124\n","Epoch 171/250, Batch 1/3, Loss: 0.0166891198605299\n","Validation Loss after Epoch 171: 0.03886759281158447\n","Epoch 172/250, Batch 1/3, Loss: 0.012065332382917404\n","Validation Loss after Epoch 172: 0.043351911008358\n","Epoch 173/250, Batch 1/3, Loss: 0.01267653051763773\n","Validation Loss after Epoch 173: 0.04224323481321335\n","Epoch 174/250, Batch 1/3, Loss: 0.0108953807502985\n","Validation Loss after Epoch 174: 0.037024326622486115\n","Epoch 175/250, Batch 1/3, Loss: 0.01049590203911066\n","Validation Loss after Epoch 175: 0.03513823449611664\n","Epoch 176/250, Batch 1/3, Loss: 0.011695118620991707\n","Validation Loss after Epoch 176: 0.0546543262898922\n","Epoch 177/250, Batch 1/3, Loss: 0.012699604965746403\n","Validation Loss after Epoch 177: 0.0448521263897419\n","Epoch 178/250, Batch 1/3, Loss: 0.013123737648129463\n","Validation Loss after Epoch 178: 0.03766316547989845\n","Epoch 179/250, Batch 1/3, Loss: 0.012868328019976616\n","Validation Loss after Epoch 179: 0.03739437833428383\n","Epoch 180/250, Batch 1/3, Loss: 0.013826659880578518\n","Validation Loss after Epoch 180: 0.044048115611076355\n","Epoch 181/250, Batch 1/3, Loss: 0.013718807138502598\n","Validation Loss after Epoch 181: 0.03186235576868057\n","Epoch 182/250, Batch 1/3, Loss: 0.011337528936564922\n","Validation Loss after Epoch 182: 0.035632260143756866\n","Epoch 183/250, Batch 1/3, Loss: 0.012613020837306976\n","Validation Loss after Epoch 183: 0.033508867025375366\n","Epoch 184/250, Batch 1/3, Loss: 0.018865177407860756\n","Validation Loss after Epoch 184: 0.03615953028202057\n","Epoch 185/250, Batch 1/3, Loss: 0.016984477639198303\n","Validation Loss after Epoch 185: 0.0302299652248621\n","Epoch 186/250, Batch 1/3, Loss: 0.011244889348745346\n","Validation Loss after Epoch 186: 0.03838156536221504\n","Epoch 187/250, Batch 1/3, Loss: 0.016068527474999428\n","Validation Loss after Epoch 187: 0.0315445214509964\n","Epoch 188/250, Batch 1/3, Loss: 0.01093893963843584\n","Validation Loss after Epoch 188: 0.036398932337760925\n","Epoch 189/250, Batch 1/3, Loss: 0.013203680515289307\n","Validation Loss after Epoch 189: 0.03740645945072174\n","Epoch 190/250, Batch 1/3, Loss: 0.011150418780744076\n","Validation Loss after Epoch 190: 0.03768754005432129\n","Epoch 191/250, Batch 1/3, Loss: 0.013782563619315624\n","Validation Loss after Epoch 191: 0.04037608206272125\n","Epoch 192/250, Batch 1/3, Loss: 0.011650217697024345\n","Validation Loss after Epoch 192: 0.030151644721627235\n","Epoch 193/250, Batch 1/3, Loss: 0.016971657052636147\n","Validation Loss after Epoch 193: 0.042190004140138626\n","Epoch 194/250, Batch 1/3, Loss: 0.011235851794481277\n","Validation Loss after Epoch 194: 0.03711728751659393\n","Epoch 195/250, Batch 1/3, Loss: 0.010629446245729923\n","Validation Loss after Epoch 195: 0.049688540399074554\n","Epoch 196/250, Batch 1/3, Loss: 0.012424513697624207\n","Validation Loss after Epoch 196: 0.030601071193814278\n","Epoch 197/250, Batch 1/3, Loss: 0.01186454389244318\n","Validation Loss after Epoch 197: 0.04045923799276352\n","Epoch 198/250, Batch 1/3, Loss: 0.010468197986483574\n","Validation Loss after Epoch 198: 0.03715473785996437\n","Epoch 199/250, Batch 1/3, Loss: 0.011176824569702148\n","Validation Loss after Epoch 199: 0.03414897993206978\n","Epoch 200/250, Batch 1/3, Loss: 0.01124449260532856\n","Validation Loss after Epoch 200: 0.051756151020526886\n","Epoch 201/250, Batch 1/3, Loss: 0.013374753296375275\n","Validation Loss after Epoch 201: 0.033526841551065445\n","Epoch 202/250, Batch 1/3, Loss: 0.018522625789046288\n","Validation Loss after Epoch 202: 0.04150039330124855\n","Epoch 203/250, Batch 1/3, Loss: 0.011446159332990646\n","Validation Loss after Epoch 203: 0.031921111047267914\n","Epoch 204/250, Batch 1/3, Loss: 0.010038563050329685\n","Validation Loss after Epoch 204: 0.03341968357563019\n","Epoch 205/250, Batch 1/3, Loss: 0.010201675817370415\n","Validation Loss after Epoch 205: 0.05046327784657478\n","Epoch 206/250, Batch 1/3, Loss: 0.010421691462397575\n","Validation Loss after Epoch 206: 0.04364052042365074\n","Epoch 207/250, Batch 1/3, Loss: 0.009526856243610382\n","Validation Loss after Epoch 207: 0.05144757404923439\n","Epoch 208/250, Batch 1/3, Loss: 0.010340612381696701\n","Validation Loss after Epoch 208: 0.034008484333753586\n","Epoch 209/250, Batch 1/3, Loss: 0.010828760452568531\n","Validation Loss after Epoch 209: 0.045704007148742676\n","Epoch 210/250, Batch 1/3, Loss: 0.011746736243367195\n","Validation Loss after Epoch 210: 0.04488100856542587\n","Epoch 211/250, Batch 1/3, Loss: 0.009857693687081337\n","Validation Loss after Epoch 211: 0.03659297898411751\n","Epoch 212/250, Batch 1/3, Loss: 0.009405492804944515\n","Validation Loss after Epoch 212: 0.0383472703397274\n","Epoch 213/250, Batch 1/3, Loss: 0.01091870479285717\n","Validation Loss after Epoch 213: 0.037290677428245544\n","Epoch 214/250, Batch 1/3, Loss: 0.00859997421503067\n","Validation Loss after Epoch 214: 0.039446353912353516\n","Epoch 215/250, Batch 1/3, Loss: 0.0086671756580472\n","Validation Loss after Epoch 215: 0.044457435607910156\n","Epoch 216/250, Batch 1/3, Loss: 0.008900237269699574\n","Validation Loss after Epoch 216: 0.03987343981862068\n","Epoch 217/250, Batch 1/3, Loss: 0.007659014780074358\n","Validation Loss after Epoch 217: 0.048957709223032\n","Epoch 218/250, Batch 1/3, Loss: 0.009868869557976723\n","Validation Loss after Epoch 218: 0.03877828270196915\n","Epoch 219/250, Batch 1/3, Loss: 0.0076437825337052345\n","Validation Loss after Epoch 219: 0.03915842995047569\n","Epoch 220/250, Batch 1/3, Loss: 0.00918404757976532\n","Validation Loss after Epoch 220: 0.05720387026667595\n","Epoch 221/250, Batch 1/3, Loss: 0.011768666096031666\n","Validation Loss after Epoch 221: 0.03632563352584839\n","Epoch 222/250, Batch 1/3, Loss: 0.012194143608212471\n","Validation Loss after Epoch 222: 0.038935210555791855\n","Epoch 223/250, Batch 1/3, Loss: 0.01279289461672306\n","Validation Loss after Epoch 223: 0.03285977244377136\n","Epoch 224/250, Batch 1/3, Loss: 0.008823038078844547\n","Validation Loss after Epoch 224: 0.04419136419892311\n","Epoch 225/250, Batch 1/3, Loss: 0.008801046758890152\n","Validation Loss after Epoch 225: 0.04092263802886009\n","Epoch 226/250, Batch 1/3, Loss: 0.007172405254095793\n","Validation Loss after Epoch 226: 0.03968816623091698\n","Epoch 227/250, Batch 1/3, Loss: 0.006987591739743948\n","Validation Loss after Epoch 227: 0.051209788769483566\n","Epoch 228/250, Batch 1/3, Loss: 0.007963557727634907\n","Validation Loss after Epoch 228: 0.03613794222474098\n","Epoch 229/250, Batch 1/3, Loss: 0.007899072952568531\n","Validation Loss after Epoch 229: 0.048463743180036545\n","Epoch 230/250, Batch 1/3, Loss: 0.00839463621377945\n","Validation Loss after Epoch 230: 0.034412696957588196\n","Epoch 231/250, Batch 1/3, Loss: 0.00806474033743143\n","Validation Loss after Epoch 231: 0.03534014895558357\n","Epoch 232/250, Batch 1/3, Loss: 0.0066274781711399555\n","Validation Loss after Epoch 232: 0.04953738674521446\n","Epoch 233/250, Batch 1/3, Loss: 0.00741957314312458\n","Validation Loss after Epoch 233: 0.037791844457387924\n","Epoch 234/250, Batch 1/3, Loss: 0.007732289377599955\n","Validation Loss after Epoch 234: 0.03954241797327995\n","Epoch 235/250, Batch 1/3, Loss: 0.007339159958064556\n","Validation Loss after Epoch 235: 0.04717203974723816\n","Epoch 236/250, Batch 1/3, Loss: 0.007074661087244749\n","Validation Loss after Epoch 236: 0.038933902978897095\n","Epoch 237/250, Batch 1/3, Loss: 0.008014734834432602\n","Validation Loss after Epoch 237: 0.05702831223607063\n","Epoch 238/250, Batch 1/3, Loss: 0.006542930379509926\n","Validation Loss after Epoch 238: 0.04107741639018059\n","Epoch 239/250, Batch 1/3, Loss: 0.0061936331912875175\n","Validation Loss after Epoch 239: 0.04081033170223236\n","Epoch 240/250, Batch 1/3, Loss: 0.00756812421604991\n","Validation Loss after Epoch 240: 0.040291983634233475\n","Epoch 241/250, Batch 1/3, Loss: 0.005837444681674242\n","Validation Loss after Epoch 241: 0.03924719989299774\n","Epoch 242/250, Batch 1/3, Loss: 0.005643137264996767\n","Validation Loss after Epoch 242: 0.04077551141381264\n","Epoch 243/250, Batch 1/3, Loss: 0.005788096226751804\n","Validation Loss after Epoch 243: 0.039260681718587875\n","Epoch 244/250, Batch 1/3, Loss: 0.005441612098366022\n","Validation Loss after Epoch 244: 0.039266254752874374\n","Epoch 245/250, Batch 1/3, Loss: 0.005277368705719709\n","Validation Loss after Epoch 245: 0.052011627703905106\n","Epoch 246/250, Batch 1/3, Loss: 0.006696777883917093\n","Validation Loss after Epoch 246: 0.040258318185806274\n","Epoch 247/250, Batch 1/3, Loss: 0.00518383551388979\n","Validation Loss after Epoch 247: 0.04628109186887741\n","Epoch 248/250, Batch 1/3, Loss: 0.005827595014125109\n","Validation Loss after Epoch 248: 0.04500710219144821\n","Epoch 249/250, Batch 1/3, Loss: 0.005971918813884258\n","Validation Loss after Epoch 249: 0.04372670128941536\n","Epoch 250/250, Batch 1/3, Loss: 0.00540867168456316\n","Validation Loss after Epoch 250: 0.04786580428481102\n","Subset size 125 - Test Loss: 0.0582, Test Accuracy: 98.35%, Average Dice Score: 0.9817\n","\n","Training on subset size: 250\n","Epoch 1/250, Batch 1/6, Loss: 1.1620618104934692\n","Validation Loss after Epoch 1: 1.0498344898223877\n","Epoch 2/250, Batch 1/6, Loss: 0.2280551791191101\n","Validation Loss after Epoch 2: 1.1692847609519958\n","Epoch 3/250, Batch 1/6, Loss: 0.1554471254348755\n","Validation Loss after Epoch 3: 0.9749561548233032\n","Epoch 4/250, Batch 1/6, Loss: 0.11961551755666733\n","Validation Loss after Epoch 4: 0.7711872160434723\n","Epoch 5/250, Batch 1/6, Loss: 0.1163703203201294\n","Validation Loss after Epoch 5: 0.7229596674442291\n","Epoch 6/250, Batch 1/6, Loss: 0.09042608737945557\n","Validation Loss after Epoch 6: 0.6505783796310425\n","Epoch 7/250, Batch 1/6, Loss: 0.07968944311141968\n","Validation Loss after Epoch 7: 0.6419353187084198\n","Epoch 8/250, Batch 1/6, Loss: 0.08275920897722244\n","Validation Loss after Epoch 8: 0.6140618175268173\n","Epoch 9/250, Batch 1/6, Loss: 0.08895035088062286\n","Validation Loss after Epoch 9: 0.5205222517251968\n","Epoch 10/250, Batch 1/6, Loss: 0.05737018212676048\n","Validation Loss after Epoch 10: 0.12955837324261665\n","Epoch 11/250, Batch 1/6, Loss: 0.061683524399995804\n","Validation Loss after Epoch 11: 0.13399795815348625\n","Epoch 12/250, Batch 1/6, Loss: 0.05300052464008331\n","Validation Loss after Epoch 12: 0.10635999962687492\n","Epoch 13/250, Batch 1/6, Loss: 0.056830164045095444\n","Validation Loss after Epoch 13: 0.04800103232264519\n","Epoch 14/250, Batch 1/6, Loss: 0.07697755098342896\n","Validation Loss after Epoch 14: 0.08836102485656738\n","Epoch 15/250, Batch 1/6, Loss: 0.052743278443813324\n","Validation Loss after Epoch 15: 0.045540329068899155\n","Epoch 16/250, Batch 1/6, Loss: 0.054796118289232254\n","Validation Loss after Epoch 16: 0.04245877265930176\n","Epoch 17/250, Batch 1/6, Loss: 0.04825954884290695\n","Validation Loss after Epoch 17: 0.05560004897415638\n","Epoch 18/250, Batch 1/6, Loss: 0.04857668653130531\n","Validation Loss after Epoch 18: 0.04471864178776741\n","Epoch 19/250, Batch 1/6, Loss: 0.04782600328326225\n","Validation Loss after Epoch 19: 0.041516467928886414\n","Epoch 20/250, Batch 1/6, Loss: 0.043351806700229645\n","Validation Loss after Epoch 20: 0.07610929384827614\n","Epoch 21/250, Batch 1/6, Loss: 0.05002112314105034\n","Validation Loss after Epoch 21: 0.04240929335355759\n","Epoch 22/250, Batch 1/6, Loss: 0.04446352273225784\n","Validation Loss after Epoch 22: 0.06440392509102821\n","Epoch 23/250, Batch 1/6, Loss: 0.043964583426713943\n","Validation Loss after Epoch 23: 0.04483129642903805\n","Epoch 24/250, Batch 1/6, Loss: 0.040875039994716644\n","Validation Loss after Epoch 24: 0.042279426008462906\n","Epoch 25/250, Batch 1/6, Loss: 0.03876691311597824\n","Validation Loss after Epoch 25: 0.1301317662000656\n","Epoch 26/250, Batch 1/6, Loss: 0.04515150189399719\n","Validation Loss after Epoch 26: 0.07171117514371872\n","Epoch 27/250, Batch 1/6, Loss: 0.043923232704401016\n","Validation Loss after Epoch 27: 0.04108157195150852\n","Epoch 28/250, Batch 1/6, Loss: 0.04619617387652397\n","Validation Loss after Epoch 28: 0.04927133210003376\n","Epoch 29/250, Batch 1/6, Loss: 0.03870971500873566\n","Validation Loss after Epoch 29: 0.04197441041469574\n","Epoch 30/250, Batch 1/6, Loss: 0.04972846806049347\n","Validation Loss after Epoch 30: 0.03895304165780544\n","Epoch 31/250, Batch 1/6, Loss: 0.04452688246965408\n","Validation Loss after Epoch 31: 0.03528057411313057\n","Epoch 32/250, Batch 1/6, Loss: 0.04476737603545189\n","Validation Loss after Epoch 32: 0.03356318362057209\n","Epoch 33/250, Batch 1/6, Loss: 0.037674982100725174\n","Validation Loss after Epoch 33: 0.03702550567686558\n","Epoch 34/250, Batch 1/6, Loss: 0.0502559132874012\n","Validation Loss after Epoch 34: 0.03337043058127165\n","Epoch 35/250, Batch 1/6, Loss: 0.037094634026288986\n","Validation Loss after Epoch 35: 0.0319342827424407\n","Epoch 36/250, Batch 1/6, Loss: 0.04644671827554703\n","Validation Loss after Epoch 36: 0.04653746634721756\n","Epoch 37/250, Batch 1/6, Loss: 0.03372512385249138\n","Validation Loss after Epoch 37: 0.050406211987137794\n","Epoch 38/250, Batch 1/6, Loss: 0.032061781734228134\n","Validation Loss after Epoch 38: 0.031658719293773174\n","Epoch 39/250, Batch 1/6, Loss: 0.035671167075634\n","Validation Loss after Epoch 39: 0.029755698516964912\n","Epoch 40/250, Batch 1/6, Loss: 0.037143487483263016\n","Validation Loss after Epoch 40: 0.03190098237246275\n","Epoch 41/250, Batch 1/6, Loss: 0.031223714351654053\n","Validation Loss after Epoch 41: 0.034747302532196045\n","Epoch 42/250, Batch 1/6, Loss: 0.02806856483221054\n","Validation Loss after Epoch 42: 0.037218114361166954\n","Epoch 43/250, Batch 1/6, Loss: 0.027506981045007706\n","Validation Loss after Epoch 43: 0.03315821476280689\n","Epoch 44/250, Batch 1/6, Loss: 0.02857033535838127\n","Validation Loss after Epoch 44: 0.032031310722231865\n","Epoch 45/250, Batch 1/6, Loss: 0.031031597405672073\n","Validation Loss after Epoch 45: 0.040814975276589394\n","Epoch 46/250, Batch 1/6, Loss: 0.03044597990810871\n","Validation Loss after Epoch 46: 0.031756067648530006\n","Epoch 47/250, Batch 1/6, Loss: 0.02809438481926918\n","Validation Loss after Epoch 47: 0.030047872103750706\n","Epoch 48/250, Batch 1/6, Loss: 0.03529931604862213\n","Validation Loss after Epoch 48: 0.03061370551586151\n","Epoch 49/250, Batch 1/6, Loss: 0.024380922317504883\n","Validation Loss after Epoch 49: 0.03715428523719311\n","Epoch 50/250, Batch 1/6, Loss: 0.028191059827804565\n","Validation Loss after Epoch 50: 0.03661556355655193\n","Epoch 51/250, Batch 1/6, Loss: 0.0325770378112793\n","Validation Loss after Epoch 51: 0.027987495996057987\n","Epoch 52/250, Batch 1/6, Loss: 0.02844974771142006\n","Validation Loss after Epoch 52: 0.0293194567784667\n","Epoch 53/250, Batch 1/6, Loss: 0.03623849153518677\n","Validation Loss after Epoch 53: 0.03366110101342201\n","Epoch 54/250, Batch 1/6, Loss: 0.03700072318315506\n","Validation Loss after Epoch 54: 0.032842205837368965\n","Epoch 55/250, Batch 1/6, Loss: 0.03159241005778313\n","Validation Loss after Epoch 55: 0.035384438931941986\n","Epoch 56/250, Batch 1/6, Loss: 0.027709046378731728\n","Validation Loss after Epoch 56: 0.028424070216715336\n","Epoch 57/250, Batch 1/6, Loss: 0.03171010687947273\n","Validation Loss after Epoch 57: 0.03367545269429684\n","Epoch 58/250, Batch 1/6, Loss: 0.02488224022090435\n","Validation Loss after Epoch 58: 0.03067079745233059\n","Epoch 59/250, Batch 1/6, Loss: 0.029751956462860107\n","Validation Loss after Epoch 59: 0.030384876765310764\n","Epoch 60/250, Batch 1/6, Loss: 0.02526731789112091\n","Validation Loss after Epoch 60: 0.03530273959040642\n","Epoch 61/250, Batch 1/6, Loss: 0.029787367209792137\n","Validation Loss after Epoch 61: 0.0316776018589735\n","Epoch 62/250, Batch 1/6, Loss: 0.034820180386304855\n","Validation Loss after Epoch 62: 0.028460446745157242\n","Epoch 63/250, Batch 1/6, Loss: 0.03023693524301052\n","Validation Loss after Epoch 63: 0.02974980976432562\n","Epoch 64/250, Batch 1/6, Loss: 0.027806390076875687\n","Validation Loss after Epoch 64: 0.027999280020594597\n","Epoch 65/250, Batch 1/6, Loss: 0.03586958721280098\n","Validation Loss after Epoch 65: 0.0325970146805048\n","Epoch 66/250, Batch 1/6, Loss: 0.02711847424507141\n","Validation Loss after Epoch 66: 0.026285811327397823\n","Epoch 67/250, Batch 1/6, Loss: 0.028278980404138565\n","Validation Loss after Epoch 67: 0.03127128258347511\n","Epoch 68/250, Batch 1/6, Loss: 0.027396373450756073\n","Validation Loss after Epoch 68: 0.031210019253194332\n","Epoch 69/250, Batch 1/6, Loss: 0.0266756322234869\n","Validation Loss after Epoch 69: 0.02731422707438469\n","Epoch 70/250, Batch 1/6, Loss: 0.02808382734656334\n","Validation Loss after Epoch 70: 0.04031104035675526\n","Epoch 71/250, Batch 1/6, Loss: 0.03708760067820549\n","Validation Loss after Epoch 71: 0.03579803183674812\n","Epoch 72/250, Batch 1/6, Loss: 0.02709386684000492\n","Validation Loss after Epoch 72: 0.031012501567602158\n","Epoch 73/250, Batch 1/6, Loss: 0.02738221362233162\n","Validation Loss after Epoch 73: 0.02667334768921137\n","Epoch 74/250, Batch 1/6, Loss: 0.031422924250364304\n","Validation Loss after Epoch 74: 0.026615566574037075\n","Epoch 75/250, Batch 1/6, Loss: 0.0316927507519722\n","Validation Loss after Epoch 75: 0.026378244161605835\n","Epoch 76/250, Batch 1/6, Loss: 0.02396579273045063\n","Validation Loss after Epoch 76: 0.02631922159343958\n","Epoch 77/250, Batch 1/6, Loss: 0.02097206562757492\n","Validation Loss after Epoch 77: 0.03177903965115547\n","Epoch 78/250, Batch 1/6, Loss: 0.027810873463749886\n","Validation Loss after Epoch 78: 0.02493004873394966\n","Epoch 79/250, Batch 1/6, Loss: 0.02524871937930584\n","Validation Loss after Epoch 79: 0.038972605019807816\n","Epoch 80/250, Batch 1/6, Loss: 0.023571191355586052\n","Validation Loss after Epoch 80: 0.026846514083445072\n","Epoch 81/250, Batch 1/6, Loss: 0.021943090483546257\n","Validation Loss after Epoch 81: 0.03739551454782486\n","Epoch 82/250, Batch 1/6, Loss: 0.025824781507253647\n","Validation Loss after Epoch 82: 0.03677305579185486\n","Epoch 83/250, Batch 1/6, Loss: 0.022714948281645775\n","Validation Loss after Epoch 83: 0.028709269128739834\n","Epoch 84/250, Batch 1/6, Loss: 0.019604943692684174\n","Validation Loss after Epoch 84: 0.030755270272493362\n","Epoch 85/250, Batch 1/6, Loss: 0.024611223489046097\n","Validation Loss after Epoch 85: 0.029344338923692703\n","Epoch 86/250, Batch 1/6, Loss: 0.02335219644010067\n","Validation Loss after Epoch 86: 0.025649345479905605\n","Epoch 87/250, Batch 1/6, Loss: 0.028493953868746758\n","Validation Loss after Epoch 87: 0.026474758982658386\n","Epoch 88/250, Batch 1/6, Loss: 0.023423105478286743\n","Validation Loss after Epoch 88: 0.026452677324414253\n","Epoch 89/250, Batch 1/6, Loss: 0.02106456272304058\n","Validation Loss after Epoch 89: 0.02866434119641781\n","Epoch 90/250, Batch 1/6, Loss: 0.022020218893885612\n","Validation Loss after Epoch 90: 0.024844984523952007\n","Epoch 91/250, Batch 1/6, Loss: 0.02092575840651989\n","Validation Loss after Epoch 91: 0.028573290444910526\n","Epoch 92/250, Batch 1/6, Loss: 0.026731930673122406\n","Validation Loss after Epoch 92: 0.024190041236579418\n","Epoch 93/250, Batch 1/6, Loss: 0.024628225713968277\n","Validation Loss after Epoch 93: 0.025625615380704403\n","Epoch 94/250, Batch 1/6, Loss: 0.018950212746858597\n","Validation Loss after Epoch 94: 0.026250329799950123\n","Epoch 95/250, Batch 1/6, Loss: 0.021124519407749176\n","Validation Loss after Epoch 95: 0.02584206499159336\n","Epoch 96/250, Batch 1/6, Loss: 0.01899236999452114\n","Validation Loss after Epoch 96: 0.02877085655927658\n","Epoch 97/250, Batch 1/6, Loss: 0.02139052376151085\n","Validation Loss after Epoch 97: 0.026657670736312866\n","Epoch 98/250, Batch 1/6, Loss: 0.02073088102042675\n","Validation Loss after Epoch 98: 0.02728841919451952\n","Epoch 99/250, Batch 1/6, Loss: 0.02326117642223835\n","Validation Loss after Epoch 99: 0.025431334972381592\n","Epoch 100/250, Batch 1/6, Loss: 0.022235803306102753\n","Validation Loss after Epoch 100: 0.04401033744215965\n","Epoch 101/250, Batch 1/6, Loss: 0.021547749638557434\n","Validation Loss after Epoch 101: 0.030923504382371902\n","Epoch 102/250, Batch 1/6, Loss: 0.024622725322842598\n","Validation Loss after Epoch 102: 0.028594686649739742\n","Epoch 103/250, Batch 1/6, Loss: 0.02187068574130535\n","Validation Loss after Epoch 103: 0.02893047872930765\n","Epoch 104/250, Batch 1/6, Loss: 0.021630382165312767\n","Validation Loss after Epoch 104: 0.028256770223379135\n","Epoch 105/250, Batch 1/6, Loss: 0.023718714714050293\n","Validation Loss after Epoch 105: 0.02594779711216688\n","Epoch 106/250, Batch 1/6, Loss: 0.021893683820962906\n","Validation Loss after Epoch 106: 0.023717611096799374\n","Epoch 107/250, Batch 1/6, Loss: 0.018232880160212517\n","Validation Loss after Epoch 107: 0.026067117229104042\n","Epoch 108/250, Batch 1/6, Loss: 0.019231950864195824\n","Validation Loss after Epoch 108: 0.026268712244927883\n","Epoch 109/250, Batch 1/6, Loss: 0.019048670306801796\n","Validation Loss after Epoch 109: 0.025039552710950375\n","Epoch 110/250, Batch 1/6, Loss: 0.01905844360589981\n","Validation Loss after Epoch 110: 0.026888270862400532\n","Epoch 111/250, Batch 1/6, Loss: 0.020832130685448647\n","Validation Loss after Epoch 111: 0.027186302468180656\n","Epoch 112/250, Batch 1/6, Loss: 0.018105989322066307\n","Validation Loss after Epoch 112: 0.02765902318060398\n","Epoch 113/250, Batch 1/6, Loss: 0.02269113063812256\n","Validation Loss after Epoch 113: 0.023823133669793606\n","Epoch 114/250, Batch 1/6, Loss: 0.021463515236973763\n","Validation Loss after Epoch 114: 0.02711843978613615\n","Epoch 115/250, Batch 1/6, Loss: 0.021059278398752213\n","Validation Loss after Epoch 115: 0.02944653108716011\n","Epoch 116/250, Batch 1/6, Loss: 0.023318085819482803\n","Validation Loss after Epoch 116: 0.027387846261262894\n","Epoch 117/250, Batch 1/6, Loss: 0.018129201605916023\n","Validation Loss after Epoch 117: 0.026250991970300674\n","Epoch 118/250, Batch 1/6, Loss: 0.018317969515919685\n","Validation Loss after Epoch 118: 0.02703272458165884\n","Epoch 119/250, Batch 1/6, Loss: 0.017319316044449806\n","Validation Loss after Epoch 119: 0.024913949891924858\n","Epoch 120/250, Batch 1/6, Loss: 0.02626357600092888\n","Validation Loss after Epoch 120: 0.025404575280845165\n","Epoch 121/250, Batch 1/6, Loss: 0.01677144691348076\n","Validation Loss after Epoch 121: 0.025081980042159557\n","Epoch 122/250, Batch 1/6, Loss: 0.01681898534297943\n","Validation Loss after Epoch 122: 0.02454458922147751\n","Epoch 123/250, Batch 1/6, Loss: 0.018686987459659576\n","Validation Loss after Epoch 123: 0.022574354894459248\n","Epoch 124/250, Batch 1/6, Loss: 0.01679890975356102\n","Validation Loss after Epoch 124: 0.02455358672887087\n","Epoch 125/250, Batch 1/6, Loss: 0.01717451587319374\n","Validation Loss after Epoch 125: 0.02505541406571865\n","Epoch 126/250, Batch 1/6, Loss: 0.01904880255460739\n","Validation Loss after Epoch 126: 0.02671300619840622\n","Epoch 127/250, Batch 1/6, Loss: 0.016127847135066986\n","Validation Loss after Epoch 127: 0.02858807798475027\n","Epoch 128/250, Batch 1/6, Loss: 0.01632288657128811\n","Validation Loss after Epoch 128: 0.02565142698585987\n","Epoch 129/250, Batch 1/6, Loss: 0.0249223243445158\n","Validation Loss after Epoch 129: 0.024680214934051037\n","Epoch 130/250, Batch 1/6, Loss: 0.015612169168889523\n","Validation Loss after Epoch 130: 0.022360273636877537\n","Epoch 131/250, Batch 1/6, Loss: 0.0189357902854681\n","Validation Loss after Epoch 131: 0.023026619106531143\n","Epoch 132/250, Batch 1/6, Loss: 0.016642719507217407\n","Validation Loss after Epoch 132: 0.02559436671435833\n","Epoch 133/250, Batch 1/6, Loss: 0.0217556431889534\n","Validation Loss after Epoch 133: 0.02615559007972479\n","Epoch 134/250, Batch 1/6, Loss: 0.01956506259739399\n","Validation Loss after Epoch 134: 0.026549894362688065\n","Epoch 135/250, Batch 1/6, Loss: 0.03701464831829071\n","Validation Loss after Epoch 135: 0.03606431558728218\n","Epoch 136/250, Batch 1/6, Loss: 0.07170271873474121\n","Validation Loss after Epoch 136: 0.04891636595129967\n","Epoch 137/250, Batch 1/6, Loss: 0.0233350470662117\n","Validation Loss after Epoch 137: 0.05817391723394394\n","Epoch 138/250, Batch 1/6, Loss: 0.023286867886781693\n","Validation Loss after Epoch 138: 0.04678934067487717\n","Epoch 139/250, Batch 1/6, Loss: 0.032722458243370056\n","Validation Loss after Epoch 139: 0.0269735399633646\n","Epoch 140/250, Batch 1/6, Loss: 0.03317660093307495\n","Validation Loss after Epoch 140: 0.034960031509399414\n","Epoch 141/250, Batch 1/6, Loss: 0.027762768790125847\n","Validation Loss after Epoch 141: 0.03144271019846201\n","Epoch 142/250, Batch 1/6, Loss: 0.02382199838757515\n","Validation Loss after Epoch 142: 0.027207855135202408\n","Epoch 143/250, Batch 1/6, Loss: 0.0200799573212862\n","Validation Loss after Epoch 143: 0.027228837832808495\n","Epoch 144/250, Batch 1/6, Loss: 0.020068861544132233\n","Validation Loss after Epoch 144: 0.024787919595837593\n","Epoch 145/250, Batch 1/6, Loss: 0.017145369201898575\n","Validation Loss after Epoch 145: 0.027469070628285408\n","Epoch 146/250, Batch 1/6, Loss: 0.01641378551721573\n","Validation Loss after Epoch 146: 0.026903996244072914\n","Epoch 147/250, Batch 1/6, Loss: 0.016268912702798843\n","Validation Loss after Epoch 147: 0.02831549197435379\n","Epoch 148/250, Batch 1/6, Loss: 0.01612269878387451\n","Validation Loss after Epoch 148: 0.02189639862626791\n","Epoch 149/250, Batch 1/6, Loss: 0.017183955758810043\n","Validation Loss after Epoch 149: 0.03131155110895634\n","Epoch 150/250, Batch 1/6, Loss: 0.017546135932207108\n","Validation Loss after Epoch 150: 0.02432384341955185\n","Epoch 151/250, Batch 1/6, Loss: 0.01991400495171547\n","Validation Loss after Epoch 151: 0.025327584706246853\n","Epoch 152/250, Batch 1/6, Loss: 0.021865425631403923\n","Validation Loss after Epoch 152: 0.024763532914221287\n","Epoch 153/250, Batch 1/6, Loss: 0.017544740810990334\n","Validation Loss after Epoch 153: 0.024389425292611122\n","Epoch 154/250, Batch 1/6, Loss: 0.020785527303814888\n","Validation Loss after Epoch 154: 0.024011136032640934\n","Epoch 155/250, Batch 1/6, Loss: 0.014882584102451801\n","Validation Loss after Epoch 155: 0.025882930494844913\n","Epoch 156/250, Batch 1/6, Loss: 0.01852862350642681\n","Validation Loss after Epoch 156: 0.02246849052608013\n","Epoch 157/250, Batch 1/6, Loss: 0.01921752654016018\n","Validation Loss after Epoch 157: 0.028216996230185032\n","Epoch 158/250, Batch 1/6, Loss: 0.015963174402713776\n","Validation Loss after Epoch 158: 0.024102242663502693\n","Epoch 159/250, Batch 1/6, Loss: 0.01712164282798767\n","Validation Loss after Epoch 159: 0.02700386382639408\n","Epoch 160/250, Batch 1/6, Loss: 0.015490253455936909\n","Validation Loss after Epoch 160: 0.025057432241737843\n","Epoch 161/250, Batch 1/6, Loss: 0.0153485843911767\n","Validation Loss after Epoch 161: 0.022943557240068913\n","Epoch 162/250, Batch 1/6, Loss: 0.018173018470406532\n","Validation Loss after Epoch 162: 0.023117702454328537\n","Epoch 163/250, Batch 1/6, Loss: 0.019677862524986267\n","Validation Loss after Epoch 163: 0.024068349041044712\n","Epoch 164/250, Batch 1/6, Loss: 0.02133677899837494\n","Validation Loss after Epoch 164: 0.02551398240029812\n","Epoch 165/250, Batch 1/6, Loss: 0.015066259540617466\n","Validation Loss after Epoch 165: 0.03362728841602802\n","Epoch 166/250, Batch 1/6, Loss: 0.014083132147789001\n","Validation Loss after Epoch 166: 0.0290611470118165\n","Epoch 167/250, Batch 1/6, Loss: 0.01599840074777603\n","Validation Loss after Epoch 167: 0.028841973282396793\n","Epoch 168/250, Batch 1/6, Loss: 0.014042490161955357\n","Validation Loss after Epoch 168: 0.025356861762702465\n","Epoch 169/250, Batch 1/6, Loss: 0.015840735286474228\n","Validation Loss after Epoch 169: 0.023463129997253418\n","Epoch 170/250, Batch 1/6, Loss: 0.015978967770934105\n","Validation Loss after Epoch 170: 0.022471674717962742\n","Epoch 171/250, Batch 1/6, Loss: 0.012289170175790787\n","Validation Loss after Epoch 171: 0.026357514783740044\n","Epoch 172/250, Batch 1/6, Loss: 0.015917999669909477\n","Validation Loss after Epoch 172: 0.03259188495576382\n","Epoch 173/250, Batch 1/6, Loss: 0.02673477679491043\n","Validation Loss after Epoch 173: 0.02519961539655924\n","Epoch 174/250, Batch 1/6, Loss: 0.014378693886101246\n","Validation Loss after Epoch 174: 0.025926942005753517\n","Epoch 175/250, Batch 1/6, Loss: 0.01261233538389206\n","Validation Loss after Epoch 175: 0.027319401502609253\n","Epoch 176/250, Batch 1/6, Loss: 0.014236675575375557\n","Validation Loss after Epoch 176: 0.02367127314209938\n","Epoch 177/250, Batch 1/6, Loss: 0.013116595335304737\n","Validation Loss after Epoch 177: 0.02318837959319353\n","Epoch 178/250, Batch 1/6, Loss: 0.014521594159305096\n","Validation Loss after Epoch 178: 0.022712641395628452\n","Epoch 179/250, Batch 1/6, Loss: 0.012585814110934734\n","Validation Loss after Epoch 179: 0.023584351874887943\n","Epoch 180/250, Batch 1/6, Loss: 0.011332985945045948\n","Validation Loss after Epoch 180: 0.023920376785099506\n","Epoch 181/250, Batch 1/6, Loss: 0.013881289400160313\n","Validation Loss after Epoch 181: 0.02531432919204235\n","Epoch 182/250, Batch 1/6, Loss: 0.014432178810238838\n","Validation Loss after Epoch 182: 0.02334433700889349\n","Epoch 183/250, Batch 1/6, Loss: 0.015145113691687584\n","Validation Loss after Epoch 183: 0.024037902243435383\n","Epoch 184/250, Batch 1/6, Loss: 0.013811455108225346\n","Validation Loss after Epoch 184: 0.02379400935024023\n","Epoch 185/250, Batch 1/6, Loss: 0.012152192182838917\n","Validation Loss after Epoch 185: 0.023360504768788815\n","Epoch 186/250, Batch 1/6, Loss: 0.010774156078696251\n","Validation Loss after Epoch 186: 0.02350635640323162\n","Epoch 187/250, Batch 1/6, Loss: 0.01074480265378952\n","Validation Loss after Epoch 187: 0.02347472496330738\n","Epoch 188/250, Batch 1/6, Loss: 0.012333455495536327\n","Validation Loss after Epoch 188: 0.026043658144772053\n","Epoch 189/250, Batch 1/6, Loss: 0.013123259879648685\n","Validation Loss after Epoch 189: 0.024373270571231842\n","Epoch 190/250, Batch 1/6, Loss: 0.010473662987351418\n","Validation Loss after Epoch 190: 0.022683587856590748\n","Epoch 191/250, Batch 1/6, Loss: 0.011586586013436317\n","Validation Loss after Epoch 191: 0.02342813555151224\n","Epoch 192/250, Batch 1/6, Loss: 0.013360251672565937\n","Validation Loss after Epoch 192: 0.023913242854177952\n","Epoch 193/250, Batch 1/6, Loss: 0.013184532523155212\n","Validation Loss after Epoch 193: 0.029611569829285145\n","Epoch 194/250, Batch 1/6, Loss: 0.012097762897610664\n","Validation Loss after Epoch 194: 0.02257660962641239\n","Epoch 195/250, Batch 1/6, Loss: 0.013416493311524391\n","Validation Loss after Epoch 195: 0.022952109575271606\n","Epoch 196/250, Batch 1/6, Loss: 0.010557468049228191\n","Validation Loss after Epoch 196: 0.02751982305198908\n","Epoch 197/250, Batch 1/6, Loss: 0.011086183600127697\n","Validation Loss after Epoch 197: 0.027478016912937164\n","Epoch 198/250, Batch 1/6, Loss: 0.016476554796099663\n","Validation Loss after Epoch 198: 0.023911627009510994\n","Epoch 199/250, Batch 1/6, Loss: 0.012018234468996525\n","Validation Loss after Epoch 199: 0.023854714818298817\n","Epoch 200/250, Batch 1/6, Loss: 0.010645768605172634\n","Validation Loss after Epoch 200: 0.023462063632905483\n","Epoch 201/250, Batch 1/6, Loss: 0.012112668715417385\n","Validation Loss after Epoch 201: 0.02174057811498642\n","Epoch 202/250, Batch 1/6, Loss: 0.014511434361338615\n","Validation Loss after Epoch 202: 0.023323913104832172\n","Epoch 203/250, Batch 1/6, Loss: 0.011126972734928131\n","Validation Loss after Epoch 203: 0.023284826427698135\n","Epoch 204/250, Batch 1/6, Loss: 0.01140223816037178\n","Validation Loss after Epoch 204: 0.026773599907755852\n","Epoch 205/250, Batch 1/6, Loss: 0.013700257055461407\n","Validation Loss after Epoch 205: 0.025328252464532852\n","Epoch 206/250, Batch 1/6, Loss: 0.01063165906816721\n","Validation Loss after Epoch 206: 0.024769817478954792\n","Epoch 207/250, Batch 1/6, Loss: 0.011561251245439053\n","Validation Loss after Epoch 207: 0.026328686624765396\n","Epoch 208/250, Batch 1/6, Loss: 0.011422481387853622\n","Validation Loss after Epoch 208: 0.02678376715630293\n","Epoch 209/250, Batch 1/6, Loss: 0.010266108438372612\n","Validation Loss after Epoch 209: 0.02355548646301031\n","Epoch 210/250, Batch 1/6, Loss: 0.009454429149627686\n","Validation Loss after Epoch 210: 0.023859737440943718\n","Epoch 211/250, Batch 1/6, Loss: 0.00837580393999815\n","Validation Loss after Epoch 211: 0.025341495871543884\n","Epoch 212/250, Batch 1/6, Loss: 0.009523727931082249\n","Validation Loss after Epoch 212: 0.023759322240948677\n","Epoch 213/250, Batch 1/6, Loss: 0.008610039949417114\n","Validation Loss after Epoch 213: 0.02445570658892393\n","Epoch 214/250, Batch 1/6, Loss: 0.010766800493001938\n","Validation Loss after Epoch 214: 0.0245589641854167\n","Epoch 215/250, Batch 1/6, Loss: 0.009571687318384647\n","Validation Loss after Epoch 215: 0.023531011305749416\n","Epoch 216/250, Batch 1/6, Loss: 0.008697756566107273\n","Validation Loss after Epoch 216: 0.024202125146985054\n","Epoch 217/250, Batch 1/6, Loss: 0.010887774638831615\n","Validation Loss after Epoch 217: 0.02980825211852789\n","Epoch 218/250, Batch 1/6, Loss: 0.009540867991745472\n","Validation Loss after Epoch 218: 0.027530542574822903\n","Epoch 219/250, Batch 1/6, Loss: 0.00962817296385765\n","Validation Loss after Epoch 219: 0.026485989801585674\n","Epoch 220/250, Batch 1/6, Loss: 0.01200417522341013\n","Validation Loss after Epoch 220: 0.025478173978626728\n","Epoch 221/250, Batch 1/6, Loss: 0.010265754535794258\n","Validation Loss after Epoch 221: 0.027735136449337006\n","Epoch 222/250, Batch 1/6, Loss: 0.010105332359671593\n","Validation Loss after Epoch 222: 0.02965424209833145\n","Epoch 223/250, Batch 1/6, Loss: 0.013246105052530766\n","Validation Loss after Epoch 223: 0.025295989587903023\n","Epoch 224/250, Batch 1/6, Loss: 0.009720445610582829\n","Validation Loss after Epoch 224: 0.02569661196321249\n","Epoch 225/250, Batch 1/6, Loss: 0.00924304872751236\n","Validation Loss after Epoch 225: 0.02575329039245844\n","Epoch 226/250, Batch 1/6, Loss: 0.010927317664027214\n","Validation Loss after Epoch 226: 0.02769803535193205\n","Epoch 227/250, Batch 1/6, Loss: 0.009356856346130371\n","Validation Loss after Epoch 227: 0.026392808184027672\n","Epoch 228/250, Batch 1/6, Loss: 0.009136263281106949\n","Validation Loss after Epoch 228: 0.02744672540575266\n","Epoch 229/250, Batch 1/6, Loss: 0.008521812967956066\n","Validation Loss after Epoch 229: 0.027378998696804047\n","Epoch 230/250, Batch 1/6, Loss: 0.01081888098269701\n","Validation Loss after Epoch 230: 0.025556189008057117\n","Epoch 231/250, Batch 1/6, Loss: 0.010736861266195774\n","Validation Loss after Epoch 231: 0.024850978516042233\n","Epoch 232/250, Batch 1/6, Loss: 0.012873174622654915\n","Validation Loss after Epoch 232: 0.03155588451772928\n","Epoch 233/250, Batch 1/6, Loss: 0.03479532152414322\n","Validation Loss after Epoch 233: 0.03892088867723942\n","Epoch 234/250, Batch 1/6, Loss: 0.045818619430065155\n","Validation Loss after Epoch 234: 0.061251308768987656\n","Epoch 235/250, Batch 1/6, Loss: 0.023401208221912384\n","Validation Loss after Epoch 235: 0.0380487646907568\n","Epoch 236/250, Batch 1/6, Loss: 0.01790248602628708\n","Validation Loss after Epoch 236: 0.06662042438983917\n","Epoch 237/250, Batch 1/6, Loss: 0.02025255747139454\n","Validation Loss after Epoch 237: 0.02306271158158779\n","Epoch 238/250, Batch 1/6, Loss: 0.014544589444994926\n","Validation Loss after Epoch 238: 0.023428061977028847\n","Epoch 239/250, Batch 1/6, Loss: 0.014797399751842022\n","Validation Loss after Epoch 239: 0.025647945702075958\n","Epoch 240/250, Batch 1/6, Loss: 0.023457210510969162\n","Validation Loss after Epoch 240: 0.02697632648050785\n","Epoch 241/250, Batch 1/6, Loss: 0.012532342225313187\n","Validation Loss after Epoch 241: 0.023134129121899605\n","Epoch 242/250, Batch 1/6, Loss: 0.015028744004666805\n","Validation Loss after Epoch 242: 0.02669889573007822\n","Epoch 243/250, Batch 1/6, Loss: 0.015588453970849514\n","Validation Loss after Epoch 243: 0.024169652722775936\n","Epoch 244/250, Batch 1/6, Loss: 0.012759532779455185\n","Validation Loss after Epoch 244: 0.029187885113060474\n","Epoch 245/250, Batch 1/6, Loss: 0.01551991980522871\n","Validation Loss after Epoch 245: 0.02716971654444933\n","Epoch 246/250, Batch 1/6, Loss: 0.013653903268277645\n","Validation Loss after Epoch 246: 0.02687513642013073\n","Epoch 247/250, Batch 1/6, Loss: 0.013825568370521069\n","Validation Loss after Epoch 247: 0.025754738599061966\n","Epoch 248/250, Batch 1/6, Loss: 0.009382685646414757\n","Validation Loss after Epoch 248: 0.025545316748321056\n","Epoch 249/250, Batch 1/6, Loss: 0.00840089377015829\n","Validation Loss after Epoch 249: 0.028512082062661648\n","Epoch 250/250, Batch 1/6, Loss: 0.010099958628416061\n","Validation Loss after Epoch 250: 0.024861063808202744\n","Subset size 250 - Test Loss: 0.0225, Test Accuracy: 99.07%, Average Dice Score: 0.9905\n","\n","Training on subset size: 500\n","Epoch 1/250, Batch 1/11, Loss: 1.1236438751220703\n","Epoch 1/250, Batch 11/11, Loss: 0.17965981364250183\n","Validation Loss after Epoch 1: 0.9798169930775961\n","Epoch 2/250, Batch 1/11, Loss: 0.1729620397090912\n","Epoch 2/250, Batch 11/11, Loss: 0.11797880381345749\n","Validation Loss after Epoch 2: 0.8046200275421143\n","Epoch 3/250, Batch 1/11, Loss: 0.11718790233135223\n","Epoch 3/250, Batch 11/11, Loss: 0.08421920984983444\n","Validation Loss after Epoch 3: 0.7246241966883341\n","Epoch 4/250, Batch 1/11, Loss: 0.10571154206991196\n","Epoch 4/250, Batch 11/11, Loss: 0.07059839367866516\n","Validation Loss after Epoch 4: 0.6125139792760214\n","Epoch 5/250, Batch 1/11, Loss: 0.09225338697433472\n","Epoch 5/250, Batch 11/11, Loss: 0.08955308794975281\n","Validation Loss after Epoch 5: 0.44753941893577576\n","Epoch 6/250, Batch 1/11, Loss: 0.07052936404943466\n","Epoch 6/250, Batch 11/11, Loss: 0.05728384479880333\n","Validation Loss after Epoch 6: 0.06498994181553523\n","Epoch 7/250, Batch 1/11, Loss: 0.06399369239807129\n","Epoch 7/250, Batch 11/11, Loss: 0.04915059730410576\n","Validation Loss after Epoch 7: 0.05149484798312187\n","Epoch 8/250, Batch 1/11, Loss: 0.06320980191230774\n","Epoch 8/250, Batch 11/11, Loss: 0.05958278104662895\n","Validation Loss after Epoch 8: 0.049615200608968735\n","Epoch 9/250, Batch 1/11, Loss: 0.05674707517027855\n","Epoch 9/250, Batch 11/11, Loss: 0.053986068814992905\n","Validation Loss after Epoch 9: 0.051179484774669014\n","Epoch 10/250, Batch 1/11, Loss: 0.05336343124508858\n","Epoch 10/250, Batch 11/11, Loss: 0.05180114507675171\n","Validation Loss after Epoch 10: 0.04522438968221346\n","Epoch 11/250, Batch 1/11, Loss: 0.044161781668663025\n","Epoch 11/250, Batch 11/11, Loss: 0.06108696386218071\n","Validation Loss after Epoch 11: 0.043922893702983856\n","Epoch 12/250, Batch 1/11, Loss: 0.046604353934526443\n","Epoch 12/250, Batch 11/11, Loss: 0.04171106591820717\n","Validation Loss after Epoch 12: 0.038794552286465965\n","Epoch 13/250, Batch 1/11, Loss: 0.05473775044083595\n","Epoch 13/250, Batch 11/11, Loss: 0.040864333510398865\n","Validation Loss after Epoch 13: 0.03707768519719442\n","Epoch 14/250, Batch 1/11, Loss: 0.045869890600442886\n","Epoch 14/250, Batch 11/11, Loss: 0.060081712901592255\n","Validation Loss after Epoch 14: 0.04463625326752663\n","Epoch 15/250, Batch 1/11, Loss: 0.06375842541456223\n","Epoch 15/250, Batch 11/11, Loss: 0.04561222344636917\n","Validation Loss after Epoch 15: 0.0386941134929657\n","Epoch 16/250, Batch 1/11, Loss: 0.04576265439391136\n","Epoch 16/250, Batch 11/11, Loss: 0.03691256418824196\n","Validation Loss after Epoch 16: 0.05246296897530556\n","Epoch 17/250, Batch 1/11, Loss: 0.053329627960920334\n","Epoch 17/250, Batch 11/11, Loss: 0.04879014566540718\n","Validation Loss after Epoch 17: 0.039689164608716965\n","Epoch 18/250, Batch 1/11, Loss: 0.04587545245885849\n","Epoch 18/250, Batch 11/11, Loss: 0.05300789698958397\n","Validation Loss after Epoch 18: 0.051980809619029365\n","Epoch 19/250, Batch 1/11, Loss: 0.04356708005070686\n","Epoch 19/250, Batch 11/11, Loss: 0.032737527042627335\n","Validation Loss after Epoch 19: 0.036319067080815635\n","Epoch 20/250, Batch 1/11, Loss: 0.043091028928756714\n","Epoch 20/250, Batch 11/11, Loss: 0.033448152244091034\n","Validation Loss after Epoch 20: 0.030217039088408153\n","Epoch 21/250, Batch 1/11, Loss: 0.03368500992655754\n","Epoch 21/250, Batch 11/11, Loss: 0.03235713765025139\n","Validation Loss after Epoch 21: 0.04325665285189947\n","Epoch 22/250, Batch 1/11, Loss: 0.06853882223367691\n","Epoch 22/250, Batch 11/11, Loss: 0.031878888607025146\n","Validation Loss after Epoch 22: 0.031090697273612022\n","Epoch 23/250, Batch 1/11, Loss: 0.039317693561315536\n","Epoch 23/250, Batch 11/11, Loss: 0.04790472239255905\n","Validation Loss after Epoch 23: 0.03267728599409262\n","Epoch 24/250, Batch 1/11, Loss: 0.031012313440442085\n","Epoch 24/250, Batch 11/11, Loss: 0.05109024792909622\n","Validation Loss after Epoch 24: 0.029384715482592583\n","Epoch 25/250, Batch 1/11, Loss: 0.03575357794761658\n","Epoch 25/250, Batch 11/11, Loss: 0.030718212947249413\n","Validation Loss after Epoch 25: 0.03660643224914869\n","Epoch 26/250, Batch 1/11, Loss: 0.02925879694521427\n","Epoch 26/250, Batch 11/11, Loss: 0.03145139291882515\n","Validation Loss after Epoch 26: 0.031779068832596145\n","Epoch 27/250, Batch 1/11, Loss: 0.039404671639204025\n","Epoch 27/250, Batch 11/11, Loss: 0.04248225688934326\n","Validation Loss after Epoch 27: 0.0385969877243042\n","Epoch 28/250, Batch 1/11, Loss: 0.029023045673966408\n","Epoch 28/250, Batch 11/11, Loss: 0.0300553347915411\n","Validation Loss after Epoch 28: 0.03066062492628892\n","Epoch 29/250, Batch 1/11, Loss: 0.03601600229740143\n","Epoch 29/250, Batch 11/11, Loss: 0.02788134478032589\n","Validation Loss after Epoch 29: 0.03126073318223158\n","Epoch 30/250, Batch 1/11, Loss: 0.03345637023448944\n","Epoch 30/250, Batch 11/11, Loss: 0.031589336693286896\n","Validation Loss after Epoch 30: 0.03372729135056337\n","Epoch 31/250, Batch 1/11, Loss: 0.030360355973243713\n","Epoch 31/250, Batch 11/11, Loss: 0.03236396983265877\n","Validation Loss after Epoch 31: 0.03424319004019102\n","Epoch 32/250, Batch 1/11, Loss: 0.029283298179507256\n","Epoch 32/250, Batch 11/11, Loss: 0.02959289401769638\n","Validation Loss after Epoch 32: 0.03143521584570408\n","Epoch 33/250, Batch 1/11, Loss: 0.029416387900710106\n","Epoch 33/250, Batch 11/11, Loss: 0.028179563581943512\n","Validation Loss after Epoch 33: 0.02948794203499953\n","Epoch 34/250, Batch 1/11, Loss: 0.02824654057621956\n","Epoch 34/250, Batch 11/11, Loss: 0.03336044028401375\n","Validation Loss after Epoch 34: 0.02932118314007918\n","Epoch 35/250, Batch 1/11, Loss: 0.04471893236041069\n","Epoch 35/250, Batch 11/11, Loss: 0.028912045061588287\n","Validation Loss after Epoch 35: 0.03137750985721747\n","Epoch 36/250, Batch 1/11, Loss: 0.02738429605960846\n","Epoch 36/250, Batch 11/11, Loss: 0.029509251937270164\n","Validation Loss after Epoch 36: 0.02796437218785286\n","Epoch 37/250, Batch 1/11, Loss: 0.03327089175581932\n","Epoch 37/250, Batch 11/11, Loss: 0.02946472354233265\n","Validation Loss after Epoch 37: 0.02693960877756278\n","Epoch 38/250, Batch 1/11, Loss: 0.030443262308835983\n","Epoch 38/250, Batch 11/11, Loss: 0.0293401088565588\n","Validation Loss after Epoch 38: 0.03237620865305265\n","Epoch 39/250, Batch 1/11, Loss: 0.024531207978725433\n","Epoch 39/250, Batch 11/11, Loss: 0.030275480821728706\n","Validation Loss after Epoch 39: 0.024903065835436184\n","Epoch 40/250, Batch 1/11, Loss: 0.026782192289829254\n","Epoch 40/250, Batch 11/11, Loss: 0.02921021543443203\n","Validation Loss after Epoch 40: 0.02763914130628109\n","Epoch 41/250, Batch 1/11, Loss: 0.025120701640844345\n","Epoch 41/250, Batch 11/11, Loss: 0.022825904190540314\n","Validation Loss after Epoch 41: 0.027043366183837254\n","Epoch 42/250, Batch 1/11, Loss: 0.026101399213075638\n","Epoch 42/250, Batch 11/11, Loss: 0.02893352136015892\n","Validation Loss after Epoch 42: 0.044312061121066414\n","Epoch 43/250, Batch 1/11, Loss: 0.0314955972135067\n","Epoch 43/250, Batch 11/11, Loss: 0.02714327909052372\n","Validation Loss after Epoch 43: 0.026722785085439682\n","Epoch 44/250, Batch 1/11, Loss: 0.03751949220895767\n","Epoch 44/250, Batch 11/11, Loss: 0.024766186252236366\n","Validation Loss after Epoch 44: 0.02998066134750843\n","Epoch 45/250, Batch 1/11, Loss: 0.025587018579244614\n","Epoch 45/250, Batch 11/11, Loss: 0.02203277125954628\n","Validation Loss after Epoch 45: 0.057104369004567467\n","Epoch 46/250, Batch 1/11, Loss: 0.029909608885645866\n","Epoch 46/250, Batch 11/11, Loss: 0.02248740941286087\n","Validation Loss after Epoch 46: 0.02542070175210635\n","Epoch 47/250, Batch 1/11, Loss: 0.025303885340690613\n","Epoch 47/250, Batch 11/11, Loss: 0.02108791284263134\n","Validation Loss after Epoch 47: 0.025264989584684372\n","Epoch 48/250, Batch 1/11, Loss: 0.020706193521618843\n","Epoch 48/250, Batch 11/11, Loss: 0.024503011256456375\n","Validation Loss after Epoch 48: 0.030269696066776913\n","Epoch 49/250, Batch 1/11, Loss: 0.024430841207504272\n","Epoch 49/250, Batch 11/11, Loss: 0.026246700435876846\n","Validation Loss after Epoch 49: 0.02884575476249059\n","Epoch 50/250, Batch 1/11, Loss: 0.028131037950515747\n","Epoch 50/250, Batch 11/11, Loss: 0.024679187685251236\n","Validation Loss after Epoch 50: 0.05060205360253652\n","Epoch 51/250, Batch 1/11, Loss: 0.022225579246878624\n","Epoch 51/250, Batch 11/11, Loss: 0.030121484771370888\n","Validation Loss after Epoch 51: 0.049094535410404205\n","Epoch 52/250, Batch 1/11, Loss: 0.024875596165657043\n","Epoch 52/250, Batch 11/11, Loss: 0.028423497453331947\n","Validation Loss after Epoch 52: 0.03224994490544001\n","Epoch 53/250, Batch 1/11, Loss: 0.03738841414451599\n","Epoch 53/250, Batch 11/11, Loss: 0.030731989070773125\n","Validation Loss after Epoch 53: 0.05999802673856417\n","Epoch 54/250, Batch 1/11, Loss: 0.029820341616868973\n","Epoch 54/250, Batch 11/11, Loss: 0.03172127902507782\n","Validation Loss after Epoch 54: 0.026092673962314922\n","Epoch 55/250, Batch 1/11, Loss: 0.029901329427957535\n","Epoch 55/250, Batch 11/11, Loss: 0.027862386777997017\n","Validation Loss after Epoch 55: 0.029463854307929676\n","Epoch 56/250, Batch 1/11, Loss: 0.025043878704309464\n","Epoch 56/250, Batch 11/11, Loss: 0.022537464275956154\n","Validation Loss after Epoch 56: 0.026298086469372112\n","Epoch 57/250, Batch 1/11, Loss: 0.02702001854777336\n","Epoch 57/250, Batch 11/11, Loss: 0.022495122626423836\n","Validation Loss after Epoch 57: 0.02544739345709483\n","Epoch 58/250, Batch 1/11, Loss: 0.018812283873558044\n","Epoch 58/250, Batch 11/11, Loss: 0.02184527926146984\n","Validation Loss after Epoch 58: 0.02906326825420062\n","Epoch 59/250, Batch 1/11, Loss: 0.020122632384300232\n","Epoch 59/250, Batch 11/11, Loss: 0.022718697786331177\n","Validation Loss after Epoch 59: 0.027121645708878834\n","Epoch 60/250, Batch 1/11, Loss: 0.023050110787153244\n","Epoch 60/250, Batch 11/11, Loss: 0.021882273256778717\n","Validation Loss after Epoch 60: 0.030676109716296196\n","Epoch 61/250, Batch 1/11, Loss: 0.023553883656859398\n","Epoch 61/250, Batch 11/11, Loss: 0.020716970786452293\n","Validation Loss after Epoch 61: 0.023230166484912235\n","Epoch 62/250, Batch 1/11, Loss: 0.019057856872677803\n","Epoch 62/250, Batch 11/11, Loss: 0.030684299767017365\n","Validation Loss after Epoch 62: 0.026264208058516186\n","Epoch 63/250, Batch 1/11, Loss: 0.0284714512526989\n","Epoch 63/250, Batch 11/11, Loss: 0.03963567689061165\n","Validation Loss after Epoch 63: 0.023691672831773758\n","Epoch 64/250, Batch 1/11, Loss: 0.02057950384914875\n","Epoch 64/250, Batch 11/11, Loss: 0.04531320556998253\n","Validation Loss after Epoch 64: 0.025392012670636177\n","Epoch 65/250, Batch 1/11, Loss: 0.021725235506892204\n","Epoch 65/250, Batch 11/11, Loss: 0.023147186264395714\n","Validation Loss after Epoch 65: 0.02663302111128966\n","Epoch 66/250, Batch 1/11, Loss: 0.023538419976830482\n","Epoch 66/250, Batch 11/11, Loss: 0.020782100036740303\n","Validation Loss after Epoch 66: 0.027083707973361015\n","Epoch 67/250, Batch 1/11, Loss: 0.030856512486934662\n","Epoch 67/250, Batch 11/11, Loss: 0.02311849780380726\n","Validation Loss after Epoch 67: 0.025009055932362873\n","Epoch 68/250, Batch 1/11, Loss: 0.021498197689652443\n","Epoch 68/250, Batch 11/11, Loss: 0.022261392325162888\n","Validation Loss after Epoch 68: 0.027009097238381703\n","Epoch 69/250, Batch 1/11, Loss: 0.01878291927278042\n","Epoch 69/250, Batch 11/11, Loss: 0.018636267632246017\n","Validation Loss after Epoch 69: 0.0218402116249005\n","Epoch 70/250, Batch 1/11, Loss: 0.01876789890229702\n","Epoch 70/250, Batch 11/11, Loss: 0.020650753751397133\n","Validation Loss after Epoch 70: 0.02220407562951247\n","Epoch 71/250, Batch 1/11, Loss: 0.020760657265782356\n","Epoch 71/250, Batch 11/11, Loss: 0.01813439093530178\n","Validation Loss after Epoch 71: 0.024421249826749165\n","Epoch 72/250, Batch 1/11, Loss: 0.01924975961446762\n","Epoch 72/250, Batch 11/11, Loss: 0.015960458666086197\n","Validation Loss after Epoch 72: 0.02463802509009838\n","Epoch 73/250, Batch 1/11, Loss: 0.01692284643650055\n","Epoch 73/250, Batch 11/11, Loss: 0.018208462744951248\n","Validation Loss after Epoch 73: 0.027537123610575993\n","Epoch 74/250, Batch 1/11, Loss: 0.02146649733185768\n","Epoch 74/250, Batch 11/11, Loss: 0.020661575719714165\n","Validation Loss after Epoch 74: 0.02714576634267966\n","Epoch 75/250, Batch 1/11, Loss: 0.018375998362898827\n","Epoch 75/250, Batch 11/11, Loss: 0.019996559247374535\n","Validation Loss after Epoch 75: 0.02402277911702792\n","Epoch 76/250, Batch 1/11, Loss: 0.01711474359035492\n","Epoch 76/250, Batch 11/11, Loss: 0.02042350172996521\n","Validation Loss after Epoch 76: 0.022349095592896145\n","Epoch 77/250, Batch 1/11, Loss: 0.01965666376054287\n","Epoch 77/250, Batch 11/11, Loss: 0.018229585140943527\n","Validation Loss after Epoch 77: 0.0217228215187788\n","Epoch 78/250, Batch 1/11, Loss: 0.017784597352147102\n","Epoch 78/250, Batch 11/11, Loss: 0.016229428350925446\n","Validation Loss after Epoch 78: 0.02598131572206815\n","Epoch 79/250, Batch 1/11, Loss: 0.017882319167256355\n","Epoch 79/250, Batch 11/11, Loss: 0.016967536881566048\n","Validation Loss after Epoch 79: 0.024977886428435642\n","Epoch 80/250, Batch 1/11, Loss: 0.01980414427816868\n","Epoch 80/250, Batch 11/11, Loss: 0.019111458212137222\n","Validation Loss after Epoch 80: 0.024299852550029755\n","Epoch 81/250, Batch 1/11, Loss: 0.022453654557466507\n","Epoch 81/250, Batch 11/11, Loss: 0.017948511987924576\n","Validation Loss after Epoch 81: 0.020895191157857578\n","Epoch 82/250, Batch 1/11, Loss: 0.018614470958709717\n","Epoch 82/250, Batch 11/11, Loss: 0.02325380966067314\n","Validation Loss after Epoch 82: 0.021980242803692818\n","Epoch 83/250, Batch 1/11, Loss: 0.017659839242696762\n","Epoch 83/250, Batch 11/11, Loss: 0.015899453312158585\n","Validation Loss after Epoch 83: 0.0254562608897686\n","Epoch 84/250, Batch 1/11, Loss: 0.020844532176852226\n","Epoch 84/250, Batch 11/11, Loss: 0.01744343526661396\n","Validation Loss after Epoch 84: 0.021283740177750587\n","Epoch 85/250, Batch 1/11, Loss: 0.018114246428012848\n","Epoch 85/250, Batch 11/11, Loss: 0.01840239018201828\n","Validation Loss after Epoch 85: 0.02216237224638462\n","Epoch 86/250, Batch 1/11, Loss: 0.017182355746626854\n","Epoch 86/250, Batch 11/11, Loss: 0.017816465348005295\n","Validation Loss after Epoch 86: 0.027491886168718338\n","Epoch 87/250, Batch 1/11, Loss: 0.0193741824477911\n","Epoch 87/250, Batch 11/11, Loss: 0.01860901340842247\n","Validation Loss after Epoch 87: 0.026010505234201748\n","Epoch 88/250, Batch 1/11, Loss: 0.014174450188875198\n","Epoch 88/250, Batch 11/11, Loss: 0.015396910719573498\n","Validation Loss after Epoch 88: 0.02578570197025935\n","Epoch 89/250, Batch 1/11, Loss: 0.016473254188895226\n","Epoch 89/250, Batch 11/11, Loss: 0.016667988151311874\n","Validation Loss after Epoch 89: 0.02804384132226308\n","Epoch 90/250, Batch 1/11, Loss: 0.01632915437221527\n","Epoch 90/250, Batch 11/11, Loss: 0.014746176078915596\n","Validation Loss after Epoch 90: 0.023998375982046127\n","Epoch 91/250, Batch 1/11, Loss: 0.015090789645910263\n","Epoch 91/250, Batch 11/11, Loss: 0.01649651676416397\n","Validation Loss after Epoch 91: 0.022608802964289982\n","Epoch 92/250, Batch 1/11, Loss: 0.01333526149392128\n","Epoch 92/250, Batch 11/11, Loss: 0.01683265157043934\n","Validation Loss after Epoch 92: 0.018957112605373066\n","Epoch 93/250, Batch 1/11, Loss: 0.015949811786413193\n","Epoch 93/250, Batch 11/11, Loss: 0.01430568564683199\n","Validation Loss after Epoch 93: 0.021399186924099922\n","Epoch 94/250, Batch 1/11, Loss: 0.01623096689581871\n","Epoch 94/250, Batch 11/11, Loss: 0.01562589965760708\n","Validation Loss after Epoch 94: 0.024027268091837566\n","Epoch 95/250, Batch 1/11, Loss: 0.014038939028978348\n","Epoch 95/250, Batch 11/11, Loss: 0.016168227419257164\n","Validation Loss after Epoch 95: 0.02922294723490874\n","Epoch 96/250, Batch 1/11, Loss: 0.016933199018239975\n","Epoch 96/250, Batch 11/11, Loss: 0.01639283262193203\n","Validation Loss after Epoch 96: 0.0217649694532156\n","Epoch 97/250, Batch 1/11, Loss: 0.016813842579722404\n","Epoch 97/250, Batch 11/11, Loss: 0.021156903356313705\n","Validation Loss after Epoch 97: 0.01995410459736983\n","Epoch 98/250, Batch 1/11, Loss: 0.017159845679998398\n","Epoch 98/250, Batch 11/11, Loss: 0.014878297224640846\n","Validation Loss after Epoch 98: 0.02166627285381158\n","Epoch 99/250, Batch 1/11, Loss: 0.018374089151620865\n","Epoch 99/250, Batch 11/11, Loss: 0.01690482906997204\n","Validation Loss after Epoch 99: 0.02177652157843113\n","Epoch 100/250, Batch 1/11, Loss: 0.012518632225692272\n","Epoch 100/250, Batch 11/11, Loss: 0.016708936542272568\n","Validation Loss after Epoch 100: 0.024416898066798847\n","Epoch 101/250, Batch 1/11, Loss: 0.014720949344336987\n","Epoch 101/250, Batch 11/11, Loss: 0.015660390257835388\n","Validation Loss after Epoch 101: 0.022750676920016605\n","Epoch 102/250, Batch 1/11, Loss: 0.014066285453736782\n","Epoch 102/250, Batch 11/11, Loss: 0.019061727449297905\n","Validation Loss after Epoch 102: 0.021283216153581936\n","Epoch 103/250, Batch 1/11, Loss: 0.012424681335687637\n","Epoch 103/250, Batch 11/11, Loss: 0.013077429495751858\n","Validation Loss after Epoch 103: 0.02314637042582035\n","Epoch 104/250, Batch 1/11, Loss: 0.01851768046617508\n","Epoch 104/250, Batch 11/11, Loss: 0.013609244488179684\n","Validation Loss after Epoch 104: 0.019355561584234238\n","Epoch 105/250, Batch 1/11, Loss: 0.016537358984351158\n","Epoch 105/250, Batch 11/11, Loss: 0.014673524536192417\n","Validation Loss after Epoch 105: 0.02343681827187538\n","Epoch 106/250, Batch 1/11, Loss: 0.011866279877722263\n","Epoch 106/250, Batch 11/11, Loss: 0.015356753021478653\n","Validation Loss after Epoch 106: 0.020334151883920033\n","Epoch 107/250, Batch 1/11, Loss: 0.012747427448630333\n","Epoch 107/250, Batch 11/11, Loss: 0.01364771369844675\n","Validation Loss after Epoch 107: 0.02016347584625085\n","Epoch 108/250, Batch 1/11, Loss: 0.01226657535880804\n","Epoch 108/250, Batch 11/11, Loss: 0.012527531944215298\n","Validation Loss after Epoch 108: 0.021934513623515766\n","Epoch 109/250, Batch 1/11, Loss: 0.015269093215465546\n","Epoch 109/250, Batch 11/11, Loss: 0.014000925235450268\n","Validation Loss after Epoch 109: 0.023448464771111805\n","Epoch 110/250, Batch 1/11, Loss: 0.01384323462843895\n","Epoch 110/250, Batch 11/11, Loss: 0.016848482191562653\n","Validation Loss after Epoch 110: 0.019795013591647148\n","Epoch 111/250, Batch 1/11, Loss: 0.01969684287905693\n","Epoch 111/250, Batch 11/11, Loss: 0.017884882166981697\n","Validation Loss after Epoch 111: 0.02017926300565402\n","Epoch 112/250, Batch 1/11, Loss: 0.013576718978583813\n","Epoch 112/250, Batch 11/11, Loss: 0.014440512284636497\n","Validation Loss after Epoch 112: 0.020841877907514572\n","Epoch 113/250, Batch 1/11, Loss: 0.014870971441268921\n","Epoch 113/250, Batch 11/11, Loss: 0.012430221773684025\n","Validation Loss after Epoch 113: 0.02014121599495411\n","Epoch 114/250, Batch 1/11, Loss: 0.013597160577774048\n","Epoch 114/250, Batch 11/11, Loss: 0.01619558595120907\n","Validation Loss after Epoch 114: 0.025149335463841755\n","Epoch 115/250, Batch 1/11, Loss: 0.011906309984624386\n","Epoch 115/250, Batch 11/11, Loss: 0.013084769248962402\n","Validation Loss after Epoch 115: 0.021793203428387642\n","Epoch 116/250, Batch 1/11, Loss: 0.012969313189387321\n","Epoch 116/250, Batch 11/11, Loss: 0.01214063260704279\n","Validation Loss after Epoch 116: 0.022639835253357887\n","Epoch 117/250, Batch 1/11, Loss: 0.01148761622607708\n","Epoch 117/250, Batch 11/11, Loss: 0.01916664093732834\n","Validation Loss after Epoch 117: 0.019641743352015812\n","Epoch 118/250, Batch 1/11, Loss: 0.012530017644166946\n","Epoch 118/250, Batch 11/11, Loss: 0.01438091415911913\n","Validation Loss after Epoch 118: 0.02075960797568162\n","Epoch 119/250, Batch 1/11, Loss: 0.017343368381261826\n","Epoch 119/250, Batch 11/11, Loss: 0.01670023612678051\n","Validation Loss after Epoch 119: 0.022896631931265194\n","Epoch 120/250, Batch 1/11, Loss: 0.015880346298217773\n","Epoch 120/250, Batch 11/11, Loss: 0.01167644839733839\n","Validation Loss after Epoch 120: 0.024265005563696224\n","Epoch 121/250, Batch 1/11, Loss: 0.022642964497208595\n","Epoch 121/250, Batch 11/11, Loss: 0.01544230617582798\n","Validation Loss after Epoch 121: 0.021717711662252743\n","Epoch 122/250, Batch 1/11, Loss: 0.012163544073700905\n","Epoch 122/250, Batch 11/11, Loss: 0.014213690534234047\n","Validation Loss after Epoch 122: 0.020041177049279213\n","Epoch 123/250, Batch 1/11, Loss: 0.011052439920604229\n","Epoch 123/250, Batch 11/11, Loss: 0.0122153889387846\n","Validation Loss after Epoch 123: 0.02262711524963379\n","Epoch 124/250, Batch 1/11, Loss: 0.01348236482590437\n","Epoch 124/250, Batch 11/11, Loss: 0.011571326293051243\n","Validation Loss after Epoch 124: 0.01795940473675728\n","Epoch 125/250, Batch 1/11, Loss: 0.013132551684975624\n","Epoch 125/250, Batch 11/11, Loss: 0.01270587183535099\n","Validation Loss after Epoch 125: 0.020060458530982334\n","Epoch 126/250, Batch 1/11, Loss: 0.010845622979104519\n","Epoch 126/250, Batch 11/11, Loss: 0.011743932031095028\n","Validation Loss after Epoch 126: 0.0218574907630682\n","Epoch 127/250, Batch 1/11, Loss: 0.013455566950142384\n","Epoch 127/250, Batch 11/11, Loss: 0.012177356518805027\n","Validation Loss after Epoch 127: 0.02050907351076603\n","Epoch 128/250, Batch 1/11, Loss: 0.018998583778738976\n","Epoch 128/250, Batch 11/11, Loss: 0.015587824396789074\n","Validation Loss after Epoch 128: 0.018655202972392242\n","Epoch 129/250, Batch 1/11, Loss: 0.01325665507465601\n","Epoch 129/250, Batch 11/11, Loss: 0.017221810296177864\n","Validation Loss after Epoch 129: 0.019389511396487553\n","Epoch 130/250, Batch 1/11, Loss: 0.012573134154081345\n","Epoch 130/250, Batch 11/11, Loss: 0.012852529995143414\n","Validation Loss after Epoch 130: 0.020700044309099514\n","Epoch 131/250, Batch 1/11, Loss: 0.01229116041213274\n","Epoch 131/250, Batch 11/11, Loss: 0.011508469469845295\n","Validation Loss after Epoch 131: 0.02079142505923907\n","Epoch 132/250, Batch 1/11, Loss: 0.013866337016224861\n","Epoch 132/250, Batch 11/11, Loss: 0.012071373872458935\n","Validation Loss after Epoch 132: 0.02050659495095412\n","Epoch 133/250, Batch 1/11, Loss: 0.010265661403536797\n","Epoch 133/250, Batch 11/11, Loss: 0.010395257733762264\n","Validation Loss after Epoch 133: 0.018411864216128986\n","Epoch 134/250, Batch 1/11, Loss: 0.010618705302476883\n","Epoch 134/250, Batch 11/11, Loss: 0.010829880833625793\n","Validation Loss after Epoch 134: 0.018919695789615314\n","Epoch 135/250, Batch 1/11, Loss: 0.01153561845421791\n","Epoch 135/250, Batch 11/11, Loss: 0.01209977362304926\n","Validation Loss after Epoch 135: 0.02605671559770902\n","Epoch 136/250, Batch 1/11, Loss: 0.010628296062350273\n","Epoch 136/250, Batch 11/11, Loss: 0.011657998897135258\n","Validation Loss after Epoch 136: 0.021243600795666378\n","Epoch 137/250, Batch 1/11, Loss: 0.009487329982221127\n","Epoch 137/250, Batch 11/11, Loss: 0.013705597259104252\n","Validation Loss after Epoch 137: 0.020573195070028305\n","Epoch 138/250, Batch 1/11, Loss: 0.010241679847240448\n","Epoch 138/250, Batch 11/11, Loss: 0.013302248902618885\n","Validation Loss after Epoch 138: 0.018882079049944878\n","Epoch 139/250, Batch 1/11, Loss: 0.01063724234700203\n","Epoch 139/250, Batch 11/11, Loss: 0.010839954949915409\n","Validation Loss after Epoch 139: 0.02129329244295756\n","Epoch 140/250, Batch 1/11, Loss: 0.011497865431010723\n","Epoch 140/250, Batch 11/11, Loss: 0.010889897122979164\n","Validation Loss after Epoch 140: 0.021361257880926132\n","Epoch 141/250, Batch 1/11, Loss: 0.011270329356193542\n","Epoch 141/250, Batch 11/11, Loss: 0.010522784665226936\n","Validation Loss after Epoch 141: 0.01827198453247547\n","Epoch 142/250, Batch 1/11, Loss: 0.012227848172187805\n","Epoch 142/250, Batch 11/11, Loss: 0.014844206161797047\n","Validation Loss after Epoch 142: 0.020948729788263638\n","Epoch 143/250, Batch 1/11, Loss: 0.011647316627204418\n","Epoch 143/250, Batch 11/11, Loss: 0.016965635120868683\n","Validation Loss after Epoch 143: 0.021369111413757007\n","Epoch 144/250, Batch 1/11, Loss: 0.011300479993224144\n","Epoch 144/250, Batch 11/11, Loss: 0.01033397950232029\n","Validation Loss after Epoch 144: 0.01926364315052827\n","Epoch 145/250, Batch 1/11, Loss: 0.009912724606692791\n","Epoch 145/250, Batch 11/11, Loss: 0.013367942534387112\n","Validation Loss after Epoch 145: 0.02106955647468567\n","Epoch 146/250, Batch 1/11, Loss: 0.014247702434659004\n","Epoch 146/250, Batch 11/11, Loss: 0.012745263986289501\n","Validation Loss after Epoch 146: 0.019722493054966133\n","Epoch 147/250, Batch 1/11, Loss: 0.011015820316970348\n","Epoch 147/250, Batch 11/11, Loss: 0.011852975934743881\n","Validation Loss after Epoch 147: 0.02185611551006635\n","Epoch 148/250, Batch 1/11, Loss: 0.014063696376979351\n","Epoch 148/250, Batch 11/11, Loss: 0.011844191700220108\n","Validation Loss after Epoch 148: 0.019396779437859852\n","Epoch 149/250, Batch 1/11, Loss: 0.009474941529333591\n","Epoch 149/250, Batch 11/11, Loss: 0.00986473634839058\n","Validation Loss after Epoch 149: 0.019281179333726566\n","Epoch 150/250, Batch 1/11, Loss: 0.011393893510103226\n","Epoch 150/250, Batch 11/11, Loss: 0.009763120673596859\n","Validation Loss after Epoch 150: 0.020454447716474533\n","Epoch 151/250, Batch 1/11, Loss: 0.009414155967533588\n","Epoch 151/250, Batch 11/11, Loss: 0.011966768652200699\n","Validation Loss after Epoch 151: 0.020001713186502457\n","Epoch 152/250, Batch 1/11, Loss: 0.011971245519816875\n","Epoch 152/250, Batch 11/11, Loss: 0.01067775022238493\n","Validation Loss after Epoch 152: 0.019300454606612522\n","Epoch 153/250, Batch 1/11, Loss: 0.011128582060337067\n","Epoch 153/250, Batch 11/11, Loss: 0.011496313847601414\n","Validation Loss after Epoch 153: 0.02078617364168167\n","Epoch 154/250, Batch 1/11, Loss: 0.011399145238101482\n","Epoch 154/250, Batch 11/11, Loss: 0.012894794344902039\n","Validation Loss after Epoch 154: 0.020479500914613407\n","Epoch 155/250, Batch 1/11, Loss: 0.011407890357077122\n","Epoch 155/250, Batch 11/11, Loss: 0.01753656007349491\n","Validation Loss after Epoch 155: 0.0197770061592261\n","Epoch 156/250, Batch 1/11, Loss: 0.015571802854537964\n","Epoch 156/250, Batch 11/11, Loss: 0.010828584432601929\n","Validation Loss after Epoch 156: 0.023758231351772945\n","Epoch 157/250, Batch 1/11, Loss: 0.012264165095984936\n","Epoch 157/250, Batch 11/11, Loss: 0.011004787869751453\n","Validation Loss after Epoch 157: 0.023412964617212612\n","Epoch 158/250, Batch 1/11, Loss: 0.011378352530300617\n","Epoch 158/250, Batch 11/11, Loss: 0.010476256720721722\n","Validation Loss after Epoch 158: 0.019224851081768673\n","Epoch 159/250, Batch 1/11, Loss: 0.008298266679048538\n","Epoch 159/250, Batch 11/11, Loss: 0.009496761485934258\n","Validation Loss after Epoch 159: 0.021231316030025482\n","Epoch 160/250, Batch 1/11, Loss: 0.010494510643184185\n","Epoch 160/250, Batch 11/11, Loss: 0.009646367281675339\n","Validation Loss after Epoch 160: 0.022645248100161552\n","Epoch 161/250, Batch 1/11, Loss: 0.00969224888831377\n","Epoch 161/250, Batch 11/11, Loss: 0.009019194170832634\n","Validation Loss after Epoch 161: 0.020889556656281155\n","Epoch 162/250, Batch 1/11, Loss: 0.009583404287695885\n","Epoch 162/250, Batch 11/11, Loss: 0.009735513478517532\n","Validation Loss after Epoch 162: 0.026033251235882442\n","Epoch 163/250, Batch 1/11, Loss: 0.010288127698004246\n","Epoch 163/250, Batch 11/11, Loss: 0.010067777708172798\n","Validation Loss after Epoch 163: 0.021703484778602917\n","Epoch 164/250, Batch 1/11, Loss: 0.008750936016440392\n","Epoch 164/250, Batch 11/11, Loss: 0.009635514579713345\n","Validation Loss after Epoch 164: 0.020537568877140682\n","Epoch 165/250, Batch 1/11, Loss: 0.00852449145168066\n","Epoch 165/250, Batch 11/11, Loss: 0.00857214443385601\n","Validation Loss after Epoch 165: 0.02121685693661372\n","Epoch 166/250, Batch 1/11, Loss: 0.009835844859480858\n","Epoch 166/250, Batch 11/11, Loss: 0.009670319966971874\n","Validation Loss after Epoch 166: 0.023654818534851074\n","Epoch 167/250, Batch 1/11, Loss: 0.014867226593196392\n","Epoch 167/250, Batch 11/11, Loss: 0.009834752418100834\n","Validation Loss after Epoch 167: 0.02472655785580476\n","Epoch 168/250, Batch 1/11, Loss: 0.011187203228473663\n","Epoch 168/250, Batch 11/11, Loss: 0.010007527656853199\n","Validation Loss after Epoch 168: 0.02064596799512704\n","Epoch 169/250, Batch 1/11, Loss: 0.008263828232884407\n","Epoch 169/250, Batch 11/11, Loss: 0.00926950853317976\n","Validation Loss after Epoch 169: 0.021764264752467472\n","Epoch 170/250, Batch 1/11, Loss: 0.00888816174119711\n","Epoch 170/250, Batch 11/11, Loss: 0.008668733760714531\n","Validation Loss after Epoch 170: 0.021296535308162372\n","Epoch 171/250, Batch 1/11, Loss: 0.007578595541417599\n","Epoch 171/250, Batch 11/11, Loss: 0.00803268514573574\n","Validation Loss after Epoch 171: 0.027410519619782765\n","Epoch 172/250, Batch 1/11, Loss: 0.00881282240152359\n","Epoch 172/250, Batch 11/11, Loss: 0.009192435070872307\n","Validation Loss after Epoch 172: 0.024274927874406178\n","Epoch 173/250, Batch 1/11, Loss: 0.015366301871836185\n","Epoch 173/250, Batch 11/11, Loss: 0.01456123124808073\n","Validation Loss after Epoch 173: 0.020096944645047188\n","Epoch 174/250, Batch 1/11, Loss: 0.009282221086323261\n","Epoch 174/250, Batch 11/11, Loss: 0.014822658151388168\n","Validation Loss after Epoch 174: 0.018343238160014153\n","Epoch 175/250, Batch 1/11, Loss: 0.010175436735153198\n","Epoch 175/250, Batch 11/11, Loss: 0.010770591907203197\n","Validation Loss after Epoch 175: 0.021636934330066044\n","Epoch 176/250, Batch 1/11, Loss: 0.012649374082684517\n","Epoch 176/250, Batch 11/11, Loss: 0.011862690560519695\n","Validation Loss after Epoch 176: 0.02251448482275009\n","Epoch 177/250, Batch 1/11, Loss: 0.008919774554669857\n","Epoch 177/250, Batch 11/11, Loss: 0.009954345412552357\n","Validation Loss after Epoch 177: 0.017864350229501724\n","Epoch 178/250, Batch 1/11, Loss: 0.0082883695140481\n","Epoch 178/250, Batch 11/11, Loss: 0.010383818298578262\n","Validation Loss after Epoch 178: 0.02125606561700503\n","Epoch 179/250, Batch 1/11, Loss: 0.007841183803975582\n","Epoch 179/250, Batch 11/11, Loss: 0.009408491663634777\n","Validation Loss after Epoch 179: 0.020612579459945362\n","Epoch 180/250, Batch 1/11, Loss: 0.010018764063715935\n","Epoch 180/250, Batch 11/11, Loss: 0.012130067683756351\n","Validation Loss after Epoch 180: 0.022664019217093784\n","Epoch 181/250, Batch 1/11, Loss: 0.01020810380578041\n","Epoch 181/250, Batch 11/11, Loss: 0.008188405074179173\n","Validation Loss after Epoch 181: 0.019472259407242138\n","Epoch 182/250, Batch 1/11, Loss: 0.008744429796934128\n","Epoch 182/250, Batch 11/11, Loss: 0.008667287416756153\n","Validation Loss after Epoch 182: 0.02840995353957017\n","Epoch 183/250, Batch 1/11, Loss: 0.008569772355258465\n","Epoch 183/250, Batch 11/11, Loss: 0.008619149215519428\n","Validation Loss after Epoch 183: 0.021531136706471443\n","Epoch 184/250, Batch 1/11, Loss: 0.007322084624320269\n","Epoch 184/250, Batch 11/11, Loss: 0.007435693871229887\n","Validation Loss after Epoch 184: 0.021861931309103966\n","Epoch 185/250, Batch 1/11, Loss: 0.008220045827329159\n","Epoch 185/250, Batch 11/11, Loss: 0.008377010934054852\n","Validation Loss after Epoch 185: 0.023831394190589588\n","Epoch 186/250, Batch 1/11, Loss: 0.006477871909737587\n","Epoch 186/250, Batch 11/11, Loss: 0.007972299121320248\n","Validation Loss after Epoch 186: 0.020833791544040043\n","Epoch 187/250, Batch 1/11, Loss: 0.00756761385127902\n","Epoch 187/250, Batch 11/11, Loss: 0.008074365556240082\n","Validation Loss after Epoch 187: 0.024279556547602017\n","Epoch 188/250, Batch 1/11, Loss: 0.0067762164399027824\n","Epoch 188/250, Batch 11/11, Loss: 0.027121152728796005\n","Validation Loss after Epoch 188: 0.020484176153937977\n","Epoch 189/250, Batch 1/11, Loss: 0.011077169328927994\n","Epoch 189/250, Batch 11/11, Loss: 0.011526315473020077\n","Validation Loss after Epoch 189: 0.02596038704117139\n","Epoch 190/250, Batch 1/11, Loss: 0.009984861128032207\n","Epoch 190/250, Batch 11/11, Loss: 0.010578260757029057\n","Validation Loss after Epoch 190: 0.029546473796168964\n","Epoch 191/250, Batch 1/11, Loss: 0.009209569543600082\n","Epoch 191/250, Batch 11/11, Loss: 0.00816023163497448\n","Validation Loss after Epoch 191: 0.02295019415517648\n","Epoch 192/250, Batch 1/11, Loss: 0.008491883054375648\n","Epoch 192/250, Batch 11/11, Loss: 0.008984697982668877\n","Validation Loss after Epoch 192: 0.02357919638355573\n","Epoch 193/250, Batch 1/11, Loss: 0.0073898485861718655\n","Epoch 193/250, Batch 11/11, Loss: 0.007349558640271425\n","Validation Loss after Epoch 193: 0.019601327056686085\n","Epoch 194/250, Batch 1/11, Loss: 0.00703322421759367\n","Epoch 194/250, Batch 11/11, Loss: 0.007811565417796373\n","Validation Loss after Epoch 194: 0.02069484939177831\n","Epoch 195/250, Batch 1/11, Loss: 0.006922746077179909\n","Epoch 195/250, Batch 11/11, Loss: 0.009646807797253132\n","Validation Loss after Epoch 195: 0.026518292725086212\n","Epoch 196/250, Batch 1/11, Loss: 0.00786204170435667\n","Epoch 196/250, Batch 11/11, Loss: 0.006615576334297657\n","Validation Loss after Epoch 196: 0.023214948053161304\n","Epoch 197/250, Batch 1/11, Loss: 0.007383458781987429\n","Epoch 197/250, Batch 11/11, Loss: 0.007468302734196186\n","Validation Loss after Epoch 197: 0.02457180681327979\n","Epoch 198/250, Batch 1/11, Loss: 0.006839146837592125\n","Epoch 198/250, Batch 11/11, Loss: 0.007683648727834225\n","Validation Loss after Epoch 198: 0.020822918663422268\n","Epoch 199/250, Batch 1/11, Loss: 0.007084599696099758\n","Epoch 199/250, Batch 11/11, Loss: 0.0061881886795163155\n","Validation Loss after Epoch 199: 0.022622670357426006\n","Epoch 200/250, Batch 1/11, Loss: 0.006492876913398504\n","Epoch 200/250, Batch 11/11, Loss: 0.007001209072768688\n","Validation Loss after Epoch 200: 0.024914516756931942\n","Epoch 201/250, Batch 1/11, Loss: 0.00666596507653594\n","Epoch 201/250, Batch 11/11, Loss: 0.008607835508883\n","Validation Loss after Epoch 201: 0.022521681462725002\n","Epoch 202/250, Batch 1/11, Loss: 0.008447625674307346\n","Epoch 202/250, Batch 11/11, Loss: 0.011537439189851284\n","Validation Loss after Epoch 202: 0.023283957814176876\n","Epoch 203/250, Batch 1/11, Loss: 0.007352485321462154\n","Epoch 203/250, Batch 11/11, Loss: 0.007198153994977474\n","Validation Loss after Epoch 203: 0.024782033637166023\n","Epoch 204/250, Batch 1/11, Loss: 0.006902843713760376\n","Epoch 204/250, Batch 11/11, Loss: 0.008924338966608047\n","Validation Loss after Epoch 204: 0.024844547112782795\n","Epoch 205/250, Batch 1/11, Loss: 0.01057090237736702\n","Epoch 205/250, Batch 11/11, Loss: 0.009196920320391655\n","Validation Loss after Epoch 205: 0.02183326706290245\n","Epoch 206/250, Batch 1/11, Loss: 0.007570044137537479\n","Epoch 206/250, Batch 11/11, Loss: 0.007986584678292274\n","Validation Loss after Epoch 206: 0.028230177859465282\n","Epoch 207/250, Batch 1/11, Loss: 0.009438457898795605\n","Epoch 207/250, Batch 11/11, Loss: 0.008806277997791767\n","Validation Loss after Epoch 207: 0.031410349532961845\n","Epoch 208/250, Batch 1/11, Loss: 0.014297213405370712\n","Epoch 208/250, Batch 11/11, Loss: 0.01316932961344719\n","Validation Loss after Epoch 208: 0.02328636993964513\n","Epoch 209/250, Batch 1/11, Loss: 0.00952491257339716\n","Epoch 209/250, Batch 11/11, Loss: 0.010421054437756538\n","Validation Loss after Epoch 209: 0.022882477690776188\n","Epoch 210/250, Batch 1/11, Loss: 0.007243587169796228\n","Epoch 210/250, Batch 11/11, Loss: 0.010334772989153862\n","Validation Loss after Epoch 210: 0.032426174730062485\n","Epoch 211/250, Batch 1/11, Loss: 0.009858280420303345\n","Epoch 211/250, Batch 11/11, Loss: 0.009171565063297749\n","Validation Loss after Epoch 211: 0.026447034751375515\n","Epoch 212/250, Batch 1/11, Loss: 0.014081407338380814\n","Epoch 212/250, Batch 11/11, Loss: 0.007419173140078783\n","Validation Loss after Epoch 212: 0.020689716562628746\n","Epoch 213/250, Batch 1/11, Loss: 0.00834368634968996\n","Epoch 213/250, Batch 11/11, Loss: 0.006032934878021479\n","Validation Loss after Epoch 213: 0.019649129981795948\n","Epoch 214/250, Batch 1/11, Loss: 0.006634808611124754\n","Epoch 214/250, Batch 11/11, Loss: 0.005485678091645241\n","Validation Loss after Epoch 214: 0.025907871623833973\n","Epoch 215/250, Batch 1/11, Loss: 0.006740078330039978\n","Epoch 215/250, Batch 11/11, Loss: 0.007712597958743572\n","Validation Loss after Epoch 215: 0.02422624019285043\n","Epoch 216/250, Batch 1/11, Loss: 0.005119789857417345\n","Epoch 216/250, Batch 11/11, Loss: 0.0051910728216171265\n","Validation Loss after Epoch 216: 0.02219431350628535\n","Epoch 217/250, Batch 1/11, Loss: 0.0072281695902347565\n","Epoch 217/250, Batch 11/11, Loss: 0.005332777742296457\n","Validation Loss after Epoch 217: 0.02069636931022008\n","Epoch 218/250, Batch 1/11, Loss: 0.00588305713608861\n","Epoch 218/250, Batch 11/11, Loss: 0.008680369704961777\n","Validation Loss after Epoch 218: 0.02569257840514183\n","Epoch 219/250, Batch 1/11, Loss: 0.0054436479695141315\n","Epoch 219/250, Batch 11/11, Loss: 0.006857945583760738\n","Validation Loss after Epoch 219: 0.02421547720829646\n","Epoch 220/250, Batch 1/11, Loss: 0.00908429455012083\n","Epoch 220/250, Batch 11/11, Loss: 0.00706297205761075\n","Validation Loss after Epoch 220: 0.0222670113046964\n","Epoch 221/250, Batch 1/11, Loss: 0.005619010888040066\n","Epoch 221/250, Batch 11/11, Loss: 0.008740810677409172\n","Validation Loss after Epoch 221: 0.021627681329846382\n","Epoch 222/250, Batch 1/11, Loss: 0.006157097406685352\n","Epoch 222/250, Batch 11/11, Loss: 0.006255166605114937\n","Validation Loss after Epoch 222: 0.022682934378584225\n","Epoch 223/250, Batch 1/11, Loss: 0.005470866337418556\n","Epoch 223/250, Batch 11/11, Loss: 0.005613027140498161\n","Validation Loss after Epoch 223: 0.02194983201722304\n","Epoch 224/250, Batch 1/11, Loss: 0.006066277623176575\n","Epoch 224/250, Batch 11/11, Loss: 0.0060510290786623955\n","Validation Loss after Epoch 224: 0.025420362129807472\n","Epoch 225/250, Batch 1/11, Loss: 0.005482666194438934\n","Epoch 225/250, Batch 11/11, Loss: 0.005603752098977566\n","Validation Loss after Epoch 225: 0.026365815351406734\n","Epoch 226/250, Batch 1/11, Loss: 0.005404321942478418\n","Epoch 226/250, Batch 11/11, Loss: 0.00606556236743927\n","Validation Loss after Epoch 226: 0.024002475664019585\n","Epoch 227/250, Batch 1/11, Loss: 0.0052767666056752205\n","Epoch 227/250, Batch 11/11, Loss: 0.005955342669039965\n","Validation Loss after Epoch 227: 0.022151486948132515\n","Epoch 228/250, Batch 1/11, Loss: 0.006424400024116039\n","Epoch 228/250, Batch 11/11, Loss: 0.006035777740180492\n","Validation Loss after Epoch 228: 0.02169296455880006\n","Epoch 229/250, Batch 1/11, Loss: 0.005222496576607227\n","Epoch 229/250, Batch 11/11, Loss: 0.006034036632627249\n","Validation Loss after Epoch 229: 0.022834112867712975\n","Epoch 230/250, Batch 1/11, Loss: 0.005640681833028793\n","Epoch 230/250, Batch 11/11, Loss: 0.008716758340597153\n","Validation Loss after Epoch 230: 0.02406412735581398\n","Epoch 231/250, Batch 1/11, Loss: 0.00480234669521451\n","Epoch 231/250, Batch 11/11, Loss: 0.00588646437972784\n","Validation Loss after Epoch 231: 0.02621051234503587\n","Epoch 232/250, Batch 1/11, Loss: 0.005338361486792564\n","Epoch 232/250, Batch 11/11, Loss: 0.0058588082902133465\n","Validation Loss after Epoch 232: 0.02485259311894576\n","Epoch 233/250, Batch 1/11, Loss: 0.004521727561950684\n","Epoch 233/250, Batch 11/11, Loss: 0.005866972263902426\n","Validation Loss after Epoch 233: 0.0240663339694341\n","Epoch 234/250, Batch 1/11, Loss: 0.005046474281698465\n","Epoch 234/250, Batch 11/11, Loss: 0.005262122489511967\n","Validation Loss after Epoch 234: 0.027636563405394554\n","Epoch 235/250, Batch 1/11, Loss: 0.005169709678739309\n","Epoch 235/250, Batch 11/11, Loss: 0.004967431537806988\n","Validation Loss after Epoch 235: 0.02110341501732667\n","Epoch 236/250, Batch 1/11, Loss: 0.007498602848500013\n","Epoch 236/250, Batch 11/11, Loss: 0.005671578925102949\n","Validation Loss after Epoch 236: 0.03375728180011114\n","Epoch 237/250, Batch 1/11, Loss: 0.005719052627682686\n","Epoch 237/250, Batch 11/11, Loss: 0.005457730498164892\n","Validation Loss after Epoch 237: 0.023460807899634045\n","Epoch 238/250, Batch 1/11, Loss: 0.004713765345513821\n","Epoch 238/250, Batch 11/11, Loss: 0.007430561352521181\n","Validation Loss after Epoch 238: 0.025282020370165508\n","Epoch 239/250, Batch 1/11, Loss: 0.004069923423230648\n","Epoch 239/250, Batch 11/11, Loss: 0.008638384751975536\n","Validation Loss after Epoch 239: 0.022192903483907383\n","Epoch 240/250, Batch 1/11, Loss: 0.005350855644792318\n","Epoch 240/250, Batch 11/11, Loss: 0.005652924999594688\n","Validation Loss after Epoch 240: 0.022733089203635853\n","Epoch 241/250, Batch 1/11, Loss: 0.005522961728274822\n","Epoch 241/250, Batch 11/11, Loss: 0.007510756608098745\n","Validation Loss after Epoch 241: 0.02302916223804156\n","Epoch 242/250, Batch 1/11, Loss: 0.005205957684665918\n","Epoch 242/250, Batch 11/11, Loss: 0.005628649611026049\n","Validation Loss after Epoch 242: 0.026625622684756916\n","Epoch 243/250, Batch 1/11, Loss: 0.0048480527475476265\n","Epoch 243/250, Batch 11/11, Loss: 0.005252627655863762\n","Validation Loss after Epoch 243: 0.025433169056971867\n","Epoch 244/250, Batch 1/11, Loss: 0.004693913273513317\n","Epoch 244/250, Batch 11/11, Loss: 0.008934423327445984\n","Validation Loss after Epoch 244: 0.024970185632507007\n","Epoch 245/250, Batch 1/11, Loss: 0.005047372076660395\n","Epoch 245/250, Batch 11/11, Loss: 0.005728248041123152\n","Validation Loss after Epoch 245: 0.02181415446102619\n","Epoch 246/250, Batch 1/11, Loss: 0.004466160200536251\n","Epoch 246/250, Batch 11/11, Loss: 0.004840915556997061\n","Validation Loss after Epoch 246: 0.028416142488519352\n","Epoch 247/250, Batch 1/11, Loss: 0.004730667918920517\n","Epoch 247/250, Batch 11/11, Loss: 0.0044693974778056145\n","Validation Loss after Epoch 247: 0.029023483395576477\n","Epoch 248/250, Batch 1/11, Loss: 0.005110707134008408\n","Epoch 248/250, Batch 11/11, Loss: 0.004896567203104496\n","Validation Loss after Epoch 248: 0.0241852564116319\n","Epoch 249/250, Batch 1/11, Loss: 0.005665590055286884\n","Epoch 249/250, Batch 11/11, Loss: 0.00570637034252286\n","Validation Loss after Epoch 249: 0.023599112406373024\n","Epoch 250/250, Batch 1/11, Loss: 0.0038762306794524193\n","Epoch 250/250, Batch 11/11, Loss: 0.009553310461342335\n","Validation Loss after Epoch 250: 0.023167612651983898\n","Subset size 500 - Test Loss: 0.0227, Test Accuracy: 99.23%, Average Dice Score: 0.9918\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '64x64smaller_subsets_with_250_Epoch.pth')"],"metadata":{"id":"Q8t0sPgbeVZs","executionInfo":{"status":"ok","timestamp":1701185287882,"user_tz":-60,"elapsed":4,"user":{"displayName":"Erikas Mikuzis","userId":"02233197238408831535"}}},"execution_count":6,"outputs":[]}]}