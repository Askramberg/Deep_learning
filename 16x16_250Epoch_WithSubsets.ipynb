{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlZIzh7_h_bN",
        "outputId": "4d7aaac9-6c62-45ac-9958-c0f33f5ac977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m95UJeDziH6e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torch.nn.functional import relu\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_paths, label_paths):\n",
        "        self.image_paths = image_paths\n",
        "        self.label_paths = label_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image using PIL\n",
        "        with Image.open(self.image_paths[idx]) as img:\n",
        "            np_image = np.array(img, dtype=np.float32)\n",
        "\n",
        "        # Normalize the image\n",
        "        normalized_image = np_image / 65535.0  # Assuming 16-bit images\n",
        "\n",
        "        # Load and process the label data using PIL\n",
        "        with Image.open(self.label_paths[idx]) as label_img:\n",
        "            label_array = np.array(label_img, dtype=np.float32)\n",
        "\n",
        "        grayscale_to_class_mapping = {0: 0, 128: 1, 255: 2} # Maps gray-levels to class\n",
        "\n",
        "        # Map grayscale values to class labels\n",
        "        mapped_labels = np.copy(label_array)\n",
        "        for grayscale_value, class_id in grayscale_to_class_mapping.items():\n",
        "            mapped_labels[label_array == grayscale_value] = class_id\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        image_tensor = torch.from_numpy(normalized_image).unsqueeze(0) # Add channel dimension\n",
        "        label_tensor = torch.from_numpy(mapped_labels).long() # Ensure label tensor is long type\n",
        "\n",
        "        return image_tensor, label_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Bc52XUliJtl"
      },
      "outputs": [],
      "source": [
        "### Label images ###\n",
        "# white class - 255 nickel\n",
        "# gray class - 128 ysz\n",
        "# black class - 0 pores\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define a helper function for creating a block\n",
        "        def conv_block(in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=0.1)\n",
        "            )\n",
        "\n",
        "        # Encoder\n",
        "        self.e11 = conv_block(1, 64)\n",
        "        self.e12 = conv_block(64, 64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.e21 = conv_block(64, 128)\n",
        "        self.e22 = conv_block(128, 128)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.e31 = conv_block(128, 256)\n",
        "        self.e32 = conv_block(256, 256)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.e41 = conv_block(256, 512)\n",
        "        self.e42 = conv_block(512, 512)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.e51 = conv_block(512, 1024)\n",
        "        self.e52 = conv_block(1024, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.d11 = conv_block(1024, 512)\n",
        "        self.d12 = conv_block(512, 512)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.d21 = conv_block(512, 256)\n",
        "        self.d22 = conv_block(256, 256)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.d31 = conv_block(256, 128)\n",
        "        self.d32 = conv_block(128, 128)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.d41 = conv_block(128, 64)\n",
        "        self.d42 = conv_block(64, 64)\n",
        "\n",
        "        # Output layer\n",
        "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        xe11 = self.e11(x)\n",
        "        xe12 = self.e12(xe11)\n",
        "        xp1 = self.pool1(xe12)\n",
        "\n",
        "        xe21 = self.e21(xp1)\n",
        "        xe22 = self.e22(xe21)\n",
        "        xp2 = self.pool2(xe22)\n",
        "\n",
        "        xe31 = self.e31(xp2)\n",
        "        xe32 = self.e32(xe31)\n",
        "        xp3 = self.pool3(xe32)\n",
        "\n",
        "        xe41 = self.e41(xp3)\n",
        "        xe42 = self.e42(xe41)\n",
        "        xp4 = self.pool4(xe42)\n",
        "\n",
        "        xe51 = self.e51(xp4)\n",
        "        xe52 = self.e52(xe51)\n",
        "\n",
        "        # Decoder\n",
        "        xu1 = self.upconv1(xe52)\n",
        "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
        "        xd11 = self.d11(xu11)\n",
        "        xd12 = self.d12(xd11)\n",
        "\n",
        "        xu2 = self.upconv2(xd12)\n",
        "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
        "        xd21 = self.d21(xu22)\n",
        "        xd22 = self.d22(xd21)\n",
        "\n",
        "        xu3 = self.upconv3(xd22)\n",
        "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
        "        xd31 = self.d31(xu33)\n",
        "        xd32 = self.d32(xd31)\n",
        "\n",
        "        xu4 = self.upconv4(xd32)\n",
        "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
        "        xd41 = self.d41(xu44)\n",
        "        xd42 = self.d42(xd41)\n",
        "\n",
        "        # Output layer\n",
        "        out = self.outconv(xd42)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tjYoI2YS8WpH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch import optim\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def dice_coefficient(predicted, target, num_classes):\n",
        "    dice_scores = []  # To store dice coefficient for each class\n",
        "\n",
        "    # Convert predictions and targets to one-hot encoded form\n",
        "    predicted_one_hot = F.one_hot(predicted, num_classes).permute(0, 3, 1, 2).float()\n",
        "    target_one_hot = F.one_hot(target, num_classes).permute(0, 3, 1, 2).float()\n",
        "\n",
        "    # Calculate Dice coefficient for each class\n",
        "    for class_index in range(num_classes):\n",
        "        intersection = (predicted_one_hot[:, class_index, :, :] * target_one_hot[:, class_index, :, :]).sum()\n",
        "        union = predicted_one_hot[:, class_index, :, :].sum() + target_one_hot[:, class_index, :, :].sum()\n",
        "        dice_score = (2 * intersection + 1e-6) / (union + 1e-6)  # Adding a small epsilon to avoid division by zero\n",
        "        dice_scores.append(dice_score)\n",
        "\n",
        "    # Average Dice score across all classes\n",
        "    avg_dice_score = sum(dice_scores) / len(dice_scores)\n",
        "    return avg_dice_score.item()  # Return the value as a Python scalar\n",
        "\n",
        "def get_image_paths(data_dir, label_dir):\n",
        "    data_paths = [os.path.join(data_dir, img) for img in sorted(os.listdir(data_dir))]\n",
        "    label_paths = [os.path.join(label_dir, lbl) for lbl in sorted(os.listdir(label_dir))]\n",
        "    return data_paths, label_paths\n",
        "\n",
        "def create_subsets(dataset, subset_sizes):\n",
        "    subsets = {}\n",
        "    for size in subset_sizes:\n",
        "        if size == len(dataset):\n",
        "            subsets[size] = dataset  # Use the full dataset\n",
        "        else:\n",
        "            subset, _ = random_split(dataset, [size, len(dataset) - size])\n",
        "            subsets[size] = subset\n",
        "    return subsets\n",
        "\n",
        "def plot_predictions(images, actuals, predicted):\n",
        "    plt.figure(figsize=(20, 5))  # Adjust the figure size as needed\n",
        "    for i in range(len(images)):\n",
        "        # Rescale the images from [0, 1] to [0, 65535]\n",
        "        image = images[i].squeeze() * 65535\n",
        "\n",
        "        # Contrast stretching the original image for visualization (directly doing 16bit to 8bit got fucked)\n",
        "        vmin, vmax = image.min(), image.max()\n",
        "        image = (image - vmin) / (vmax - vmin)  # Scale pixel values to 0-1 range\n",
        "        image = (image * 255).to(torch.uint8).cpu().numpy()  # Scale up to 0-255 range and convert to 8-bit\n",
        "\n",
        "        # Display the original image\n",
        "        plt.subplot(1, len(images)*3, i*3+1)\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.title(f'Original Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Display the actual label image\n",
        "        actual = actuals[i].squeeze()  # Assuming actuals is a batch of label images\n",
        "        plt.subplot(1, len(images)*3, i*3+2)\n",
        "        plt.imshow(actual, cmap='gray')\n",
        "        plt.title(f'Actual Label')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Display the predicted label image\n",
        "        pred = predicted[i].squeeze()  # Assuming predicted is a batch of label images\n",
        "        plt.subplot(1, len(images)*3, i*3+3)\n",
        "        plt.imshow(pred, cmap='gray')\n",
        "        plt.title(f'Predicted Label')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zbb85T-1iL01",
        "outputId": "982b48c8-eaa7-4aea-b53f-897140141efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n",
            "\n",
            "Training on subset size: 100\n",
            "Epoch 1/250, Batch 1/3, Loss: 1.1048831939697266\n",
            "Validation Loss after Epoch 1: 1.0918713808059692\n",
            "Epoch 2/250, Batch 1/3, Loss: 0.8326792120933533\n",
            "Validation Loss after Epoch 2: 1.0770385265350342\n",
            "Epoch 3/250, Batch 1/3, Loss: 0.7375583648681641\n",
            "Validation Loss after Epoch 3: 1.0553982257843018\n",
            "Epoch 4/250, Batch 1/3, Loss: 0.6643237471580505\n",
            "Validation Loss after Epoch 4: 1.043819546699524\n",
            "Epoch 5/250, Batch 1/3, Loss: 0.6203867793083191\n",
            "Validation Loss after Epoch 5: 1.0650546550750732\n",
            "Epoch 6/250, Batch 1/3, Loss: 0.5677503943443298\n",
            "Validation Loss after Epoch 6: 1.1324055194854736\n",
            "Epoch 7/250, Batch 1/3, Loss: 0.5216399431228638\n",
            "Validation Loss after Epoch 7: 1.2057721614837646\n",
            "Epoch 8/250, Batch 1/3, Loss: 0.4835641384124756\n",
            "Validation Loss after Epoch 8: 1.2749710083007812\n",
            "Epoch 9/250, Batch 1/3, Loss: 0.4536415636539459\n",
            "Validation Loss after Epoch 9: 1.3010973930358887\n",
            "Epoch 10/250, Batch 1/3, Loss: 0.4059072434902191\n",
            "Validation Loss after Epoch 10: 1.2740721702575684\n",
            "Epoch 11/250, Batch 1/3, Loss: 0.3841013014316559\n",
            "Validation Loss after Epoch 11: 1.2827767133712769\n",
            "Epoch 12/250, Batch 1/3, Loss: 0.3425856828689575\n",
            "Validation Loss after Epoch 12: 1.2172116041183472\n",
            "Epoch 13/250, Batch 1/3, Loss: 0.31071552634239197\n",
            "Validation Loss after Epoch 13: 1.1420397758483887\n",
            "Epoch 14/250, Batch 1/3, Loss: 0.29004454612731934\n",
            "Validation Loss after Epoch 14: 0.9649193286895752\n",
            "Epoch 15/250, Batch 1/3, Loss: 0.2597583532333374\n",
            "Validation Loss after Epoch 15: 0.8728826642036438\n",
            "Epoch 16/250, Batch 1/3, Loss: 0.23340201377868652\n",
            "Validation Loss after Epoch 16: 0.7316984534263611\n",
            "Epoch 17/250, Batch 1/3, Loss: 0.20901407301425934\n",
            "Validation Loss after Epoch 17: 0.5728906989097595\n",
            "Epoch 18/250, Batch 1/3, Loss: 0.19373640418052673\n",
            "Validation Loss after Epoch 18: 0.3703804910182953\n",
            "Epoch 19/250, Batch 1/3, Loss: 0.18348285555839539\n",
            "Validation Loss after Epoch 19: 0.3227645456790924\n",
            "Epoch 20/250, Batch 1/3, Loss: 0.18192751705646515\n",
            "Validation Loss after Epoch 20: 0.28007638454437256\n",
            "Epoch 21/250, Batch 1/3, Loss: 0.17891016602516174\n",
            "Validation Loss after Epoch 21: 0.2615850269794464\n",
            "Epoch 22/250, Batch 1/3, Loss: 0.15061041712760925\n",
            "Validation Loss after Epoch 22: 0.23910704255104065\n",
            "Epoch 23/250, Batch 1/3, Loss: 0.1590520292520523\n",
            "Validation Loss after Epoch 23: 0.21222376823425293\n",
            "Epoch 24/250, Batch 1/3, Loss: 0.14974962174892426\n",
            "Validation Loss after Epoch 24: 0.20531868934631348\n",
            "Epoch 25/250, Batch 1/3, Loss: 0.13206501305103302\n",
            "Validation Loss after Epoch 25: 0.20507901906967163\n",
            "Epoch 26/250, Batch 1/3, Loss: 0.12954136729240417\n",
            "Validation Loss after Epoch 26: 0.20046663284301758\n",
            "Epoch 27/250, Batch 1/3, Loss: 0.11428658664226532\n",
            "Validation Loss after Epoch 27: 0.19853831827640533\n",
            "Epoch 28/250, Batch 1/3, Loss: 0.12089882045984268\n",
            "Validation Loss after Epoch 28: 0.19555065035820007\n",
            "Epoch 29/250, Batch 1/3, Loss: 0.11162956058979034\n",
            "Validation Loss after Epoch 29: 0.18681122362613678\n",
            "Epoch 30/250, Batch 1/3, Loss: 0.10575095564126968\n",
            "Validation Loss after Epoch 30: 0.19474747776985168\n",
            "Epoch 31/250, Batch 1/3, Loss: 0.10274267196655273\n",
            "Validation Loss after Epoch 31: 0.20021666586399078\n",
            "Epoch 32/250, Batch 1/3, Loss: 0.10337237268686295\n",
            "Validation Loss after Epoch 32: 0.19383245706558228\n",
            "Epoch 33/250, Batch 1/3, Loss: 0.09601972997188568\n",
            "Validation Loss after Epoch 33: 0.18882574141025543\n",
            "Epoch 34/250, Batch 1/3, Loss: 0.09058213979005814\n",
            "Validation Loss after Epoch 34: 0.19591496884822845\n",
            "Epoch 35/250, Batch 1/3, Loss: 0.09054064005613327\n",
            "Validation Loss after Epoch 35: 0.19591562449932098\n",
            "Epoch 36/250, Batch 1/3, Loss: 0.0887703001499176\n",
            "Validation Loss after Epoch 36: 0.20017242431640625\n",
            "Epoch 37/250, Batch 1/3, Loss: 0.08705997467041016\n",
            "Validation Loss after Epoch 37: 0.2037082016468048\n",
            "Epoch 38/250, Batch 1/3, Loss: 0.09142827987670898\n",
            "Validation Loss after Epoch 38: 0.20375826954841614\n",
            "Epoch 39/250, Batch 1/3, Loss: 0.08172786235809326\n",
            "Validation Loss after Epoch 39: 0.20116139948368073\n",
            "Epoch 40/250, Batch 1/3, Loss: 0.0854853168129921\n",
            "Validation Loss after Epoch 40: 0.2017090767621994\n",
            "Epoch 41/250, Batch 1/3, Loss: 0.07504460215568542\n",
            "Validation Loss after Epoch 41: 0.19802644848823547\n",
            "Epoch 42/250, Batch 1/3, Loss: 0.0823342427611351\n",
            "Validation Loss after Epoch 42: 0.1983613669872284\n",
            "Epoch 43/250, Batch 1/3, Loss: 0.07254325598478317\n",
            "Validation Loss after Epoch 43: 0.1973641812801361\n",
            "Epoch 44/250, Batch 1/3, Loss: 0.07855305075645447\n",
            "Validation Loss after Epoch 44: 0.19170257449150085\n",
            "Epoch 45/250, Batch 1/3, Loss: 0.06748369336128235\n",
            "Validation Loss after Epoch 45: 0.1960321068763733\n",
            "Epoch 46/250, Batch 1/3, Loss: 0.07437138259410858\n",
            "Validation Loss after Epoch 46: 0.20601731538772583\n",
            "Epoch 47/250, Batch 1/3, Loss: 0.0719885304570198\n",
            "Validation Loss after Epoch 47: 0.20107804238796234\n",
            "Epoch 48/250, Batch 1/3, Loss: 0.06684315949678421\n",
            "Validation Loss after Epoch 48: 0.199971005320549\n",
            "Epoch 49/250, Batch 1/3, Loss: 0.06539376825094223\n",
            "Validation Loss after Epoch 49: 0.20249062776565552\n",
            "Epoch 50/250, Batch 1/3, Loss: 0.060021232813596725\n",
            "Validation Loss after Epoch 50: 0.20783357322216034\n",
            "Epoch 51/250, Batch 1/3, Loss: 0.06741223484277725\n",
            "Validation Loss after Epoch 51: 0.2116965502500534\n",
            "Epoch 52/250, Batch 1/3, Loss: 0.06461324542760849\n",
            "Validation Loss after Epoch 52: 0.21466663479804993\n",
            "Epoch 53/250, Batch 1/3, Loss: 0.06940830498933792\n",
            "Validation Loss after Epoch 53: 0.20326659083366394\n",
            "Epoch 54/250, Batch 1/3, Loss: 0.06375589966773987\n",
            "Validation Loss after Epoch 54: 0.2034595012664795\n",
            "Epoch 55/250, Batch 1/3, Loss: 0.06434710323810577\n",
            "Validation Loss after Epoch 55: 0.2080775648355484\n",
            "Epoch 56/250, Batch 1/3, Loss: 0.05943569168448448\n",
            "Validation Loss after Epoch 56: 0.20888932049274445\n",
            "Epoch 57/250, Batch 1/3, Loss: 0.05642470344901085\n",
            "Validation Loss after Epoch 57: 0.20351026952266693\n",
            "Epoch 58/250, Batch 1/3, Loss: 0.056314051151275635\n",
            "Validation Loss after Epoch 58: 0.2017040103673935\n",
            "Epoch 59/250, Batch 1/3, Loss: 0.05098741129040718\n",
            "Validation Loss after Epoch 59: 0.2088109850883484\n",
            "Epoch 60/250, Batch 1/3, Loss: 0.05219871550798416\n",
            "Validation Loss after Epoch 60: 0.21140466630458832\n",
            "Epoch 61/250, Batch 1/3, Loss: 0.053579505532979965\n",
            "Validation Loss after Epoch 61: 0.20725186169147491\n",
            "Epoch 62/250, Batch 1/3, Loss: 0.05376502498984337\n",
            "Validation Loss after Epoch 62: 0.20788979530334473\n",
            "Epoch 63/250, Batch 1/3, Loss: 0.057600393891334534\n",
            "Validation Loss after Epoch 63: 0.21473421156406403\n",
            "Epoch 64/250, Batch 1/3, Loss: 0.05606165900826454\n",
            "Validation Loss after Epoch 64: 0.2120574563741684\n",
            "Epoch 65/250, Batch 1/3, Loss: 0.05466644838452339\n",
            "Validation Loss after Epoch 65: 0.20837058126926422\n",
            "Epoch 66/250, Batch 1/3, Loss: 0.050760917365550995\n",
            "Validation Loss after Epoch 66: 0.21111883223056793\n",
            "Epoch 67/250, Batch 1/3, Loss: 0.05050186440348625\n",
            "Validation Loss after Epoch 67: 0.2111484706401825\n",
            "Epoch 68/250, Batch 1/3, Loss: 0.048598263412714005\n",
            "Validation Loss after Epoch 68: 0.21571771800518036\n",
            "Epoch 69/250, Batch 1/3, Loss: 0.05582405999302864\n",
            "Validation Loss after Epoch 69: 0.2191496193408966\n",
            "Epoch 70/250, Batch 1/3, Loss: 0.04750485718250275\n",
            "Validation Loss after Epoch 70: 0.21922184526920319\n",
            "Epoch 71/250, Batch 1/3, Loss: 0.04170231893658638\n",
            "Validation Loss after Epoch 71: 0.2134481519460678\n",
            "Epoch 72/250, Batch 1/3, Loss: 0.04485754296183586\n",
            "Validation Loss after Epoch 72: 0.21140141785144806\n",
            "Epoch 73/250, Batch 1/3, Loss: 0.04421168938279152\n",
            "Validation Loss after Epoch 73: 0.21137362718582153\n",
            "Epoch 74/250, Batch 1/3, Loss: 0.04798293486237526\n",
            "Validation Loss after Epoch 74: 0.21233734488487244\n",
            "Epoch 75/250, Batch 1/3, Loss: 0.04210946336388588\n",
            "Validation Loss after Epoch 75: 0.2200574427843094\n",
            "Epoch 76/250, Batch 1/3, Loss: 0.04775072634220123\n",
            "Validation Loss after Epoch 76: 0.22312839329242706\n",
            "Epoch 77/250, Batch 1/3, Loss: 0.049468621611595154\n",
            "Validation Loss after Epoch 77: 0.22089196741580963\n",
            "Epoch 78/250, Batch 1/3, Loss: 0.04383315518498421\n",
            "Validation Loss after Epoch 78: 0.2235896736383438\n",
            "Epoch 79/250, Batch 1/3, Loss: 0.0461648665368557\n",
            "Validation Loss after Epoch 79: 0.2307935655117035\n",
            "Epoch 80/250, Batch 1/3, Loss: 0.039800338447093964\n",
            "Validation Loss after Epoch 80: 0.22844679653644562\n",
            "Epoch 81/250, Batch 1/3, Loss: 0.040833067148923874\n",
            "Validation Loss after Epoch 81: 0.22048933804035187\n",
            "Epoch 82/250, Batch 1/3, Loss: 0.036990538239479065\n",
            "Validation Loss after Epoch 82: 0.21233251690864563\n",
            "Epoch 83/250, Batch 1/3, Loss: 0.03706064075231552\n",
            "Validation Loss after Epoch 83: 0.21173042058944702\n",
            "Epoch 84/250, Batch 1/3, Loss: 0.03664273023605347\n",
            "Validation Loss after Epoch 84: 0.2178596407175064\n",
            "Epoch 85/250, Batch 1/3, Loss: 0.0405849814414978\n",
            "Validation Loss after Epoch 85: 0.22367365658283234\n",
            "Epoch 86/250, Batch 1/3, Loss: 0.04403975233435631\n",
            "Validation Loss after Epoch 86: 0.2237570732831955\n",
            "Epoch 87/250, Batch 1/3, Loss: 0.035417601466178894\n",
            "Validation Loss after Epoch 87: 0.22613567113876343\n",
            "Epoch 88/250, Batch 1/3, Loss: 0.0445021390914917\n",
            "Validation Loss after Epoch 88: 0.22233960032463074\n",
            "Epoch 89/250, Batch 1/3, Loss: 0.0357385091483593\n",
            "Validation Loss after Epoch 89: 0.21987758576869965\n",
            "Epoch 90/250, Batch 1/3, Loss: 0.038554802536964417\n",
            "Validation Loss after Epoch 90: 0.2107376754283905\n",
            "Epoch 91/250, Batch 1/3, Loss: 0.03446190804243088\n",
            "Validation Loss after Epoch 91: 0.20890124142169952\n",
            "Epoch 92/250, Batch 1/3, Loss: 0.034943412989377975\n",
            "Validation Loss after Epoch 92: 0.20986108481884003\n",
            "Epoch 93/250, Batch 1/3, Loss: 0.036878641694784164\n",
            "Validation Loss after Epoch 93: 0.20875807106494904\n",
            "Epoch 94/250, Batch 1/3, Loss: 0.03768949210643768\n",
            "Validation Loss after Epoch 94: 0.21856729686260223\n",
            "Epoch 95/250, Batch 1/3, Loss: 0.03937060385942459\n",
            "Validation Loss after Epoch 95: 0.22876380383968353\n",
            "Epoch 96/250, Batch 1/3, Loss: 0.03802257776260376\n",
            "Validation Loss after Epoch 96: 0.22881466150283813\n",
            "Epoch 97/250, Batch 1/3, Loss: 0.04080962389707565\n",
            "Validation Loss after Epoch 97: 0.22186869382858276\n",
            "Epoch 98/250, Batch 1/3, Loss: 0.038306824862957\n",
            "Validation Loss after Epoch 98: 0.2179315835237503\n",
            "Epoch 99/250, Batch 1/3, Loss: 0.034695178270339966\n",
            "Validation Loss after Epoch 99: 0.22060684859752655\n",
            "Epoch 100/250, Batch 1/3, Loss: 0.03528251126408577\n",
            "Validation Loss after Epoch 100: 0.22247114777565002\n",
            "Epoch 101/250, Batch 1/3, Loss: 0.035811685025691986\n",
            "Validation Loss after Epoch 101: 0.21970811486244202\n",
            "Epoch 102/250, Batch 1/3, Loss: 0.03552206978201866\n",
            "Validation Loss after Epoch 102: 0.21696306765079498\n",
            "Epoch 103/250, Batch 1/3, Loss: 0.03539027273654938\n",
            "Validation Loss after Epoch 103: 0.21906276047229767\n",
            "Epoch 104/250, Batch 1/3, Loss: 0.0342487134039402\n",
            "Validation Loss after Epoch 104: 0.22022101283073425\n",
            "Epoch 105/250, Batch 1/3, Loss: 0.036113057285547256\n",
            "Validation Loss after Epoch 105: 0.22205416858196259\n",
            "Epoch 106/250, Batch 1/3, Loss: 0.034652359783649445\n",
            "Validation Loss after Epoch 106: 0.2253681868314743\n",
            "Epoch 107/250, Batch 1/3, Loss: 0.03054078109562397\n",
            "Validation Loss after Epoch 107: 0.2327050119638443\n",
            "Epoch 108/250, Batch 1/3, Loss: 0.033823080360889435\n",
            "Validation Loss after Epoch 108: 0.2367764115333557\n",
            "Epoch 109/250, Batch 1/3, Loss: 0.03524358570575714\n",
            "Validation Loss after Epoch 109: 0.23387810587882996\n",
            "Epoch 110/250, Batch 1/3, Loss: 0.03021974489092827\n",
            "Validation Loss after Epoch 110: 0.22906562685966492\n",
            "Epoch 111/250, Batch 1/3, Loss: 0.02947091870009899\n",
            "Validation Loss after Epoch 111: 0.22684235870838165\n",
            "Epoch 112/250, Batch 1/3, Loss: 0.03146630898118019\n",
            "Validation Loss after Epoch 112: 0.22814297676086426\n",
            "Epoch 113/250, Batch 1/3, Loss: 0.032430268824100494\n",
            "Validation Loss after Epoch 113: 0.2280692160129547\n",
            "Epoch 114/250, Batch 1/3, Loss: 0.029925234615802765\n",
            "Validation Loss after Epoch 114: 0.22832636535167694\n",
            "Epoch 115/250, Batch 1/3, Loss: 0.0346180684864521\n",
            "Validation Loss after Epoch 115: 0.22855983674526215\n",
            "Epoch 116/250, Batch 1/3, Loss: 0.030867908149957657\n",
            "Validation Loss after Epoch 116: 0.23006770014762878\n",
            "Epoch 117/250, Batch 1/3, Loss: 0.03431957960128784\n",
            "Validation Loss after Epoch 117: 0.23099178075790405\n",
            "Epoch 118/250, Batch 1/3, Loss: 0.03655536100268364\n",
            "Validation Loss after Epoch 118: 0.23404741287231445\n",
            "Epoch 119/250, Batch 1/3, Loss: 0.029653631150722504\n",
            "Validation Loss after Epoch 119: 0.23318412899971008\n",
            "Epoch 120/250, Batch 1/3, Loss: 0.028576767072081566\n",
            "Validation Loss after Epoch 120: 0.23075029253959656\n",
            "Epoch 121/250, Batch 1/3, Loss: 0.02658895216882229\n",
            "Validation Loss after Epoch 121: 0.22963200509548187\n",
            "Epoch 122/250, Batch 1/3, Loss: 0.030743099749088287\n",
            "Validation Loss after Epoch 122: 0.23508355021476746\n",
            "Epoch 123/250, Batch 1/3, Loss: 0.029951320961117744\n",
            "Validation Loss after Epoch 123: 0.24374812841415405\n",
            "Epoch 124/250, Batch 1/3, Loss: 0.031047873198986053\n",
            "Validation Loss after Epoch 124: 0.24602237343788147\n",
            "Epoch 125/250, Batch 1/3, Loss: 0.02893787994980812\n",
            "Validation Loss after Epoch 125: 0.24329736828804016\n",
            "Epoch 126/250, Batch 1/3, Loss: 0.023044437170028687\n",
            "Validation Loss after Epoch 126: 0.23754994571208954\n",
            "Epoch 127/250, Batch 1/3, Loss: 0.02691156417131424\n",
            "Validation Loss after Epoch 127: 0.23095612227916718\n",
            "Epoch 128/250, Batch 1/3, Loss: 0.0316479317843914\n",
            "Validation Loss after Epoch 128: 0.22886601090431213\n",
            "Epoch 129/250, Batch 1/3, Loss: 0.029290128499269485\n",
            "Validation Loss after Epoch 129: 0.22974102199077606\n",
            "Epoch 130/250, Batch 1/3, Loss: 0.03395123407244682\n",
            "Validation Loss after Epoch 130: 0.22782482206821442\n",
            "Epoch 131/250, Batch 1/3, Loss: 0.024839138612151146\n",
            "Validation Loss after Epoch 131: 0.2263219803571701\n",
            "Epoch 132/250, Batch 1/3, Loss: 0.033428288996219635\n",
            "Validation Loss after Epoch 132: 0.2283056527376175\n",
            "Epoch 133/250, Batch 1/3, Loss: 0.027118712663650513\n",
            "Validation Loss after Epoch 133: 0.230345219373703\n",
            "Epoch 134/250, Batch 1/3, Loss: 0.03432165086269379\n",
            "Validation Loss after Epoch 134: 0.23213425278663635\n",
            "Epoch 135/250, Batch 1/3, Loss: 0.03177744150161743\n",
            "Validation Loss after Epoch 135: 0.23230217397212982\n",
            "Epoch 136/250, Batch 1/3, Loss: 0.026419328525662422\n",
            "Validation Loss after Epoch 136: 0.23486876487731934\n",
            "Epoch 137/250, Batch 1/3, Loss: 0.0282334815710783\n",
            "Validation Loss after Epoch 137: 0.23572520911693573\n",
            "Epoch 138/250, Batch 1/3, Loss: 0.030358001589775085\n",
            "Validation Loss after Epoch 138: 0.2347366064786911\n",
            "Epoch 139/250, Batch 1/3, Loss: 0.028125658631324768\n",
            "Validation Loss after Epoch 139: 0.23155085742473602\n",
            "Epoch 140/250, Batch 1/3, Loss: 0.02724543772637844\n",
            "Validation Loss after Epoch 140: 0.23753663897514343\n",
            "Epoch 141/250, Batch 1/3, Loss: 0.02025330439209938\n",
            "Validation Loss after Epoch 141: 0.2433403581380844\n",
            "Epoch 142/250, Batch 1/3, Loss: 0.024470558390021324\n",
            "Validation Loss after Epoch 142: 0.2480078637599945\n",
            "Epoch 143/250, Batch 1/3, Loss: 0.030988451093435287\n",
            "Validation Loss after Epoch 143: 0.2550731301307678\n",
            "Epoch 144/250, Batch 1/3, Loss: 0.025055699050426483\n",
            "Validation Loss after Epoch 144: 0.2609676718711853\n",
            "Epoch 145/250, Batch 1/3, Loss: 0.025182625278830528\n",
            "Validation Loss after Epoch 145: 0.2554338276386261\n",
            "Epoch 146/250, Batch 1/3, Loss: 0.022966088727116585\n",
            "Validation Loss after Epoch 146: 0.24791350960731506\n",
            "Epoch 147/250, Batch 1/3, Loss: 0.0326356515288353\n",
            "Validation Loss after Epoch 147: 0.24476610124111176\n",
            "Epoch 148/250, Batch 1/3, Loss: 0.027165982872247696\n",
            "Validation Loss after Epoch 148: 0.24865378439426422\n",
            "Epoch 149/250, Batch 1/3, Loss: 0.026209719479084015\n",
            "Validation Loss after Epoch 149: 0.25515037775039673\n",
            "Epoch 150/250, Batch 1/3, Loss: 0.02998880296945572\n",
            "Validation Loss after Epoch 150: 0.25651293992996216\n",
            "Epoch 151/250, Batch 1/3, Loss: 0.021666334941983223\n",
            "Validation Loss after Epoch 151: 0.25046178698539734\n",
            "Epoch 152/250, Batch 1/3, Loss: 0.023845983669161797\n",
            "Validation Loss after Epoch 152: 0.24349123239517212\n",
            "Epoch 153/250, Batch 1/3, Loss: 0.021392976865172386\n",
            "Validation Loss after Epoch 153: 0.23794813454151154\n",
            "Epoch 154/250, Batch 1/3, Loss: 0.0269046388566494\n",
            "Validation Loss after Epoch 154: 0.23516443371772766\n",
            "Epoch 155/250, Batch 1/3, Loss: 0.025362791493535042\n",
            "Validation Loss after Epoch 155: 0.23973426222801208\n",
            "Epoch 156/250, Batch 1/3, Loss: 0.029724106192588806\n",
            "Validation Loss after Epoch 156: 0.2471778690814972\n",
            "Epoch 157/250, Batch 1/3, Loss: 0.023537153378129005\n",
            "Validation Loss after Epoch 157: 0.2535313069820404\n",
            "Epoch 158/250, Batch 1/3, Loss: 0.03156900778412819\n",
            "Validation Loss after Epoch 158: 0.25490495562553406\n",
            "Epoch 159/250, Batch 1/3, Loss: 0.024399731308221817\n",
            "Validation Loss after Epoch 159: 0.25401678681373596\n",
            "Epoch 160/250, Batch 1/3, Loss: 0.026428217068314552\n",
            "Validation Loss after Epoch 160: 0.2503642737865448\n",
            "Epoch 161/250, Batch 1/3, Loss: 0.02717546559870243\n",
            "Validation Loss after Epoch 161: 0.24870148301124573\n",
            "Epoch 162/250, Batch 1/3, Loss: 0.022496217861771584\n",
            "Validation Loss after Epoch 162: 0.2477894276380539\n",
            "Epoch 163/250, Batch 1/3, Loss: 0.023419011384248734\n",
            "Validation Loss after Epoch 163: 0.24841365218162537\n",
            "Epoch 164/250, Batch 1/3, Loss: 0.021938510239124298\n",
            "Validation Loss after Epoch 164: 0.24749130010604858\n",
            "Epoch 165/250, Batch 1/3, Loss: 0.0224627535790205\n",
            "Validation Loss after Epoch 165: 0.24609018862247467\n",
            "Epoch 166/250, Batch 1/3, Loss: 0.027290353551506996\n",
            "Validation Loss after Epoch 166: 0.24665850400924683\n",
            "Epoch 167/250, Batch 1/3, Loss: 0.027463680133223534\n",
            "Validation Loss after Epoch 167: 0.24570971727371216\n",
            "Epoch 168/250, Batch 1/3, Loss: 0.024880312383174896\n",
            "Validation Loss after Epoch 168: 0.24545332789421082\n",
            "Epoch 169/250, Batch 1/3, Loss: 0.023503120988607407\n",
            "Validation Loss after Epoch 169: 0.25018078088760376\n",
            "Epoch 170/250, Batch 1/3, Loss: 0.021188832819461823\n",
            "Validation Loss after Epoch 170: 0.24942231178283691\n",
            "Epoch 171/250, Batch 1/3, Loss: 0.023043615743517876\n",
            "Validation Loss after Epoch 171: 0.24827973544597626\n",
            "Epoch 172/250, Batch 1/3, Loss: 0.019570589065551758\n",
            "Validation Loss after Epoch 172: 0.2463027983903885\n",
            "Epoch 173/250, Batch 1/3, Loss: 0.023772912099957466\n",
            "Validation Loss after Epoch 173: 0.2437417209148407\n",
            "Epoch 174/250, Batch 1/3, Loss: 0.024850523099303246\n",
            "Validation Loss after Epoch 174: 0.24716231226921082\n",
            "Epoch 175/250, Batch 1/3, Loss: 0.022808397188782692\n",
            "Validation Loss after Epoch 175: 0.24971546232700348\n",
            "Epoch 176/250, Batch 1/3, Loss: 0.027887172996997833\n",
            "Validation Loss after Epoch 176: 0.25216177105903625\n",
            "Epoch 177/250, Batch 1/3, Loss: 0.021485047414898872\n",
            "Validation Loss after Epoch 177: 0.25420719385147095\n",
            "Epoch 178/250, Batch 1/3, Loss: 0.020605703815817833\n",
            "Validation Loss after Epoch 178: 0.2566875219345093\n",
            "Epoch 179/250, Batch 1/3, Loss: 0.01945377141237259\n",
            "Validation Loss after Epoch 179: 0.2610705494880676\n",
            "Epoch 180/250, Batch 1/3, Loss: 0.02191893756389618\n",
            "Validation Loss after Epoch 180: 0.2599072754383087\n",
            "Epoch 181/250, Batch 1/3, Loss: 0.02257920801639557\n",
            "Validation Loss after Epoch 181: 0.25433313846588135\n",
            "Epoch 182/250, Batch 1/3, Loss: 0.021290913224220276\n",
            "Validation Loss after Epoch 182: 0.2514864206314087\n",
            "Epoch 183/250, Batch 1/3, Loss: 0.02445584163069725\n",
            "Validation Loss after Epoch 183: 0.25119492411613464\n",
            "Epoch 184/250, Batch 1/3, Loss: 0.021702464669942856\n",
            "Validation Loss after Epoch 184: 0.2531750202178955\n",
            "Epoch 185/250, Batch 1/3, Loss: 0.02002043090760708\n",
            "Validation Loss after Epoch 185: 0.25300678610801697\n",
            "Epoch 186/250, Batch 1/3, Loss: 0.018846740946173668\n",
            "Validation Loss after Epoch 186: 0.25085923075675964\n",
            "Epoch 187/250, Batch 1/3, Loss: 0.023241568356752396\n",
            "Validation Loss after Epoch 187: 0.250576913356781\n",
            "Epoch 188/250, Batch 1/3, Loss: 0.021654998883605003\n",
            "Validation Loss after Epoch 188: 0.2541320323944092\n",
            "Epoch 189/250, Batch 1/3, Loss: 0.022573990747332573\n",
            "Validation Loss after Epoch 189: 0.25861865282058716\n",
            "Epoch 190/250, Batch 1/3, Loss: 0.018962323665618896\n",
            "Validation Loss after Epoch 190: 0.26352861523628235\n",
            "Epoch 191/250, Batch 1/3, Loss: 0.025157324969768524\n",
            "Validation Loss after Epoch 191: 0.2545227110385895\n",
            "Epoch 192/250, Batch 1/3, Loss: 0.02069607935845852\n",
            "Validation Loss after Epoch 192: 0.24265126883983612\n",
            "Epoch 193/250, Batch 1/3, Loss: 0.019853625446558\n",
            "Validation Loss after Epoch 193: 0.2432609498500824\n",
            "Epoch 194/250, Batch 1/3, Loss: 0.028194868937134743\n",
            "Validation Loss after Epoch 194: 0.24879677593708038\n",
            "Epoch 195/250, Batch 1/3, Loss: 0.023441903293132782\n",
            "Validation Loss after Epoch 195: 0.2621382772922516\n",
            "Epoch 196/250, Batch 1/3, Loss: 0.02829376608133316\n",
            "Validation Loss after Epoch 196: 0.26955708861351013\n",
            "Epoch 197/250, Batch 1/3, Loss: 0.02370089292526245\n",
            "Validation Loss after Epoch 197: 0.26889768242836\n",
            "Epoch 198/250, Batch 1/3, Loss: 0.026710424572229385\n",
            "Validation Loss after Epoch 198: 0.2604920268058777\n",
            "Epoch 199/250, Batch 1/3, Loss: 0.016736144199967384\n",
            "Validation Loss after Epoch 199: 0.2511155903339386\n",
            "Epoch 200/250, Batch 1/3, Loss: 0.019058212637901306\n",
            "Validation Loss after Epoch 200: 0.24702143669128418\n",
            "Epoch 201/250, Batch 1/3, Loss: 0.018187660723924637\n",
            "Validation Loss after Epoch 201: 0.2452181726694107\n",
            "Epoch 202/250, Batch 1/3, Loss: 0.024852188304066658\n",
            "Validation Loss after Epoch 202: 0.24345450103282928\n",
            "Epoch 203/250, Batch 1/3, Loss: 0.018591037020087242\n",
            "Validation Loss after Epoch 203: 0.24600951373577118\n",
            "Epoch 204/250, Batch 1/3, Loss: 0.02182396873831749\n",
            "Validation Loss after Epoch 204: 0.25035199522972107\n",
            "Epoch 205/250, Batch 1/3, Loss: 0.021554896607995033\n",
            "Validation Loss after Epoch 205: 0.25365781784057617\n",
            "Epoch 206/250, Batch 1/3, Loss: 0.01572733372449875\n",
            "Validation Loss after Epoch 206: 0.2573530375957489\n",
            "Epoch 207/250, Batch 1/3, Loss: 0.01916801743209362\n",
            "Validation Loss after Epoch 207: 0.2588433623313904\n",
            "Epoch 208/250, Batch 1/3, Loss: 0.022322459146380424\n",
            "Validation Loss after Epoch 208: 0.2639920115470886\n",
            "Epoch 209/250, Batch 1/3, Loss: 0.01889589987695217\n",
            "Validation Loss after Epoch 209: 0.2662045657634735\n",
            "Epoch 210/250, Batch 1/3, Loss: 0.023147301748394966\n",
            "Validation Loss after Epoch 210: 0.2685035765171051\n",
            "Epoch 211/250, Batch 1/3, Loss: 0.022829493507742882\n",
            "Validation Loss after Epoch 211: 0.26931822299957275\n",
            "Epoch 212/250, Batch 1/3, Loss: 0.01608269289135933\n",
            "Validation Loss after Epoch 212: 0.267451673746109\n",
            "Epoch 213/250, Batch 1/3, Loss: 0.01949501782655716\n",
            "Validation Loss after Epoch 213: 0.26530322432518005\n",
            "Epoch 214/250, Batch 1/3, Loss: 0.019497863948345184\n",
            "Validation Loss after Epoch 214: 0.2621731460094452\n",
            "Epoch 215/250, Batch 1/3, Loss: 0.022706549614667892\n",
            "Validation Loss after Epoch 215: 0.2560994029045105\n",
            "Epoch 216/250, Batch 1/3, Loss: 0.021343272179365158\n",
            "Validation Loss after Epoch 216: 0.25873449444770813\n",
            "Epoch 217/250, Batch 1/3, Loss: 0.023687690496444702\n",
            "Validation Loss after Epoch 217: 0.2682589292526245\n",
            "Epoch 218/250, Batch 1/3, Loss: 0.019619328901171684\n",
            "Validation Loss after Epoch 218: 0.271613210439682\n",
            "Epoch 219/250, Batch 1/3, Loss: 0.023269539698958397\n",
            "Validation Loss after Epoch 219: 0.27314233779907227\n",
            "Epoch 220/250, Batch 1/3, Loss: 0.023917995393276215\n",
            "Validation Loss after Epoch 220: 0.27109596133232117\n",
            "Epoch 221/250, Batch 1/3, Loss: 0.014927512966096401\n",
            "Validation Loss after Epoch 221: 0.27080488204956055\n",
            "Epoch 222/250, Batch 1/3, Loss: 0.018636269494891167\n",
            "Validation Loss after Epoch 222: 0.2739418148994446\n",
            "Epoch 223/250, Batch 1/3, Loss: 0.015510836616158485\n",
            "Validation Loss after Epoch 223: 0.2769894003868103\n",
            "Epoch 224/250, Batch 1/3, Loss: 0.016020972281694412\n",
            "Validation Loss after Epoch 224: 0.2790878117084503\n",
            "Epoch 225/250, Batch 1/3, Loss: 0.014947883784770966\n",
            "Validation Loss after Epoch 225: 0.2804136872291565\n",
            "Epoch 226/250, Batch 1/3, Loss: 0.01635274849832058\n",
            "Validation Loss after Epoch 226: 0.2758720815181732\n",
            "Epoch 227/250, Batch 1/3, Loss: 0.018328828737139702\n",
            "Validation Loss after Epoch 227: 0.2711467146873474\n",
            "Epoch 228/250, Batch 1/3, Loss: 0.01734638586640358\n",
            "Validation Loss after Epoch 228: 0.2687959671020508\n",
            "Epoch 229/250, Batch 1/3, Loss: 0.018218113109469414\n",
            "Validation Loss after Epoch 229: 0.2719530761241913\n",
            "Epoch 230/250, Batch 1/3, Loss: 0.02105778641998768\n",
            "Validation Loss after Epoch 230: 0.27603280544281006\n",
            "Epoch 231/250, Batch 1/3, Loss: 0.016173461452126503\n",
            "Validation Loss after Epoch 231: 0.2799864709377289\n",
            "Epoch 232/250, Batch 1/3, Loss: 0.01409674808382988\n",
            "Validation Loss after Epoch 232: 0.2807662785053253\n",
            "Epoch 233/250, Batch 1/3, Loss: 0.017695583403110504\n",
            "Validation Loss after Epoch 233: 0.2788105905056\n",
            "Epoch 234/250, Batch 1/3, Loss: 0.019301772117614746\n",
            "Validation Loss after Epoch 234: 0.27317702770233154\n",
            "Epoch 235/250, Batch 1/3, Loss: 0.013569974340498447\n",
            "Validation Loss after Epoch 235: 0.27147626876831055\n",
            "Epoch 236/250, Batch 1/3, Loss: 0.019551273435354233\n",
            "Validation Loss after Epoch 236: 0.2726241648197174\n",
            "Epoch 237/250, Batch 1/3, Loss: 0.017702514305710793\n",
            "Validation Loss after Epoch 237: 0.275522381067276\n",
            "Epoch 238/250, Batch 1/3, Loss: 0.016030985862016678\n",
            "Validation Loss after Epoch 238: 0.2787344753742218\n",
            "Epoch 239/250, Batch 1/3, Loss: 0.01992349699139595\n",
            "Validation Loss after Epoch 239: 0.2836736738681793\n",
            "Epoch 240/250, Batch 1/3, Loss: 0.01498082373291254\n",
            "Validation Loss after Epoch 240: 0.28562799096107483\n",
            "Epoch 241/250, Batch 1/3, Loss: 0.016748379915952682\n",
            "Validation Loss after Epoch 241: 0.2822995185852051\n",
            "Epoch 242/250, Batch 1/3, Loss: 0.01676158979535103\n",
            "Validation Loss after Epoch 242: 0.2804332971572876\n",
            "Epoch 243/250, Batch 1/3, Loss: 0.029476918280124664\n",
            "Validation Loss after Epoch 243: 0.2724953889846802\n",
            "Epoch 244/250, Batch 1/3, Loss: 0.02677520550787449\n",
            "Validation Loss after Epoch 244: 0.2642616927623749\n",
            "Epoch 245/250, Batch 1/3, Loss: 0.03326021879911423\n",
            "Validation Loss after Epoch 245: 0.267549991607666\n",
            "Epoch 246/250, Batch 1/3, Loss: 0.027691993862390518\n",
            "Validation Loss after Epoch 246: 0.27029508352279663\n",
            "Epoch 247/250, Batch 1/3, Loss: 0.02594032511115074\n",
            "Validation Loss after Epoch 247: 0.2717517018318176\n",
            "Epoch 248/250, Batch 1/3, Loss: 0.023706138134002686\n",
            "Validation Loss after Epoch 248: 0.2738907039165497\n",
            "Epoch 249/250, Batch 1/3, Loss: 0.019773943349719048\n",
            "Validation Loss after Epoch 249: 0.2762394845485687\n",
            "Epoch 250/250, Batch 1/3, Loss: 0.027901561930775642\n",
            "Validation Loss after Epoch 250: 0.28002989292144775\n",
            "Subset size 100 - Test Loss: 0.3334, Test Accuracy: 91.82%, Average Dice Score: 0.9183\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on subset size: 250\n",
            "Epoch 1/250, Batch 1/6, Loss: 1.130179762840271\n",
            "Validation Loss after Epoch 1: 1.0731057524681091\n",
            "Epoch 2/250, Batch 1/6, Loss: 0.7414453625679016\n",
            "Validation Loss after Epoch 2: 1.0557710528373718\n",
            "Epoch 3/250, Batch 1/6, Loss: 0.6087232828140259\n",
            "Validation Loss after Epoch 3: 1.1927708983421326\n",
            "Epoch 4/250, Batch 1/6, Loss: 0.5238410234451294\n",
            "Validation Loss after Epoch 4: 1.3568835854530334\n",
            "Epoch 5/250, Batch 1/6, Loss: 0.44951722025871277\n",
            "Validation Loss after Epoch 5: 1.3714237213134766\n",
            "Epoch 6/250, Batch 1/6, Loss: 0.3672674298286438\n",
            "Validation Loss after Epoch 6: 1.3150306940078735\n",
            "Epoch 7/250, Batch 1/6, Loss: 0.31790387630462646\n",
            "Validation Loss after Epoch 7: 0.9434069693088531\n",
            "Epoch 8/250, Batch 1/6, Loss: 0.266347736120224\n",
            "Validation Loss after Epoch 8: 0.600145697593689\n",
            "Epoch 9/250, Batch 1/6, Loss: 0.23237264156341553\n",
            "Validation Loss after Epoch 9: 0.4297711104154587\n",
            "Epoch 10/250, Batch 1/6, Loss: 0.2080106884241104\n",
            "Validation Loss after Epoch 10: 0.2796582728624344\n",
            "Epoch 11/250, Batch 1/6, Loss: 0.18057188391685486\n",
            "Validation Loss after Epoch 11: 0.2151302844285965\n",
            "Epoch 12/250, Batch 1/6, Loss: 0.16523729264736176\n",
            "Validation Loss after Epoch 12: 0.1860959753394127\n",
            "Epoch 13/250, Batch 1/6, Loss: 0.14860154688358307\n",
            "Validation Loss after Epoch 13: 0.16354373842477798\n",
            "Epoch 14/250, Batch 1/6, Loss: 0.13428328931331635\n",
            "Validation Loss after Epoch 14: 0.15924547612667084\n",
            "Epoch 15/250, Batch 1/6, Loss: 0.1234934851527214\n",
            "Validation Loss after Epoch 15: 0.1634536162018776\n",
            "Epoch 16/250, Batch 1/6, Loss: 0.11852429062128067\n",
            "Validation Loss after Epoch 16: 0.15153958648443222\n",
            "Epoch 17/250, Batch 1/6, Loss: 0.12100536376237869\n",
            "Validation Loss after Epoch 17: 0.148059643805027\n",
            "Epoch 18/250, Batch 1/6, Loss: 0.10584218055009842\n",
            "Validation Loss after Epoch 18: 0.1604139283299446\n",
            "Epoch 19/250, Batch 1/6, Loss: 0.10661032795906067\n",
            "Validation Loss after Epoch 19: 0.15225952863693237\n",
            "Epoch 20/250, Batch 1/6, Loss: 0.09993715584278107\n",
            "Validation Loss after Epoch 20: 0.15281981229782104\n",
            "Epoch 21/250, Batch 1/6, Loss: 0.09854120016098022\n",
            "Validation Loss after Epoch 21: 0.1456046849489212\n",
            "Epoch 22/250, Batch 1/6, Loss: 0.07927597314119339\n",
            "Validation Loss after Epoch 22: 0.14593713730573654\n",
            "Epoch 23/250, Batch 1/6, Loss: 0.08559410274028778\n",
            "Validation Loss after Epoch 23: 0.139911450445652\n",
            "Epoch 24/250, Batch 1/6, Loss: 0.08752768486738205\n",
            "Validation Loss after Epoch 24: 0.14519022405147552\n",
            "Epoch 25/250, Batch 1/6, Loss: 0.08657082915306091\n",
            "Validation Loss after Epoch 25: 0.13367853686213493\n",
            "Epoch 26/250, Batch 1/6, Loss: 0.08651944994926453\n",
            "Validation Loss after Epoch 26: 0.14529849588871002\n",
            "Epoch 27/250, Batch 1/6, Loss: 0.0807301327586174\n",
            "Validation Loss after Epoch 27: 0.14118140935897827\n",
            "Epoch 28/250, Batch 1/6, Loss: 0.07493037730455399\n",
            "Validation Loss after Epoch 28: 0.14413801580667496\n",
            "Epoch 29/250, Batch 1/6, Loss: 0.06592468172311783\n",
            "Validation Loss after Epoch 29: 0.13804535567760468\n",
            "Epoch 30/250, Batch 1/6, Loss: 0.07642056047916412\n",
            "Validation Loss after Epoch 30: 0.13856331259012222\n",
            "Epoch 31/250, Batch 1/6, Loss: 0.06990540027618408\n",
            "Validation Loss after Epoch 31: 0.1355486586689949\n",
            "Epoch 32/250, Batch 1/6, Loss: 0.07107897102832794\n",
            "Validation Loss after Epoch 32: 0.13589078187942505\n",
            "Epoch 33/250, Batch 1/6, Loss: 0.06926055252552032\n",
            "Validation Loss after Epoch 33: 0.14129360765218735\n",
            "Epoch 34/250, Batch 1/6, Loss: 0.07465261220932007\n",
            "Validation Loss after Epoch 34: 0.1396297886967659\n",
            "Epoch 35/250, Batch 1/6, Loss: 0.06650115549564362\n",
            "Validation Loss after Epoch 35: 0.14552628248929977\n",
            "Epoch 36/250, Batch 1/6, Loss: 0.06503519415855408\n",
            "Validation Loss after Epoch 36: 0.13636291772127151\n",
            "Epoch 37/250, Batch 1/6, Loss: 0.06464120000600815\n",
            "Validation Loss after Epoch 37: 0.14062859863042831\n",
            "Epoch 38/250, Batch 1/6, Loss: 0.06337513029575348\n",
            "Validation Loss after Epoch 38: 0.13244254142045975\n",
            "Epoch 39/250, Batch 1/6, Loss: 0.05598410218954086\n",
            "Validation Loss after Epoch 39: 0.13591764122247696\n",
            "Epoch 40/250, Batch 1/6, Loss: 0.059295494109392166\n",
            "Validation Loss after Epoch 40: 0.1340126320719719\n",
            "Epoch 41/250, Batch 1/6, Loss: 0.05540145933628082\n",
            "Validation Loss after Epoch 41: 0.13761234283447266\n",
            "Epoch 42/250, Batch 1/6, Loss: 0.06250227242708206\n",
            "Validation Loss after Epoch 42: 0.14087484776973724\n",
            "Epoch 43/250, Batch 1/6, Loss: 0.05548867955803871\n",
            "Validation Loss after Epoch 43: 0.13101258873939514\n",
            "Epoch 44/250, Batch 1/6, Loss: 0.055078454315662384\n",
            "Validation Loss after Epoch 44: 0.13897737488150597\n",
            "Epoch 45/250, Batch 1/6, Loss: 0.05834892764687538\n",
            "Validation Loss after Epoch 45: 0.13496199250221252\n",
            "Epoch 46/250, Batch 1/6, Loss: 0.05632121488451958\n",
            "Validation Loss after Epoch 46: 0.13224563375115395\n",
            "Epoch 47/250, Batch 1/6, Loss: 0.053043730556964874\n",
            "Validation Loss after Epoch 47: 0.12974119558930397\n",
            "Epoch 48/250, Batch 1/6, Loss: 0.05716547742486\n",
            "Validation Loss after Epoch 48: 0.12965474277734756\n",
            "Epoch 49/250, Batch 1/6, Loss: 0.05460892245173454\n",
            "Validation Loss after Epoch 49: 0.1313198208808899\n",
            "Epoch 50/250, Batch 1/6, Loss: 0.04658123850822449\n",
            "Validation Loss after Epoch 50: 0.13106796517968178\n",
            "Epoch 51/250, Batch 1/6, Loss: 0.050674594938755035\n",
            "Validation Loss after Epoch 51: 0.1290844790637493\n",
            "Epoch 52/250, Batch 1/6, Loss: 0.05283327028155327\n",
            "Validation Loss after Epoch 52: 0.1363522931933403\n",
            "Epoch 53/250, Batch 1/6, Loss: 0.04576844349503517\n",
            "Validation Loss after Epoch 53: 0.14594876766204834\n",
            "Epoch 54/250, Batch 1/6, Loss: 0.049076858907938004\n",
            "Validation Loss after Epoch 54: 0.1441870778799057\n",
            "Epoch 55/250, Batch 1/6, Loss: 0.05443229898810387\n",
            "Validation Loss after Epoch 55: 0.14491764456033707\n",
            "Epoch 56/250, Batch 1/6, Loss: 0.04534786939620972\n",
            "Validation Loss after Epoch 56: 0.14673691987991333\n",
            "Epoch 57/250, Batch 1/6, Loss: 0.05676404386758804\n",
            "Validation Loss after Epoch 57: 0.13638358563184738\n",
            "Epoch 58/250, Batch 1/6, Loss: 0.04552236944437027\n",
            "Validation Loss after Epoch 58: 0.13770485669374466\n",
            "Epoch 59/250, Batch 1/6, Loss: 0.04571777954697609\n",
            "Validation Loss after Epoch 59: 0.14230232685804367\n",
            "Epoch 60/250, Batch 1/6, Loss: 0.04706570506095886\n",
            "Validation Loss after Epoch 60: 0.14471296966075897\n",
            "Epoch 61/250, Batch 1/6, Loss: 0.04814290255308151\n",
            "Validation Loss after Epoch 61: 0.14698725938796997\n",
            "Epoch 62/250, Batch 1/6, Loss: 0.04354801028966904\n",
            "Validation Loss after Epoch 62: 0.14774585515260696\n",
            "Epoch 63/250, Batch 1/6, Loss: 0.048951614648103714\n",
            "Validation Loss after Epoch 63: 0.14238489419221878\n",
            "Epoch 64/250, Batch 1/6, Loss: 0.04008955508470535\n",
            "Validation Loss after Epoch 64: 0.14496483653783798\n",
            "Epoch 65/250, Batch 1/6, Loss: 0.050382789224386215\n",
            "Validation Loss after Epoch 65: 0.1450004056096077\n",
            "Epoch 66/250, Batch 1/6, Loss: 0.04237090423703194\n",
            "Validation Loss after Epoch 66: 0.13775545358657837\n",
            "Epoch 67/250, Batch 1/6, Loss: 0.041041478514671326\n",
            "Validation Loss after Epoch 67: 0.14493026584386826\n",
            "Epoch 68/250, Batch 1/6, Loss: 0.05060872435569763\n",
            "Validation Loss after Epoch 68: 0.14990565925836563\n",
            "Epoch 69/250, Batch 1/6, Loss: 0.04408765956759453\n",
            "Validation Loss after Epoch 69: 0.14648327976465225\n",
            "Epoch 70/250, Batch 1/6, Loss: 0.0445537194609642\n",
            "Validation Loss after Epoch 70: 0.14763640612363815\n",
            "Epoch 71/250, Batch 1/6, Loss: 0.040552422404289246\n",
            "Validation Loss after Epoch 71: 0.14901213347911835\n",
            "Epoch 72/250, Batch 1/6, Loss: 0.042408693581819534\n",
            "Validation Loss after Epoch 72: 0.14116813987493515\n",
            "Epoch 73/250, Batch 1/6, Loss: 0.04136335477232933\n",
            "Validation Loss after Epoch 73: 0.13522759079933167\n",
            "Epoch 74/250, Batch 1/6, Loss: 0.04506248980760574\n",
            "Validation Loss after Epoch 74: 0.14716742187738419\n",
            "Epoch 75/250, Batch 1/6, Loss: 0.04249963536858559\n",
            "Validation Loss after Epoch 75: 0.14421964436769485\n",
            "Epoch 76/250, Batch 1/6, Loss: 0.04288949444890022\n",
            "Validation Loss after Epoch 76: 0.1358935907483101\n",
            "Epoch 77/250, Batch 1/6, Loss: 0.03709785267710686\n",
            "Validation Loss after Epoch 77: 0.13953372836112976\n",
            "Epoch 78/250, Batch 1/6, Loss: 0.03743985295295715\n",
            "Validation Loss after Epoch 78: 0.1452668085694313\n",
            "Epoch 79/250, Batch 1/6, Loss: 0.04298177361488342\n",
            "Validation Loss after Epoch 79: 0.14688903838396072\n",
            "Epoch 80/250, Batch 1/6, Loss: 0.03765890374779701\n",
            "Validation Loss after Epoch 80: 0.14700212329626083\n",
            "Epoch 81/250, Batch 1/6, Loss: 0.04071555659174919\n",
            "Validation Loss after Epoch 81: 0.14616405963897705\n",
            "Epoch 82/250, Batch 1/6, Loss: 0.044476404786109924\n",
            "Validation Loss after Epoch 82: 0.15019818395376205\n",
            "Epoch 83/250, Batch 1/6, Loss: 0.04046417772769928\n",
            "Validation Loss after Epoch 83: 0.15598255395889282\n",
            "Epoch 84/250, Batch 1/6, Loss: 0.04192933440208435\n",
            "Validation Loss after Epoch 84: 0.15088367462158203\n",
            "Epoch 85/250, Batch 1/6, Loss: 0.038691531866788864\n",
            "Validation Loss after Epoch 85: 0.14644062519073486\n",
            "Epoch 86/250, Batch 1/6, Loss: 0.03600716218352318\n",
            "Validation Loss after Epoch 86: 0.14935610443353653\n",
            "Epoch 87/250, Batch 1/6, Loss: 0.03713170811533928\n",
            "Validation Loss after Epoch 87: 0.15354007482528687\n",
            "Epoch 88/250, Batch 1/6, Loss: 0.036360710859298706\n",
            "Validation Loss after Epoch 88: 0.14709877222776413\n",
            "Epoch 89/250, Batch 1/6, Loss: 0.035324376076459885\n",
            "Validation Loss after Epoch 89: 0.1492036059498787\n",
            "Epoch 90/250, Batch 1/6, Loss: 0.03847306966781616\n",
            "Validation Loss after Epoch 90: 0.14984683692455292\n",
            "Epoch 91/250, Batch 1/6, Loss: 0.034627191722393036\n",
            "Validation Loss after Epoch 91: 0.150586798787117\n",
            "Epoch 92/250, Batch 1/6, Loss: 0.03909045457839966\n",
            "Validation Loss after Epoch 92: 0.15155735611915588\n",
            "Epoch 93/250, Batch 1/6, Loss: 0.03625389188528061\n",
            "Validation Loss after Epoch 93: 0.1524159163236618\n",
            "Epoch 94/250, Batch 1/6, Loss: 0.03736371546983719\n",
            "Validation Loss after Epoch 94: 0.1529751718044281\n",
            "Epoch 95/250, Batch 1/6, Loss: 0.029847953468561172\n",
            "Validation Loss after Epoch 95: 0.14927886426448822\n",
            "Epoch 96/250, Batch 1/6, Loss: 0.03342114016413689\n",
            "Validation Loss after Epoch 96: 0.15410111099481583\n",
            "Epoch 97/250, Batch 1/6, Loss: 0.031163759529590607\n",
            "Validation Loss after Epoch 97: 0.14763538539409637\n",
            "Epoch 98/250, Batch 1/6, Loss: 0.039083924144506454\n",
            "Validation Loss after Epoch 98: 0.14559490233659744\n",
            "Epoch 99/250, Batch 1/6, Loss: 0.035274092108011246\n",
            "Validation Loss after Epoch 99: 0.14242009073495865\n",
            "Epoch 100/250, Batch 1/6, Loss: 0.03142274171113968\n",
            "Validation Loss after Epoch 100: 0.14575538039207458\n",
            "Epoch 101/250, Batch 1/6, Loss: 0.030962228775024414\n",
            "Validation Loss after Epoch 101: 0.1562444046139717\n",
            "Epoch 102/250, Batch 1/6, Loss: 0.03342311456799507\n",
            "Validation Loss after Epoch 102: 0.14736613631248474\n",
            "Epoch 103/250, Batch 1/6, Loss: 0.035605546087026596\n",
            "Validation Loss after Epoch 103: 0.14803844690322876\n",
            "Epoch 104/250, Batch 1/6, Loss: 0.03855574131011963\n",
            "Validation Loss after Epoch 104: 0.14890533685684204\n",
            "Epoch 105/250, Batch 1/6, Loss: 0.03450633957982063\n",
            "Validation Loss after Epoch 105: 0.14613954722881317\n",
            "Epoch 106/250, Batch 1/6, Loss: 0.033239249140024185\n",
            "Validation Loss after Epoch 106: 0.140316940844059\n",
            "Epoch 107/250, Batch 1/6, Loss: 0.03518787771463394\n",
            "Validation Loss after Epoch 107: 0.14118054509162903\n",
            "Epoch 108/250, Batch 1/6, Loss: 0.03405649587512016\n",
            "Validation Loss after Epoch 108: 0.14309222251176834\n",
            "Epoch 109/250, Batch 1/6, Loss: 0.03195353224873543\n",
            "Validation Loss after Epoch 109: 0.14359863847494125\n",
            "Epoch 110/250, Batch 1/6, Loss: 0.03508404269814491\n",
            "Validation Loss after Epoch 110: 0.14303554594516754\n",
            "Epoch 111/250, Batch 1/6, Loss: 0.03333117812871933\n",
            "Validation Loss after Epoch 111: 0.1485612839460373\n",
            "Epoch 112/250, Batch 1/6, Loss: 0.033221837133169174\n",
            "Validation Loss after Epoch 112: 0.1486280858516693\n",
            "Epoch 113/250, Batch 1/6, Loss: 0.036095310002565384\n",
            "Validation Loss after Epoch 113: 0.14403152465820312\n",
            "Epoch 114/250, Batch 1/6, Loss: 0.02836102806031704\n",
            "Validation Loss after Epoch 114: 0.13964363932609558\n",
            "Epoch 115/250, Batch 1/6, Loss: 0.027534199878573418\n",
            "Validation Loss after Epoch 115: 0.14063230156898499\n",
            "Epoch 116/250, Batch 1/6, Loss: 0.031439945101737976\n",
            "Validation Loss after Epoch 116: 0.14261745661497116\n",
            "Epoch 117/250, Batch 1/6, Loss: 0.02852769009768963\n",
            "Validation Loss after Epoch 117: 0.15268287807703018\n",
            "Epoch 118/250, Batch 1/6, Loss: 0.03409518674015999\n",
            "Validation Loss after Epoch 118: 0.15420060604810715\n",
            "Epoch 119/250, Batch 1/6, Loss: 0.029078409075737\n",
            "Validation Loss after Epoch 119: 0.15364360809326172\n",
            "Epoch 120/250, Batch 1/6, Loss: 0.02637195773422718\n",
            "Validation Loss after Epoch 120: 0.14988228678703308\n",
            "Epoch 121/250, Batch 1/6, Loss: 0.02699431963264942\n",
            "Validation Loss after Epoch 121: 0.1560460552573204\n",
            "Epoch 122/250, Batch 1/6, Loss: 0.030359720811247826\n",
            "Validation Loss after Epoch 122: 0.15995965898036957\n",
            "Epoch 123/250, Batch 1/6, Loss: 0.03146142512559891\n",
            "Validation Loss after Epoch 123: 0.1565164476633072\n",
            "Epoch 124/250, Batch 1/6, Loss: 0.032937705516815186\n",
            "Validation Loss after Epoch 124: 0.1541750133037567\n",
            "Epoch 125/250, Batch 1/6, Loss: 0.028983239084482193\n",
            "Validation Loss after Epoch 125: 0.14096616953611374\n",
            "Epoch 126/250, Batch 1/6, Loss: 0.025751734152436256\n",
            "Validation Loss after Epoch 126: 0.14148227870464325\n",
            "Epoch 127/250, Batch 1/6, Loss: 0.02995159663259983\n",
            "Validation Loss after Epoch 127: 0.151947021484375\n",
            "Epoch 128/250, Batch 1/6, Loss: 0.025284502655267715\n",
            "Validation Loss after Epoch 128: 0.15267015993595123\n",
            "Epoch 129/250, Batch 1/6, Loss: 0.029239553958177567\n",
            "Validation Loss after Epoch 129: 0.1505345031619072\n",
            "Epoch 130/250, Batch 1/6, Loss: 0.027962589636445045\n",
            "Validation Loss after Epoch 130: 0.15517333894968033\n",
            "Epoch 131/250, Batch 1/6, Loss: 0.028420019894838333\n",
            "Validation Loss after Epoch 131: 0.16007312387228012\n",
            "Epoch 132/250, Batch 1/6, Loss: 0.02800341695547104\n",
            "Validation Loss after Epoch 132: 0.1520954743027687\n",
            "Epoch 133/250, Batch 1/6, Loss: 0.027075879275798798\n",
            "Validation Loss after Epoch 133: 0.1471797078847885\n",
            "Epoch 134/250, Batch 1/6, Loss: 0.030676785856485367\n",
            "Validation Loss after Epoch 134: 0.14967456459999084\n",
            "Epoch 135/250, Batch 1/6, Loss: 0.029182180762290955\n",
            "Validation Loss after Epoch 135: 0.1470281481742859\n",
            "Epoch 136/250, Batch 1/6, Loss: 0.023940829560160637\n",
            "Validation Loss after Epoch 136: 0.1492050141096115\n",
            "Epoch 137/250, Batch 1/6, Loss: 0.028528084978461266\n",
            "Validation Loss after Epoch 137: 0.1446557715535164\n",
            "Epoch 138/250, Batch 1/6, Loss: 0.035383451730012894\n",
            "Validation Loss after Epoch 138: 0.14510823041200638\n",
            "Epoch 139/250, Batch 1/6, Loss: 0.025120407342910767\n",
            "Validation Loss after Epoch 139: 0.15143562108278275\n",
            "Epoch 140/250, Batch 1/6, Loss: 0.03436920791864395\n",
            "Validation Loss after Epoch 140: 0.15089598298072815\n",
            "Epoch 141/250, Batch 1/6, Loss: 0.030552159994840622\n",
            "Validation Loss after Epoch 141: 0.14378943294286728\n",
            "Epoch 142/250, Batch 1/6, Loss: 0.02822636067867279\n",
            "Validation Loss after Epoch 142: 0.14435628056526184\n",
            "Epoch 143/250, Batch 1/6, Loss: 0.0289189200848341\n",
            "Validation Loss after Epoch 143: 0.1466120481491089\n",
            "Epoch 144/250, Batch 1/6, Loss: 0.03202041983604431\n",
            "Validation Loss after Epoch 144: 0.15083309262990952\n",
            "Epoch 145/250, Batch 1/6, Loss: 0.022599827498197556\n",
            "Validation Loss after Epoch 145: 0.1560714691877365\n",
            "Epoch 146/250, Batch 1/6, Loss: 0.027883606031537056\n",
            "Validation Loss after Epoch 146: 0.1532265618443489\n",
            "Epoch 147/250, Batch 1/6, Loss: 0.022010907530784607\n",
            "Validation Loss after Epoch 147: 0.15008331090211868\n",
            "Epoch 148/250, Batch 1/6, Loss: 0.026183541864156723\n",
            "Validation Loss after Epoch 148: 0.15368566662073135\n",
            "Epoch 149/250, Batch 1/6, Loss: 0.026480581611394882\n",
            "Validation Loss after Epoch 149: 0.1517259180545807\n",
            "Epoch 150/250, Batch 1/6, Loss: 0.031909964978694916\n",
            "Validation Loss after Epoch 150: 0.14676283299922943\n",
            "Epoch 151/250, Batch 1/6, Loss: 0.024262798950076103\n",
            "Validation Loss after Epoch 151: 0.146925188601017\n",
            "Epoch 152/250, Batch 1/6, Loss: 0.02594422921538353\n",
            "Validation Loss after Epoch 152: 0.15243129432201385\n",
            "Epoch 153/250, Batch 1/6, Loss: 0.028546791523694992\n",
            "Validation Loss after Epoch 153: 0.15852289646863937\n",
            "Epoch 154/250, Batch 1/6, Loss: 0.022533848881721497\n",
            "Validation Loss after Epoch 154: 0.16269075125455856\n",
            "Epoch 155/250, Batch 1/6, Loss: 0.024349341168999672\n",
            "Validation Loss after Epoch 155: 0.15766595304012299\n",
            "Epoch 156/250, Batch 1/6, Loss: 0.030416369438171387\n",
            "Validation Loss after Epoch 156: 0.16202089190483093\n",
            "Epoch 157/250, Batch 1/6, Loss: 0.024938354268670082\n",
            "Validation Loss after Epoch 157: 0.15502887964248657\n",
            "Epoch 158/250, Batch 1/6, Loss: 0.029937390238046646\n",
            "Validation Loss after Epoch 158: 0.14728151261806488\n",
            "Epoch 159/250, Batch 1/6, Loss: 0.02418331429362297\n",
            "Validation Loss after Epoch 159: 0.14907319098711014\n",
            "Epoch 160/250, Batch 1/6, Loss: 0.023365506902337074\n",
            "Validation Loss after Epoch 160: 0.1560724377632141\n",
            "Epoch 161/250, Batch 1/6, Loss: 0.02555900439620018\n",
            "Validation Loss after Epoch 161: 0.15811729431152344\n",
            "Epoch 162/250, Batch 1/6, Loss: 0.027245203033089638\n",
            "Validation Loss after Epoch 162: 0.1572086587548256\n",
            "Epoch 163/250, Batch 1/6, Loss: 0.02791827730834484\n",
            "Validation Loss after Epoch 163: 0.16038640588521957\n",
            "Epoch 164/250, Batch 1/6, Loss: 0.022339239716529846\n",
            "Validation Loss after Epoch 164: 0.16273406893014908\n",
            "Epoch 165/250, Batch 1/6, Loss: 0.02561858668923378\n",
            "Validation Loss after Epoch 165: 0.15357383340597153\n",
            "Epoch 166/250, Batch 1/6, Loss: 0.023914668709039688\n",
            "Validation Loss after Epoch 166: 0.1587090641260147\n",
            "Epoch 167/250, Batch 1/6, Loss: 0.024501141160726547\n",
            "Validation Loss after Epoch 167: 0.157723069190979\n",
            "Epoch 168/250, Batch 1/6, Loss: 0.028459109365940094\n",
            "Validation Loss after Epoch 168: 0.15102007240056992\n",
            "Epoch 169/250, Batch 1/6, Loss: 0.02565128169953823\n",
            "Validation Loss after Epoch 169: 0.15169477462768555\n",
            "Epoch 170/250, Batch 1/6, Loss: 0.024038422852754593\n",
            "Validation Loss after Epoch 170: 0.1606844887137413\n",
            "Epoch 171/250, Batch 1/6, Loss: 0.02398810349404812\n",
            "Validation Loss after Epoch 171: 0.16514703631401062\n",
            "Epoch 172/250, Batch 1/6, Loss: 0.024325449019670486\n",
            "Validation Loss after Epoch 172: 0.15563317388296127\n",
            "Epoch 173/250, Batch 1/6, Loss: 0.02303691953420639\n",
            "Validation Loss after Epoch 173: 0.1548565924167633\n",
            "Epoch 174/250, Batch 1/6, Loss: 0.02205253578722477\n",
            "Validation Loss after Epoch 174: 0.15666086971759796\n",
            "Epoch 175/250, Batch 1/6, Loss: 0.02271880768239498\n",
            "Validation Loss after Epoch 175: 0.1615772843360901\n",
            "Epoch 176/250, Batch 1/6, Loss: 0.0236788559705019\n",
            "Validation Loss after Epoch 176: 0.16800835728645325\n",
            "Epoch 177/250, Batch 1/6, Loss: 0.027822066098451614\n",
            "Validation Loss after Epoch 177: 0.16717365384101868\n",
            "Epoch 178/250, Batch 1/6, Loss: 0.022427547723054886\n",
            "Validation Loss after Epoch 178: 0.15388885140419006\n",
            "Epoch 179/250, Batch 1/6, Loss: 0.021626368165016174\n",
            "Validation Loss after Epoch 179: 0.15381381660699844\n",
            "Epoch 180/250, Batch 1/6, Loss: 0.02351393550634384\n",
            "Validation Loss after Epoch 180: 0.15189100801944733\n",
            "Epoch 181/250, Batch 1/6, Loss: 0.026357479393482208\n",
            "Validation Loss after Epoch 181: 0.1565055102109909\n",
            "Epoch 182/250, Batch 1/6, Loss: 0.02127940207719803\n",
            "Validation Loss after Epoch 182: 0.16163858771324158\n",
            "Epoch 183/250, Batch 1/6, Loss: 0.02688632160425186\n",
            "Validation Loss after Epoch 183: 0.16536126285791397\n",
            "Epoch 184/250, Batch 1/6, Loss: 0.02233920246362686\n",
            "Validation Loss after Epoch 184: 0.17139584571123123\n",
            "Epoch 185/250, Batch 1/6, Loss: 0.025040671229362488\n",
            "Validation Loss after Epoch 185: 0.16877812892198563\n",
            "Epoch 186/250, Batch 1/6, Loss: 0.02453611046075821\n",
            "Validation Loss after Epoch 186: 0.15431811660528183\n",
            "Epoch 187/250, Batch 1/6, Loss: 0.027023714035749435\n",
            "Validation Loss after Epoch 187: 0.15774372965097427\n",
            "Epoch 188/250, Batch 1/6, Loss: 0.029126588255167007\n",
            "Validation Loss after Epoch 188: 0.16448797285556793\n",
            "Epoch 189/250, Batch 1/6, Loss: 0.026198411360383034\n",
            "Validation Loss after Epoch 189: 0.15822676569223404\n",
            "Epoch 190/250, Batch 1/6, Loss: 0.020411482080817223\n",
            "Validation Loss after Epoch 190: 0.15745265781879425\n",
            "Epoch 191/250, Batch 1/6, Loss: 0.023947209119796753\n",
            "Validation Loss after Epoch 191: 0.16371694952249527\n",
            "Epoch 192/250, Batch 1/6, Loss: 0.019743310287594795\n",
            "Validation Loss after Epoch 192: 0.16627245396375656\n",
            "Epoch 193/250, Batch 1/6, Loss: 0.022295624017715454\n",
            "Validation Loss after Epoch 193: 0.16397415846586227\n",
            "Epoch 194/250, Batch 1/6, Loss: 0.020439276471734047\n",
            "Validation Loss after Epoch 194: 0.1618424654006958\n",
            "Epoch 195/250, Batch 1/6, Loss: 0.026729319244623184\n",
            "Validation Loss after Epoch 195: 0.16041779518127441\n",
            "Epoch 196/250, Batch 1/6, Loss: 0.02028249390423298\n",
            "Validation Loss after Epoch 196: 0.15631936490535736\n",
            "Epoch 197/250, Batch 1/6, Loss: 0.0197603777050972\n",
            "Validation Loss after Epoch 197: 0.15755325555801392\n",
            "Epoch 198/250, Batch 1/6, Loss: 0.021304110065102577\n",
            "Validation Loss after Epoch 198: 0.16335584968328476\n",
            "Epoch 199/250, Batch 1/6, Loss: 0.022517090663313866\n",
            "Validation Loss after Epoch 199: 0.16329040378332138\n",
            "Epoch 200/250, Batch 1/6, Loss: 0.020193666219711304\n",
            "Validation Loss after Epoch 200: 0.164895698428154\n",
            "Epoch 201/250, Batch 1/6, Loss: 0.025889871641993523\n",
            "Validation Loss after Epoch 201: 0.1642797887325287\n",
            "Epoch 202/250, Batch 1/6, Loss: 0.02060840092599392\n",
            "Validation Loss after Epoch 202: 0.16020137071609497\n",
            "Epoch 203/250, Batch 1/6, Loss: 0.02450811304152012\n",
            "Validation Loss after Epoch 203: 0.15593321621418\n",
            "Epoch 204/250, Batch 1/6, Loss: 0.02624545432627201\n",
            "Validation Loss after Epoch 204: 0.15749799460172653\n",
            "Epoch 205/250, Batch 1/6, Loss: 0.019043315201997757\n",
            "Validation Loss after Epoch 205: 0.16446126252412796\n",
            "Epoch 206/250, Batch 1/6, Loss: 0.019754059612751007\n",
            "Validation Loss after Epoch 206: 0.16034313291311264\n",
            "Epoch 207/250, Batch 1/6, Loss: 0.02306428924202919\n",
            "Validation Loss after Epoch 207: 0.15566423535346985\n",
            "Epoch 208/250, Batch 1/6, Loss: 0.017109239473938942\n",
            "Validation Loss after Epoch 208: 0.16136061400175095\n",
            "Epoch 209/250, Batch 1/6, Loss: 0.018669839948415756\n",
            "Validation Loss after Epoch 209: 0.16800416260957718\n",
            "Epoch 210/250, Batch 1/6, Loss: 0.020331891253590584\n",
            "Validation Loss after Epoch 210: 0.1755402460694313\n",
            "Epoch 211/250, Batch 1/6, Loss: 0.02471235580742359\n",
            "Validation Loss after Epoch 211: 0.1735278144478798\n",
            "Epoch 212/250, Batch 1/6, Loss: 0.02050652727484703\n",
            "Validation Loss after Epoch 212: 0.16811271756887436\n",
            "Epoch 213/250, Batch 1/6, Loss: 0.02325013093650341\n",
            "Validation Loss after Epoch 213: 0.16443800181150436\n",
            "Epoch 214/250, Batch 1/6, Loss: 0.01954665407538414\n",
            "Validation Loss after Epoch 214: 0.16225602477788925\n",
            "Epoch 215/250, Batch 1/6, Loss: 0.02272753231227398\n",
            "Validation Loss after Epoch 215: 0.16094737499952316\n",
            "Epoch 216/250, Batch 1/6, Loss: 0.01643458753824234\n",
            "Validation Loss after Epoch 216: 0.16135628521442413\n",
            "Epoch 217/250, Batch 1/6, Loss: 0.0225382037460804\n",
            "Validation Loss after Epoch 217: 0.16464289277791977\n",
            "Epoch 218/250, Batch 1/6, Loss: 0.025200804695487022\n",
            "Validation Loss after Epoch 218: 0.17291201651096344\n",
            "Epoch 219/250, Batch 1/6, Loss: 0.021516799926757812\n",
            "Validation Loss after Epoch 219: 0.16707152128219604\n",
            "Epoch 220/250, Batch 1/6, Loss: 0.031557776033878326\n",
            "Validation Loss after Epoch 220: 0.16750572621822357\n",
            "Epoch 221/250, Batch 1/6, Loss: 0.02092333883047104\n",
            "Validation Loss after Epoch 221: 0.16333502531051636\n",
            "Epoch 222/250, Batch 1/6, Loss: 0.0214462298899889\n",
            "Validation Loss after Epoch 222: 0.15902268886566162\n",
            "Epoch 223/250, Batch 1/6, Loss: 0.023221289739012718\n",
            "Validation Loss after Epoch 223: 0.16632988303899765\n",
            "Epoch 224/250, Batch 1/6, Loss: 0.021980229765176773\n",
            "Validation Loss after Epoch 224: 0.1708616092801094\n",
            "Epoch 225/250, Batch 1/6, Loss: 0.019800612702965736\n",
            "Validation Loss after Epoch 225: 0.165429949760437\n",
            "Epoch 226/250, Batch 1/6, Loss: 0.019553761929273605\n",
            "Validation Loss after Epoch 226: 0.16802771389484406\n",
            "Epoch 227/250, Batch 1/6, Loss: 0.021003885194659233\n",
            "Validation Loss after Epoch 227: 0.16850586235523224\n",
            "Epoch 228/250, Batch 1/6, Loss: 0.021097155287861824\n",
            "Validation Loss after Epoch 228: 0.16978764533996582\n",
            "Epoch 229/250, Batch 1/6, Loss: 0.018731366842985153\n",
            "Validation Loss after Epoch 229: 0.16359903663396835\n",
            "Epoch 230/250, Batch 1/6, Loss: 0.021795904263854027\n",
            "Validation Loss after Epoch 230: 0.16944245994091034\n",
            "Epoch 231/250, Batch 1/6, Loss: 0.019887235015630722\n",
            "Validation Loss after Epoch 231: 0.1717657595872879\n",
            "Epoch 232/250, Batch 1/6, Loss: 0.01576976478099823\n",
            "Validation Loss after Epoch 232: 0.17648717015981674\n",
            "Epoch 233/250, Batch 1/6, Loss: 0.0195364598184824\n",
            "Validation Loss after Epoch 233: 0.17322158813476562\n",
            "Epoch 234/250, Batch 1/6, Loss: 0.02324085868895054\n",
            "Validation Loss after Epoch 234: 0.1682816594839096\n",
            "Epoch 235/250, Batch 1/6, Loss: 0.01581074483692646\n",
            "Validation Loss after Epoch 235: 0.17043738812208176\n",
            "Epoch 236/250, Batch 1/6, Loss: 0.02320467308163643\n",
            "Validation Loss after Epoch 236: 0.17185969650745392\n",
            "Epoch 237/250, Batch 1/6, Loss: 0.022397326305508614\n",
            "Validation Loss after Epoch 237: 0.17756862938404083\n",
            "Epoch 238/250, Batch 1/6, Loss: 0.020368702709674835\n",
            "Validation Loss after Epoch 238: 0.1712050437927246\n",
            "Epoch 239/250, Batch 1/6, Loss: 0.019882289692759514\n",
            "Validation Loss after Epoch 239: 0.17382271587848663\n",
            "Epoch 240/250, Batch 1/6, Loss: 0.020537864416837692\n",
            "Validation Loss after Epoch 240: 0.1749763935804367\n",
            "Epoch 241/250, Batch 1/6, Loss: 0.01839514821767807\n",
            "Validation Loss after Epoch 241: 0.17465971410274506\n",
            "Epoch 242/250, Batch 1/6, Loss: 0.019012486562132835\n",
            "Validation Loss after Epoch 242: 0.17461919784545898\n",
            "Epoch 243/250, Batch 1/6, Loss: 0.019537560641765594\n",
            "Validation Loss after Epoch 243: 0.17200016975402832\n",
            "Epoch 244/250, Batch 1/6, Loss: 0.016682066023349762\n",
            "Validation Loss after Epoch 244: 0.1763729825615883\n",
            "Epoch 245/250, Batch 1/6, Loss: 0.019637981429696083\n",
            "Validation Loss after Epoch 245: 0.1813710629940033\n",
            "Epoch 246/250, Batch 1/6, Loss: 0.020299598574638367\n",
            "Validation Loss after Epoch 246: 0.18373601138591766\n",
            "Epoch 247/250, Batch 1/6, Loss: 0.01903127320110798\n",
            "Validation Loss after Epoch 247: 0.17853770405054092\n",
            "Epoch 248/250, Batch 1/6, Loss: 0.017966289073228836\n",
            "Validation Loss after Epoch 248: 0.1742611527442932\n",
            "Epoch 249/250, Batch 1/6, Loss: 0.017168758437037468\n",
            "Validation Loss after Epoch 249: 0.1792043223977089\n",
            "Epoch 250/250, Batch 1/6, Loss: 0.021139655262231827\n",
            "Validation Loss after Epoch 250: 0.17749136686325073\n",
            "Subset size 250 - Test Loss: 0.2299, Test Accuracy: 94.96%, Average Dice Score: 0.9464\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on subset size: 500\n",
            "Epoch 1/250, Batch 1/11, Loss: 1.1715457439422607\n",
            "Epoch 1/250, Batch 11/11, Loss: 0.6679294109344482\n",
            "Validation Loss after Epoch 1: 1.0557257334391277\n",
            "Epoch 2/250, Batch 1/11, Loss: 0.6519837975502014\n",
            "Epoch 2/250, Batch 11/11, Loss: 0.5108425617218018\n",
            "Validation Loss after Epoch 2: 1.2789671818415325\n",
            "Epoch 3/250, Batch 1/11, Loss: 0.48735538125038147\n",
            "Epoch 3/250, Batch 11/11, Loss: 0.399983286857605\n",
            "Validation Loss after Epoch 3: 1.2876672347386677\n",
            "Epoch 4/250, Batch 1/11, Loss: 0.3746614456176758\n",
            "Epoch 4/250, Batch 11/11, Loss: 0.2903432548046112\n",
            "Validation Loss after Epoch 4: 0.9286861419677734\n",
            "Epoch 5/250, Batch 1/11, Loss: 0.26852625608444214\n",
            "Epoch 5/250, Batch 11/11, Loss: 0.2298412322998047\n",
            "Validation Loss after Epoch 5: 0.5281694928805033\n",
            "Epoch 6/250, Batch 1/11, Loss: 0.2115282267332077\n",
            "Epoch 6/250, Batch 11/11, Loss: 0.19963009655475616\n",
            "Validation Loss after Epoch 6: 0.2377903958161672\n",
            "Epoch 7/250, Batch 1/11, Loss: 0.18315471708774567\n",
            "Epoch 7/250, Batch 11/11, Loss: 0.16952811181545258\n",
            "Validation Loss after Epoch 7: 0.1522402216990789\n",
            "Epoch 8/250, Batch 1/11, Loss: 0.15452803671360016\n",
            "Epoch 8/250, Batch 11/11, Loss: 0.15266916155815125\n",
            "Validation Loss after Epoch 8: 0.1374771644671758\n",
            "Epoch 9/250, Batch 1/11, Loss: 0.13912686705589294\n",
            "Epoch 9/250, Batch 11/11, Loss: 0.13367781043052673\n",
            "Validation Loss after Epoch 9: 0.11695653696854909\n",
            "Epoch 10/250, Batch 1/11, Loss: 0.126430943608284\n",
            "Epoch 10/250, Batch 11/11, Loss: 0.12048281729221344\n",
            "Validation Loss after Epoch 10: 0.114187387128671\n",
            "Epoch 11/250, Batch 1/11, Loss: 0.113459512591362\n",
            "Epoch 11/250, Batch 11/11, Loss: 0.1136813685297966\n",
            "Validation Loss after Epoch 11: 0.09645977119604747\n",
            "Epoch 12/250, Batch 1/11, Loss: 0.11094780266284943\n",
            "Epoch 12/250, Batch 11/11, Loss: 0.12288828939199448\n",
            "Validation Loss after Epoch 12: 0.09539678941170375\n",
            "Epoch 13/250, Batch 1/11, Loss: 0.1140536442399025\n",
            "Epoch 13/250, Batch 11/11, Loss: 0.12467136979103088\n",
            "Validation Loss after Epoch 13: 0.08949624747037888\n",
            "Epoch 14/250, Batch 1/11, Loss: 0.09983887523412704\n",
            "Epoch 14/250, Batch 11/11, Loss: 0.11055416613817215\n",
            "Validation Loss after Epoch 14: 0.08851734797159831\n",
            "Epoch 15/250, Batch 1/11, Loss: 0.1023569107055664\n",
            "Epoch 15/250, Batch 11/11, Loss: 0.097882941365242\n",
            "Validation Loss after Epoch 15: 0.08744356284538905\n",
            "Epoch 16/250, Batch 1/11, Loss: 0.08392314612865448\n",
            "Epoch 16/250, Batch 11/11, Loss: 0.10711102187633514\n",
            "Validation Loss after Epoch 16: 0.0872772956887881\n",
            "Epoch 17/250, Batch 1/11, Loss: 0.09741111844778061\n",
            "Epoch 17/250, Batch 11/11, Loss: 0.10341296344995499\n",
            "Validation Loss after Epoch 17: 0.0828537791967392\n",
            "Epoch 18/250, Batch 1/11, Loss: 0.08585070818662643\n",
            "Epoch 18/250, Batch 11/11, Loss: 0.09324601292610168\n",
            "Validation Loss after Epoch 18: 0.08655220518509547\n",
            "Epoch 19/250, Batch 1/11, Loss: 0.07852361351251602\n",
            "Epoch 19/250, Batch 11/11, Loss: 0.09522902220487595\n",
            "Validation Loss after Epoch 19: 0.07921910534302394\n",
            "Epoch 20/250, Batch 1/11, Loss: 0.08918345719575882\n",
            "Epoch 20/250, Batch 11/11, Loss: 0.09164109081029892\n",
            "Validation Loss after Epoch 20: 0.07909454156955083\n",
            "Epoch 21/250, Batch 1/11, Loss: 0.08042892068624496\n",
            "Epoch 21/250, Batch 11/11, Loss: 0.08883693814277649\n",
            "Validation Loss after Epoch 21: 0.07467472553253174\n",
            "Epoch 22/250, Batch 1/11, Loss: 0.07675349712371826\n",
            "Epoch 22/250, Batch 11/11, Loss: 0.08413618057966232\n",
            "Validation Loss after Epoch 22: 0.07387953996658325\n",
            "Epoch 23/250, Batch 1/11, Loss: 0.07581952214241028\n",
            "Epoch 23/250, Batch 11/11, Loss: 0.08348226547241211\n",
            "Validation Loss after Epoch 23: 0.06916720916827519\n",
            "Epoch 24/250, Batch 1/11, Loss: 0.06822947412729263\n",
            "Epoch 24/250, Batch 11/11, Loss: 0.08590138703584671\n",
            "Validation Loss after Epoch 24: 0.07364656776189804\n",
            "Epoch 25/250, Batch 1/11, Loss: 0.07074689120054245\n",
            "Epoch 25/250, Batch 11/11, Loss: 0.07820703834295273\n",
            "Validation Loss after Epoch 25: 0.07306480904420216\n",
            "Epoch 26/250, Batch 1/11, Loss: 0.07672935724258423\n",
            "Epoch 26/250, Batch 11/11, Loss: 0.07954816520214081\n",
            "Validation Loss after Epoch 26: 0.07112430036067963\n",
            "Epoch 27/250, Batch 1/11, Loss: 0.06950633227825165\n",
            "Epoch 27/250, Batch 11/11, Loss: 0.07829426229000092\n",
            "Validation Loss after Epoch 27: 0.07056551426649094\n",
            "Epoch 28/250, Batch 1/11, Loss: 0.06959402561187744\n",
            "Epoch 28/250, Batch 11/11, Loss: 0.08271873742341995\n",
            "Validation Loss after Epoch 28: 0.07244313508272171\n",
            "Epoch 29/250, Batch 1/11, Loss: 0.06486041098833084\n",
            "Epoch 29/250, Batch 11/11, Loss: 0.08345364779233932\n",
            "Validation Loss after Epoch 29: 0.07169321924448013\n",
            "Epoch 30/250, Batch 1/11, Loss: 0.06843448430299759\n",
            "Epoch 30/250, Batch 11/11, Loss: 0.07456997036933899\n",
            "Validation Loss after Epoch 30: 0.06969506790240605\n",
            "Epoch 31/250, Batch 1/11, Loss: 0.067607581615448\n",
            "Epoch 31/250, Batch 11/11, Loss: 0.06902673095464706\n",
            "Validation Loss after Epoch 31: 0.0712078536550204\n",
            "Epoch 32/250, Batch 1/11, Loss: 0.06683807820081711\n",
            "Epoch 32/250, Batch 11/11, Loss: 0.07816223800182343\n",
            "Validation Loss after Epoch 32: 0.07151644428571065\n",
            "Epoch 33/250, Batch 1/11, Loss: 0.06101939082145691\n",
            "Epoch 33/250, Batch 11/11, Loss: 0.07476601749658585\n",
            "Validation Loss after Epoch 33: 0.06804048269987106\n",
            "Epoch 34/250, Batch 1/11, Loss: 0.0639675185084343\n",
            "Epoch 34/250, Batch 11/11, Loss: 0.0681317076086998\n",
            "Validation Loss after Epoch 34: 0.0674113854765892\n",
            "Epoch 35/250, Batch 1/11, Loss: 0.06636471301317215\n",
            "Epoch 35/250, Batch 11/11, Loss: 0.07269718497991562\n",
            "Validation Loss after Epoch 35: 0.06529394785563152\n",
            "Epoch 36/250, Batch 1/11, Loss: 0.06225908547639847\n",
            "Epoch 36/250, Batch 11/11, Loss: 0.07200753688812256\n",
            "Validation Loss after Epoch 36: 0.06983936826388042\n",
            "Epoch 37/250, Batch 1/11, Loss: 0.05837699770927429\n",
            "Epoch 37/250, Batch 11/11, Loss: 0.06483352929353714\n",
            "Validation Loss after Epoch 37: 0.06645844752589862\n",
            "Epoch 38/250, Batch 1/11, Loss: 0.056221090257167816\n",
            "Epoch 38/250, Batch 11/11, Loss: 0.06773054599761963\n",
            "Validation Loss after Epoch 38: 0.06373803317546844\n",
            "Epoch 39/250, Batch 1/11, Loss: 0.05464678257703781\n",
            "Epoch 39/250, Batch 11/11, Loss: 0.06578861176967621\n",
            "Validation Loss after Epoch 39: 0.06550052265326183\n",
            "Epoch 40/250, Batch 1/11, Loss: 0.05358767509460449\n",
            "Epoch 40/250, Batch 11/11, Loss: 0.06715261936187744\n",
            "Validation Loss after Epoch 40: 0.06765621528029442\n",
            "Epoch 41/250, Batch 1/11, Loss: 0.05268772318959236\n",
            "Epoch 41/250, Batch 11/11, Loss: 0.06723468750715256\n",
            "Validation Loss after Epoch 41: 0.06704656531413396\n",
            "Epoch 42/250, Batch 1/11, Loss: 0.05672704800963402\n",
            "Epoch 42/250, Batch 11/11, Loss: 0.07110477238893509\n",
            "Validation Loss after Epoch 42: 0.06590080261230469\n",
            "Epoch 43/250, Batch 1/11, Loss: 0.059359438717365265\n",
            "Epoch 43/250, Batch 11/11, Loss: 0.06589427590370178\n",
            "Validation Loss after Epoch 43: 0.06411571676532428\n",
            "Epoch 44/250, Batch 1/11, Loss: 0.054432354867458344\n",
            "Epoch 44/250, Batch 11/11, Loss: 0.06213633716106415\n",
            "Validation Loss after Epoch 44: 0.06420239309469859\n",
            "Epoch 45/250, Batch 1/11, Loss: 0.06355750560760498\n",
            "Epoch 45/250, Batch 11/11, Loss: 0.0681261345744133\n",
            "Validation Loss after Epoch 45: 0.07099483907222748\n",
            "Epoch 46/250, Batch 1/11, Loss: 0.05653264373540878\n",
            "Epoch 46/250, Batch 11/11, Loss: 0.06148601323366165\n",
            "Validation Loss after Epoch 46: 0.06362274910012881\n",
            "Epoch 47/250, Batch 1/11, Loss: 0.052365317940711975\n",
            "Epoch 47/250, Batch 11/11, Loss: 0.06121766194701195\n",
            "Validation Loss after Epoch 47: 0.06530779351790746\n",
            "Epoch 48/250, Batch 1/11, Loss: 0.051961805671453476\n",
            "Epoch 48/250, Batch 11/11, Loss: 0.06360767036676407\n",
            "Validation Loss after Epoch 48: 0.06506989647944768\n",
            "Epoch 49/250, Batch 1/11, Loss: 0.05044383555650711\n",
            "Epoch 49/250, Batch 11/11, Loss: 0.06380177289247513\n",
            "Validation Loss after Epoch 49: 0.06352705756823222\n",
            "Epoch 50/250, Batch 1/11, Loss: 0.055840615183115005\n",
            "Epoch 50/250, Batch 11/11, Loss: 0.06335146725177765\n",
            "Validation Loss after Epoch 50: 0.067598607391119\n",
            "Epoch 51/250, Batch 1/11, Loss: 0.051438357681035995\n",
            "Epoch 51/250, Batch 11/11, Loss: 0.0667070522904396\n",
            "Validation Loss after Epoch 51: 0.06119303529461225\n",
            "Epoch 52/250, Batch 1/11, Loss: 0.048095617443323135\n",
            "Epoch 52/250, Batch 11/11, Loss: 0.05662262812256813\n",
            "Validation Loss after Epoch 52: 0.06211878607670466\n",
            "Epoch 53/250, Batch 1/11, Loss: 0.0500715933740139\n",
            "Epoch 53/250, Batch 11/11, Loss: 0.054200828075408936\n",
            "Validation Loss after Epoch 53: 0.06323364252845447\n",
            "Epoch 54/250, Batch 1/11, Loss: 0.05034631863236427\n",
            "Epoch 54/250, Batch 11/11, Loss: 0.058787498623132706\n",
            "Validation Loss after Epoch 54: 0.06529435142874718\n",
            "Epoch 55/250, Batch 1/11, Loss: 0.0541217140853405\n",
            "Epoch 55/250, Batch 11/11, Loss: 0.060883715748786926\n",
            "Validation Loss after Epoch 55: 0.06104616944988569\n",
            "Epoch 56/250, Batch 1/11, Loss: 0.04890727996826172\n",
            "Epoch 56/250, Batch 11/11, Loss: 0.05484343692660332\n",
            "Validation Loss after Epoch 56: 0.06335332492987315\n",
            "Epoch 57/250, Batch 1/11, Loss: 0.046872202306985855\n",
            "Epoch 57/250, Batch 11/11, Loss: 0.06318943947553635\n",
            "Validation Loss after Epoch 57: 0.062171418219804764\n",
            "Epoch 58/250, Batch 1/11, Loss: 0.04333383962512016\n",
            "Epoch 58/250, Batch 11/11, Loss: 0.0562981441617012\n",
            "Validation Loss after Epoch 58: 0.06580923000971477\n",
            "Epoch 59/250, Batch 1/11, Loss: 0.052296336740255356\n",
            "Epoch 59/250, Batch 11/11, Loss: 0.053173162043094635\n",
            "Validation Loss after Epoch 59: 0.06197453662753105\n",
            "Epoch 60/250, Batch 1/11, Loss: 0.04799657315015793\n",
            "Epoch 60/250, Batch 11/11, Loss: 0.0514940582215786\n",
            "Validation Loss after Epoch 60: 0.061203462382157646\n",
            "Epoch 61/250, Batch 1/11, Loss: 0.04803440719842911\n",
            "Epoch 61/250, Batch 11/11, Loss: 0.0594620555639267\n",
            "Validation Loss after Epoch 61: 0.062087975442409515\n",
            "Epoch 62/250, Batch 1/11, Loss: 0.0428614504635334\n",
            "Epoch 62/250, Batch 11/11, Loss: 0.053386516869068146\n",
            "Validation Loss after Epoch 62: 0.06629331409931183\n",
            "Epoch 63/250, Batch 1/11, Loss: 0.044644664973020554\n",
            "Epoch 63/250, Batch 11/11, Loss: 0.0492057204246521\n",
            "Validation Loss after Epoch 63: 0.06335717936356862\n",
            "Epoch 64/250, Batch 1/11, Loss: 0.04364108294248581\n",
            "Epoch 64/250, Batch 11/11, Loss: 0.0554383210837841\n",
            "Validation Loss after Epoch 64: 0.06143426398436228\n",
            "Epoch 65/250, Batch 1/11, Loss: 0.04107385873794556\n",
            "Epoch 65/250, Batch 11/11, Loss: 0.05726306140422821\n",
            "Validation Loss after Epoch 65: 0.06011868889133135\n",
            "Epoch 66/250, Batch 1/11, Loss: 0.038293201476335526\n",
            "Epoch 66/250, Batch 11/11, Loss: 0.05556440353393555\n",
            "Validation Loss after Epoch 66: 0.06329836572209994\n",
            "Epoch 67/250, Batch 1/11, Loss: 0.044160787016153336\n",
            "Epoch 67/250, Batch 11/11, Loss: 0.04946412891149521\n",
            "Validation Loss after Epoch 67: 0.06449126203854878\n",
            "Epoch 68/250, Batch 1/11, Loss: 0.04556842893362045\n",
            "Epoch 68/250, Batch 11/11, Loss: 0.050966229289770126\n",
            "Validation Loss after Epoch 68: 0.06271824489037196\n",
            "Epoch 69/250, Batch 1/11, Loss: 0.04113171622157097\n",
            "Epoch 69/250, Batch 11/11, Loss: 0.05409722402691841\n",
            "Validation Loss after Epoch 69: 0.058005532870690026\n",
            "Epoch 70/250, Batch 1/11, Loss: 0.04317473992705345\n",
            "Epoch 70/250, Batch 11/11, Loss: 0.051185719668865204\n",
            "Validation Loss after Epoch 70: 0.061816384394963585\n",
            "Epoch 71/250, Batch 1/11, Loss: 0.04357247054576874\n",
            "Epoch 71/250, Batch 11/11, Loss: 0.05557470768690109\n",
            "Validation Loss after Epoch 71: 0.06337450444698334\n",
            "Epoch 72/250, Batch 1/11, Loss: 0.04040316864848137\n",
            "Epoch 72/250, Batch 11/11, Loss: 0.05298550799489021\n",
            "Validation Loss after Epoch 72: 0.061963099986314774\n",
            "Epoch 73/250, Batch 1/11, Loss: 0.04233177751302719\n",
            "Epoch 73/250, Batch 11/11, Loss: 0.051726434379816055\n",
            "Validation Loss after Epoch 73: 0.06004175047079722\n",
            "Epoch 74/250, Batch 1/11, Loss: 0.04178085923194885\n",
            "Epoch 74/250, Batch 11/11, Loss: 0.05368255451321602\n",
            "Validation Loss after Epoch 74: 0.06273049985369046\n",
            "Epoch 75/250, Batch 1/11, Loss: 0.04253660887479782\n",
            "Epoch 75/250, Batch 11/11, Loss: 0.04863898828625679\n",
            "Validation Loss after Epoch 75: 0.06274997567137082\n",
            "Epoch 76/250, Batch 1/11, Loss: 0.04312591999769211\n",
            "Epoch 76/250, Batch 11/11, Loss: 0.05200400575995445\n",
            "Validation Loss after Epoch 76: 0.06386404236157735\n",
            "Epoch 77/250, Batch 1/11, Loss: 0.04457883909344673\n",
            "Epoch 77/250, Batch 11/11, Loss: 0.05411812290549278\n",
            "Validation Loss after Epoch 77: 0.060607340186834335\n",
            "Epoch 78/250, Batch 1/11, Loss: 0.04718944802880287\n",
            "Epoch 78/250, Batch 11/11, Loss: 0.04433909431099892\n",
            "Validation Loss after Epoch 78: 0.06290020296971004\n",
            "Epoch 79/250, Batch 1/11, Loss: 0.041955430060625076\n",
            "Epoch 79/250, Batch 11/11, Loss: 0.04659084603190422\n",
            "Validation Loss after Epoch 79: 0.06077283248305321\n",
            "Epoch 80/250, Batch 1/11, Loss: 0.04086536914110184\n",
            "Epoch 80/250, Batch 11/11, Loss: 0.051895998418331146\n",
            "Validation Loss after Epoch 80: 0.06270561988155048\n",
            "Epoch 81/250, Batch 1/11, Loss: 0.0432598702609539\n",
            "Epoch 81/250, Batch 11/11, Loss: 0.05183643102645874\n",
            "Validation Loss after Epoch 81: 0.06387509157260259\n",
            "Epoch 82/250, Batch 1/11, Loss: 0.04192774370312691\n",
            "Epoch 82/250, Batch 11/11, Loss: 0.049921419471502304\n",
            "Validation Loss after Epoch 82: 0.06221677859624227\n",
            "Epoch 83/250, Batch 1/11, Loss: 0.04380892217159271\n",
            "Epoch 83/250, Batch 11/11, Loss: 0.04331076145172119\n",
            "Validation Loss after Epoch 83: 0.0617863138516744\n",
            "Epoch 84/250, Batch 1/11, Loss: 0.03789599612355232\n",
            "Epoch 84/250, Batch 11/11, Loss: 0.047099631279706955\n",
            "Validation Loss after Epoch 84: 0.06249420096476873\n",
            "Epoch 85/250, Batch 1/11, Loss: 0.03660793974995613\n",
            "Epoch 85/250, Batch 11/11, Loss: 0.04795389622449875\n",
            "Validation Loss after Epoch 85: 0.05856532727678617\n",
            "Epoch 86/250, Batch 1/11, Loss: 0.04440493509173393\n",
            "Epoch 86/250, Batch 11/11, Loss: 0.04455440118908882\n",
            "Validation Loss after Epoch 86: 0.05955254286527634\n",
            "Epoch 87/250, Batch 1/11, Loss: 0.03776494041085243\n",
            "Epoch 87/250, Batch 11/11, Loss: 0.04298882186412811\n",
            "Validation Loss after Epoch 87: 0.06059431533018748\n",
            "Epoch 88/250, Batch 1/11, Loss: 0.03979035094380379\n",
            "Epoch 88/250, Batch 11/11, Loss: 0.04854656010866165\n",
            "Validation Loss after Epoch 88: 0.059693307926257454\n",
            "Epoch 89/250, Batch 1/11, Loss: 0.04109164699912071\n",
            "Epoch 89/250, Batch 11/11, Loss: 0.04928659275174141\n",
            "Validation Loss after Epoch 89: 0.060434446980555855\n",
            "Epoch 90/250, Batch 1/11, Loss: 0.03565436601638794\n",
            "Epoch 90/250, Batch 11/11, Loss: 0.03904769569635391\n",
            "Validation Loss after Epoch 90: 0.05987247700492541\n",
            "Epoch 91/250, Batch 1/11, Loss: 0.03682548180222511\n",
            "Epoch 91/250, Batch 11/11, Loss: 0.0498797707259655\n",
            "Validation Loss after Epoch 91: 0.06244926402966181\n",
            "Epoch 92/250, Batch 1/11, Loss: 0.039007335901260376\n",
            "Epoch 92/250, Batch 11/11, Loss: 0.051340263336896896\n",
            "Validation Loss after Epoch 92: 0.06406550606091817\n",
            "Epoch 93/250, Batch 1/11, Loss: 0.03487597033381462\n",
            "Epoch 93/250, Batch 11/11, Loss: 0.03931720554828644\n",
            "Validation Loss after Epoch 93: 0.061305283258358635\n",
            "Epoch 94/250, Batch 1/11, Loss: 0.03161215782165527\n",
            "Epoch 94/250, Batch 11/11, Loss: 0.04464395344257355\n",
            "Validation Loss after Epoch 94: 0.05779025827844938\n",
            "Epoch 95/250, Batch 1/11, Loss: 0.03749202564358711\n",
            "Epoch 95/250, Batch 11/11, Loss: 0.03962421417236328\n",
            "Validation Loss after Epoch 95: 0.06282345578074455\n",
            "Epoch 96/250, Batch 1/11, Loss: 0.03685109317302704\n",
            "Epoch 96/250, Batch 11/11, Loss: 0.04570869356393814\n",
            "Validation Loss after Epoch 96: 0.06088210393985113\n",
            "Epoch 97/250, Batch 1/11, Loss: 0.03761483356356621\n",
            "Epoch 97/250, Batch 11/11, Loss: 0.05115307494997978\n",
            "Validation Loss after Epoch 97: 0.060273063679536186\n",
            "Epoch 98/250, Batch 1/11, Loss: 0.0356515608727932\n",
            "Epoch 98/250, Batch 11/11, Loss: 0.04930244758725166\n",
            "Validation Loss after Epoch 98: 0.06061824907859167\n",
            "Epoch 99/250, Batch 1/11, Loss: 0.03715338930487633\n",
            "Epoch 99/250, Batch 11/11, Loss: 0.044390156865119934\n",
            "Validation Loss after Epoch 99: 0.06172311305999756\n",
            "Epoch 100/250, Batch 1/11, Loss: 0.03521740064024925\n",
            "Epoch 100/250, Batch 11/11, Loss: 0.03723975270986557\n",
            "Validation Loss after Epoch 100: 0.059129420667886734\n",
            "Epoch 101/250, Batch 1/11, Loss: 0.03458186984062195\n",
            "Epoch 101/250, Batch 11/11, Loss: 0.04510596767067909\n",
            "Validation Loss after Epoch 101: 0.059635614355405174\n",
            "Epoch 102/250, Batch 1/11, Loss: 0.038794949650764465\n",
            "Epoch 102/250, Batch 11/11, Loss: 0.04521762207150459\n",
            "Validation Loss after Epoch 102: 0.06206113348404566\n",
            "Epoch 103/250, Batch 1/11, Loss: 0.03838394209742546\n",
            "Epoch 103/250, Batch 11/11, Loss: 0.04374320060014725\n",
            "Validation Loss after Epoch 103: 0.06385935967167218\n",
            "Epoch 104/250, Batch 1/11, Loss: 0.036094825714826584\n",
            "Epoch 104/250, Batch 11/11, Loss: 0.04061735048890114\n",
            "Validation Loss after Epoch 104: 0.06635812297463417\n",
            "Epoch 105/250, Batch 1/11, Loss: 0.03826726973056793\n",
            "Epoch 105/250, Batch 11/11, Loss: 0.04214123636484146\n",
            "Validation Loss after Epoch 105: 0.06343892092506091\n",
            "Epoch 106/250, Batch 1/11, Loss: 0.0383099764585495\n",
            "Epoch 106/250, Batch 11/11, Loss: 0.04088165983557701\n",
            "Validation Loss after Epoch 106: 0.06023792922496796\n",
            "Epoch 107/250, Batch 1/11, Loss: 0.032324984669685364\n",
            "Epoch 107/250, Batch 11/11, Loss: 0.04326044023036957\n",
            "Validation Loss after Epoch 107: 0.06484381233652432\n",
            "Epoch 108/250, Batch 1/11, Loss: 0.03356296196579933\n",
            "Epoch 108/250, Batch 11/11, Loss: 0.039482470601797104\n",
            "Validation Loss after Epoch 108: 0.0630802611509959\n",
            "Epoch 109/250, Batch 1/11, Loss: 0.03819291666150093\n",
            "Epoch 109/250, Batch 11/11, Loss: 0.03998815268278122\n",
            "Validation Loss after Epoch 109: 0.06048047294219335\n",
            "Epoch 110/250, Batch 1/11, Loss: 0.02962389774620533\n",
            "Epoch 110/250, Batch 11/11, Loss: 0.042744118720293045\n",
            "Validation Loss after Epoch 110: 0.06291791051626205\n",
            "Epoch 111/250, Batch 1/11, Loss: 0.03397907316684723\n",
            "Epoch 111/250, Batch 11/11, Loss: 0.04412190243601799\n",
            "Validation Loss after Epoch 111: 0.059738015135129295\n",
            "Epoch 112/250, Batch 1/11, Loss: 0.03456259146332741\n",
            "Epoch 112/250, Batch 11/11, Loss: 0.04340383782982826\n",
            "Validation Loss after Epoch 112: 0.06235692774256071\n",
            "Epoch 113/250, Batch 1/11, Loss: 0.03726915642619133\n",
            "Epoch 113/250, Batch 11/11, Loss: 0.04510220140218735\n",
            "Validation Loss after Epoch 113: 0.0627543752392133\n",
            "Epoch 114/250, Batch 1/11, Loss: 0.03316284343600273\n",
            "Epoch 114/250, Batch 11/11, Loss: 0.046081822365522385\n",
            "Validation Loss after Epoch 114: 0.0633414809902509\n",
            "Epoch 115/250, Batch 1/11, Loss: 0.03666030988097191\n",
            "Epoch 115/250, Batch 11/11, Loss: 0.04706154391169548\n",
            "Validation Loss after Epoch 115: 0.059431515634059906\n",
            "Epoch 116/250, Batch 1/11, Loss: 0.03322000429034233\n",
            "Epoch 116/250, Batch 11/11, Loss: 0.040638282895088196\n",
            "Validation Loss after Epoch 116: 0.06173709531625112\n",
            "Epoch 117/250, Batch 1/11, Loss: 0.03694022074341774\n",
            "Epoch 117/250, Batch 11/11, Loss: 0.03607795014977455\n",
            "Validation Loss after Epoch 117: 0.060883755485216774\n",
            "Epoch 118/250, Batch 1/11, Loss: 0.031559769064188004\n",
            "Epoch 118/250, Batch 11/11, Loss: 0.043944161385297775\n",
            "Validation Loss after Epoch 118: 0.06163142373164495\n",
            "Epoch 119/250, Batch 1/11, Loss: 0.034010257571935654\n",
            "Epoch 119/250, Batch 11/11, Loss: 0.03966403752565384\n",
            "Validation Loss after Epoch 119: 0.06339547286430995\n",
            "Epoch 120/250, Batch 1/11, Loss: 0.03282679617404938\n",
            "Epoch 120/250, Batch 11/11, Loss: 0.03730195015668869\n",
            "Validation Loss after Epoch 120: 0.0655300368865331\n",
            "Epoch 121/250, Batch 1/11, Loss: 0.038322966545820236\n",
            "Epoch 121/250, Batch 11/11, Loss: 0.040739912539720535\n",
            "Validation Loss after Epoch 121: 0.060864352931578956\n",
            "Epoch 122/250, Batch 1/11, Loss: 0.030844412744045258\n",
            "Epoch 122/250, Batch 11/11, Loss: 0.03799406439065933\n",
            "Validation Loss after Epoch 122: 0.060047211746374764\n",
            "Epoch 123/250, Batch 1/11, Loss: 0.03519893065094948\n",
            "Epoch 123/250, Batch 11/11, Loss: 0.037627313286066055\n",
            "Validation Loss after Epoch 123: 0.06007938583691915\n",
            "Epoch 124/250, Batch 1/11, Loss: 0.03235023468732834\n",
            "Epoch 124/250, Batch 11/11, Loss: 0.04237789288163185\n",
            "Validation Loss after Epoch 124: 0.05909830331802368\n",
            "Epoch 125/250, Batch 1/11, Loss: 0.034242674708366394\n",
            "Epoch 125/250, Batch 11/11, Loss: 0.040783174335956573\n",
            "Validation Loss after Epoch 125: 0.05898046617706617\n",
            "Epoch 126/250, Batch 1/11, Loss: 0.031133372336626053\n",
            "Epoch 126/250, Batch 11/11, Loss: 0.03813751041889191\n",
            "Validation Loss after Epoch 126: 0.059882803509632744\n",
            "Epoch 127/250, Batch 1/11, Loss: 0.03370813652873039\n",
            "Epoch 127/250, Batch 11/11, Loss: 0.04128885269165039\n",
            "Validation Loss after Epoch 127: 0.061994933833678566\n",
            "Epoch 128/250, Batch 1/11, Loss: 0.03257431462407112\n",
            "Epoch 128/250, Batch 11/11, Loss: 0.04051871597766876\n",
            "Validation Loss after Epoch 128: 0.06089949980378151\n",
            "Epoch 129/250, Batch 1/11, Loss: 0.03236036375164986\n",
            "Epoch 129/250, Batch 11/11, Loss: 0.037351034581661224\n",
            "Validation Loss after Epoch 129: 0.06081207220753034\n",
            "Epoch 130/250, Batch 1/11, Loss: 0.03181988373398781\n",
            "Epoch 130/250, Batch 11/11, Loss: 0.03470822423696518\n",
            "Validation Loss after Epoch 130: 0.06060087184111277\n",
            "Epoch 131/250, Batch 1/11, Loss: 0.03279609605669975\n",
            "Epoch 131/250, Batch 11/11, Loss: 0.0361865758895874\n",
            "Validation Loss after Epoch 131: 0.06113158414761225\n",
            "Epoch 132/250, Batch 1/11, Loss: 0.030794691294431686\n",
            "Epoch 132/250, Batch 11/11, Loss: 0.03663276508450508\n",
            "Validation Loss after Epoch 132: 0.06434401124715805\n",
            "Epoch 133/250, Batch 1/11, Loss: 0.0293240025639534\n",
            "Epoch 133/250, Batch 11/11, Loss: 0.03789098933339119\n",
            "Validation Loss after Epoch 133: 0.05937705934047699\n",
            "Epoch 134/250, Batch 1/11, Loss: 0.03047185204923153\n",
            "Epoch 134/250, Batch 11/11, Loss: 0.038049399852752686\n",
            "Validation Loss after Epoch 134: 0.06594496468702953\n",
            "Epoch 135/250, Batch 1/11, Loss: 0.02973267249763012\n",
            "Epoch 135/250, Batch 11/11, Loss: 0.04221731796860695\n",
            "Validation Loss after Epoch 135: 0.067940770337979\n",
            "Epoch 136/250, Batch 1/11, Loss: 0.034754615277051926\n",
            "Epoch 136/250, Batch 11/11, Loss: 0.0418267659842968\n",
            "Validation Loss after Epoch 136: 0.05803524081905683\n",
            "Epoch 137/250, Batch 1/11, Loss: 0.031077513471245766\n",
            "Epoch 137/250, Batch 11/11, Loss: 0.03821622580289841\n",
            "Validation Loss after Epoch 137: 0.062352120876312256\n",
            "Epoch 138/250, Batch 1/11, Loss: 0.03065493330359459\n",
            "Epoch 138/250, Batch 11/11, Loss: 0.03716089949011803\n",
            "Validation Loss after Epoch 138: 0.06032410760720571\n",
            "Epoch 139/250, Batch 1/11, Loss: 0.029015738517045975\n",
            "Epoch 139/250, Batch 11/11, Loss: 0.038651470094919205\n",
            "Validation Loss after Epoch 139: 0.06251796831687291\n",
            "Epoch 140/250, Batch 1/11, Loss: 0.03127985820174217\n",
            "Epoch 140/250, Batch 11/11, Loss: 0.038553494960069656\n",
            "Validation Loss after Epoch 140: 0.06265685707330704\n",
            "Epoch 141/250, Batch 1/11, Loss: 0.025454219430685043\n",
            "Epoch 141/250, Batch 11/11, Loss: 0.03811183199286461\n",
            "Validation Loss after Epoch 141: 0.06256274630626042\n",
            "Epoch 142/250, Batch 1/11, Loss: 0.03141465783119202\n",
            "Epoch 142/250, Batch 11/11, Loss: 0.03708421066403389\n",
            "Validation Loss after Epoch 142: 0.0665194441874822\n",
            "Epoch 143/250, Batch 1/11, Loss: 0.0304571520537138\n",
            "Epoch 143/250, Batch 11/11, Loss: 0.03378669545054436\n",
            "Validation Loss after Epoch 143: 0.0627133809030056\n",
            "Epoch 144/250, Batch 1/11, Loss: 0.03296031057834625\n",
            "Epoch 144/250, Batch 11/11, Loss: 0.033434782177209854\n",
            "Validation Loss after Epoch 144: 0.05875909825166067\n",
            "Epoch 145/250, Batch 1/11, Loss: 0.028484269976615906\n",
            "Epoch 145/250, Batch 11/11, Loss: 0.03678768500685692\n",
            "Validation Loss after Epoch 145: 0.06386176993449529\n",
            "Epoch 146/250, Batch 1/11, Loss: 0.02920808084309101\n",
            "Epoch 146/250, Batch 11/11, Loss: 0.036508332937955856\n",
            "Validation Loss after Epoch 146: 0.06222888578971227\n",
            "Epoch 147/250, Batch 1/11, Loss: 0.027507400140166283\n",
            "Epoch 147/250, Batch 11/11, Loss: 0.03596772626042366\n",
            "Validation Loss after Epoch 147: 0.06152167419592539\n",
            "Epoch 148/250, Batch 1/11, Loss: 0.02766418270766735\n",
            "Epoch 148/250, Batch 11/11, Loss: 0.03505629301071167\n",
            "Validation Loss after Epoch 148: 0.06236583118637403\n",
            "Epoch 149/250, Batch 1/11, Loss: 0.02842709608376026\n",
            "Epoch 149/250, Batch 11/11, Loss: 0.039960745722055435\n",
            "Validation Loss after Epoch 149: 0.061746809631586075\n",
            "Epoch 150/250, Batch 1/11, Loss: 0.02508162148296833\n",
            "Epoch 150/250, Batch 11/11, Loss: 0.03953294828534126\n",
            "Validation Loss after Epoch 150: 0.06273424004515012\n",
            "Epoch 151/250, Batch 1/11, Loss: 0.027070941403508186\n",
            "Epoch 151/250, Batch 11/11, Loss: 0.035384729504585266\n",
            "Validation Loss after Epoch 151: 0.0636134905119737\n",
            "Epoch 152/250, Batch 1/11, Loss: 0.030155595391988754\n",
            "Epoch 152/250, Batch 11/11, Loss: 0.03722774237394333\n",
            "Validation Loss after Epoch 152: 0.06773150712251663\n",
            "Epoch 153/250, Batch 1/11, Loss: 0.03160014748573303\n",
            "Epoch 153/250, Batch 11/11, Loss: 0.036085743457078934\n",
            "Validation Loss after Epoch 153: 0.06440847491224606\n",
            "Epoch 154/250, Batch 1/11, Loss: 0.02973797172307968\n",
            "Epoch 154/250, Batch 11/11, Loss: 0.03543330729007721\n",
            "Validation Loss after Epoch 154: 0.06386669973532359\n",
            "Epoch 155/250, Batch 1/11, Loss: 0.025463417172431946\n",
            "Epoch 155/250, Batch 11/11, Loss: 0.03194688260555267\n",
            "Validation Loss after Epoch 155: 0.06468171874682109\n",
            "Epoch 156/250, Batch 1/11, Loss: 0.03202979639172554\n",
            "Epoch 156/250, Batch 11/11, Loss: 0.03409808874130249\n",
            "Validation Loss after Epoch 156: 0.060592133551836014\n",
            "Epoch 157/250, Batch 1/11, Loss: 0.03023124486207962\n",
            "Epoch 157/250, Batch 11/11, Loss: 0.03280063346028328\n",
            "Validation Loss after Epoch 157: 0.060078407327334084\n",
            "Epoch 158/250, Batch 1/11, Loss: 0.029753390699625015\n",
            "Epoch 158/250, Batch 11/11, Loss: 0.033465612679719925\n",
            "Validation Loss after Epoch 158: 0.061090936263402305\n",
            "Epoch 159/250, Batch 1/11, Loss: 0.0262795127928257\n",
            "Epoch 159/250, Batch 11/11, Loss: 0.03948730602860451\n",
            "Validation Loss after Epoch 159: 0.0648144061366717\n",
            "Epoch 160/250, Batch 1/11, Loss: 0.030513357371091843\n",
            "Epoch 160/250, Batch 11/11, Loss: 0.03522966802120209\n",
            "Validation Loss after Epoch 160: 0.06331389024853706\n",
            "Epoch 161/250, Batch 1/11, Loss: 0.025397518649697304\n",
            "Epoch 161/250, Batch 11/11, Loss: 0.035220202058553696\n",
            "Validation Loss after Epoch 161: 0.06362098703781764\n",
            "Epoch 162/250, Batch 1/11, Loss: 0.027466369792819023\n",
            "Epoch 162/250, Batch 11/11, Loss: 0.03168779984116554\n",
            "Validation Loss after Epoch 162: 0.060661679754654564\n",
            "Epoch 163/250, Batch 1/11, Loss: 0.03133547678589821\n",
            "Epoch 163/250, Batch 11/11, Loss: 0.03701261058449745\n",
            "Validation Loss after Epoch 163: 0.06377830108006795\n",
            "Epoch 164/250, Batch 1/11, Loss: 0.02655169367790222\n",
            "Epoch 164/250, Batch 11/11, Loss: 0.03470562398433685\n",
            "Validation Loss after Epoch 164: 0.062209864457448326\n",
            "Epoch 165/250, Batch 1/11, Loss: 0.030634328722953796\n",
            "Epoch 165/250, Batch 11/11, Loss: 0.03526269644498825\n",
            "Validation Loss after Epoch 165: 0.06604122246305148\n",
            "Epoch 166/250, Batch 1/11, Loss: 0.027552170678973198\n",
            "Epoch 166/250, Batch 11/11, Loss: 0.03389257937669754\n",
            "Validation Loss after Epoch 166: 0.06303068995475769\n",
            "Epoch 167/250, Batch 1/11, Loss: 0.03007429465651512\n",
            "Epoch 167/250, Batch 11/11, Loss: 0.03714128956198692\n",
            "Validation Loss after Epoch 167: 0.061785905311505\n",
            "Epoch 168/250, Batch 1/11, Loss: 0.027648266404867172\n",
            "Epoch 168/250, Batch 11/11, Loss: 0.0337032750248909\n",
            "Validation Loss after Epoch 168: 0.06133445352315903\n",
            "Epoch 169/250, Batch 1/11, Loss: 0.03319967910647392\n",
            "Epoch 169/250, Batch 11/11, Loss: 0.036892350763082504\n",
            "Validation Loss after Epoch 169: 0.057761866599321365\n",
            "Epoch 170/250, Batch 1/11, Loss: 0.027933945879340172\n",
            "Epoch 170/250, Batch 11/11, Loss: 0.03643101453781128\n",
            "Validation Loss after Epoch 170: 0.0632617324590683\n",
            "Epoch 171/250, Batch 1/11, Loss: 0.02773689292371273\n",
            "Epoch 171/250, Batch 11/11, Loss: 0.03253371641039848\n",
            "Validation Loss after Epoch 171: 0.06987362230817477\n",
            "Epoch 172/250, Batch 1/11, Loss: 0.026495888829231262\n",
            "Epoch 172/250, Batch 11/11, Loss: 0.036316487938165665\n",
            "Validation Loss after Epoch 172: 0.06722278396288554\n",
            "Epoch 173/250, Batch 1/11, Loss: 0.03192117065191269\n",
            "Epoch 173/250, Batch 11/11, Loss: 0.038234733045101166\n",
            "Validation Loss after Epoch 173: 0.06525746608773868\n",
            "Epoch 174/250, Batch 1/11, Loss: 0.026486443355679512\n",
            "Epoch 174/250, Batch 11/11, Loss: 0.031385939568281174\n",
            "Validation Loss after Epoch 174: 0.06605906784534454\n",
            "Epoch 175/250, Batch 1/11, Loss: 0.023801282048225403\n",
            "Epoch 175/250, Batch 11/11, Loss: 0.03704860433936119\n",
            "Validation Loss after Epoch 175: 0.06443891301751137\n",
            "Epoch 176/250, Batch 1/11, Loss: 0.027371112257242203\n",
            "Epoch 176/250, Batch 11/11, Loss: 0.033582210540771484\n",
            "Validation Loss after Epoch 176: 0.06665670002500217\n",
            "Epoch 177/250, Batch 1/11, Loss: 0.02913765050470829\n",
            "Epoch 177/250, Batch 11/11, Loss: 0.03194273263216019\n",
            "Validation Loss after Epoch 177: 0.06409301732977231\n",
            "Epoch 178/250, Batch 1/11, Loss: 0.030396757647395134\n",
            "Epoch 178/250, Batch 11/11, Loss: 0.030960826203227043\n",
            "Validation Loss after Epoch 178: 0.06793499737977982\n",
            "Epoch 179/250, Batch 1/11, Loss: 0.03080313839018345\n",
            "Epoch 179/250, Batch 11/11, Loss: 0.03207170218229294\n",
            "Validation Loss after Epoch 179: 0.06530120472113292\n",
            "Epoch 180/250, Batch 1/11, Loss: 0.02891702391207218\n",
            "Epoch 180/250, Batch 11/11, Loss: 0.031097734346985817\n",
            "Validation Loss after Epoch 180: 0.06439132491747539\n",
            "Epoch 181/250, Batch 1/11, Loss: 0.02428823709487915\n",
            "Epoch 181/250, Batch 11/11, Loss: 0.039665140211582184\n",
            "Validation Loss after Epoch 181: 0.06283511221408844\n",
            "Epoch 182/250, Batch 1/11, Loss: 0.027629511430859566\n",
            "Epoch 182/250, Batch 11/11, Loss: 0.029178442433476448\n",
            "Validation Loss after Epoch 182: 0.06763620053728421\n",
            "Epoch 183/250, Batch 1/11, Loss: 0.025140447542071342\n",
            "Epoch 183/250, Batch 11/11, Loss: 0.0309290811419487\n",
            "Validation Loss after Epoch 183: 0.06422963986794154\n",
            "Epoch 184/250, Batch 1/11, Loss: 0.024070024490356445\n",
            "Epoch 184/250, Batch 11/11, Loss: 0.03141440823674202\n",
            "Validation Loss after Epoch 184: 0.06588316957155864\n",
            "Epoch 185/250, Batch 1/11, Loss: 0.02322814054787159\n",
            "Epoch 185/250, Batch 11/11, Loss: 0.030357951298356056\n",
            "Validation Loss after Epoch 185: 0.06751846894621849\n",
            "Epoch 186/250, Batch 1/11, Loss: 0.029762597754597664\n",
            "Epoch 186/250, Batch 11/11, Loss: 0.03353513777256012\n",
            "Validation Loss after Epoch 186: 0.06475757683316867\n",
            "Epoch 187/250, Batch 1/11, Loss: 0.034714460372924805\n",
            "Epoch 187/250, Batch 11/11, Loss: 0.03183935582637787\n",
            "Validation Loss after Epoch 187: 0.06157991414268812\n",
            "Epoch 188/250, Batch 1/11, Loss: 0.025623813271522522\n",
            "Epoch 188/250, Batch 11/11, Loss: 0.035386137664318085\n",
            "Validation Loss after Epoch 188: 0.06566895917057991\n",
            "Epoch 189/250, Batch 1/11, Loss: 0.02396390214562416\n",
            "Epoch 189/250, Batch 11/11, Loss: 0.04173557832837105\n",
            "Validation Loss after Epoch 189: 0.06431605045994122\n",
            "Epoch 190/250, Batch 1/11, Loss: 0.02559984289109707\n",
            "Epoch 190/250, Batch 11/11, Loss: 0.03286327049136162\n",
            "Validation Loss after Epoch 190: 0.06973351786533992\n",
            "Epoch 191/250, Batch 1/11, Loss: 0.026555132120847702\n",
            "Epoch 191/250, Batch 11/11, Loss: 0.03376834839582443\n",
            "Validation Loss after Epoch 191: 0.06435638541976611\n",
            "Epoch 192/250, Batch 1/11, Loss: 0.024129418656229973\n",
            "Epoch 192/250, Batch 11/11, Loss: 0.028697293251752853\n",
            "Validation Loss after Epoch 192: 0.06233858938018481\n",
            "Epoch 193/250, Batch 1/11, Loss: 0.02520740032196045\n",
            "Epoch 193/250, Batch 11/11, Loss: 0.028315294533967972\n",
            "Validation Loss after Epoch 193: 0.06534237414598465\n",
            "Epoch 194/250, Batch 1/11, Loss: 0.020709726959466934\n",
            "Epoch 194/250, Batch 11/11, Loss: 0.025297174230217934\n",
            "Validation Loss after Epoch 194: 0.06864329427480698\n",
            "Epoch 195/250, Batch 1/11, Loss: 0.027862146496772766\n",
            "Epoch 195/250, Batch 11/11, Loss: 0.025239866226911545\n",
            "Validation Loss after Epoch 195: 0.06632118672132492\n",
            "Epoch 196/250, Batch 1/11, Loss: 0.02742331102490425\n",
            "Epoch 196/250, Batch 11/11, Loss: 0.031226277351379395\n",
            "Validation Loss after Epoch 196: 0.06095897157986959\n",
            "Epoch 197/250, Batch 1/11, Loss: 0.02834867313504219\n",
            "Epoch 197/250, Batch 11/11, Loss: 0.03280320763587952\n",
            "Validation Loss after Epoch 197: 0.06481767073273659\n",
            "Epoch 198/250, Batch 1/11, Loss: 0.0285206101834774\n",
            "Epoch 198/250, Batch 11/11, Loss: 0.03324263542890549\n",
            "Validation Loss after Epoch 198: 0.0622271200021108\n",
            "Epoch 199/250, Batch 1/11, Loss: 0.022849760949611664\n",
            "Epoch 199/250, Batch 11/11, Loss: 0.031106336042284966\n",
            "Validation Loss after Epoch 199: 0.06287845224142075\n",
            "Epoch 200/250, Batch 1/11, Loss: 0.02122841402888298\n",
            "Epoch 200/250, Batch 11/11, Loss: 0.029971910640597343\n",
            "Validation Loss after Epoch 200: 0.07102272907892863\n",
            "Epoch 201/250, Batch 1/11, Loss: 0.023424316197633743\n",
            "Epoch 201/250, Batch 11/11, Loss: 0.03045780211687088\n",
            "Validation Loss after Epoch 201: 0.06634142498175304\n",
            "Epoch 202/250, Batch 1/11, Loss: 0.024079278111457825\n",
            "Epoch 202/250, Batch 11/11, Loss: 0.027465127408504486\n",
            "Validation Loss after Epoch 202: 0.06321930636962254\n",
            "Epoch 203/250, Batch 1/11, Loss: 0.022515175864100456\n",
            "Epoch 203/250, Batch 11/11, Loss: 0.027004003524780273\n",
            "Validation Loss after Epoch 203: 0.0636444886525472\n",
            "Epoch 204/250, Batch 1/11, Loss: 0.02192636765539646\n",
            "Epoch 204/250, Batch 11/11, Loss: 0.03736815229058266\n",
            "Validation Loss after Epoch 204: 0.06639569128553073\n",
            "Epoch 205/250, Batch 1/11, Loss: 0.02348307892680168\n",
            "Epoch 205/250, Batch 11/11, Loss: 0.0284994225949049\n",
            "Validation Loss after Epoch 205: 0.06229417026042938\n",
            "Epoch 206/250, Batch 1/11, Loss: 0.027225054800510406\n",
            "Epoch 206/250, Batch 11/11, Loss: 0.027122879400849342\n",
            "Validation Loss after Epoch 206: 0.06410338481267293\n",
            "Epoch 207/250, Batch 1/11, Loss: 0.028243158012628555\n",
            "Epoch 207/250, Batch 11/11, Loss: 0.02785526029765606\n",
            "Validation Loss after Epoch 207: 0.06271895269552867\n",
            "Epoch 208/250, Batch 1/11, Loss: 0.02427864260971546\n",
            "Epoch 208/250, Batch 11/11, Loss: 0.03314603865146637\n",
            "Validation Loss after Epoch 208: 0.06639080742994945\n",
            "Epoch 209/250, Batch 1/11, Loss: 0.02431781403720379\n",
            "Epoch 209/250, Batch 11/11, Loss: 0.02668389119207859\n",
            "Validation Loss after Epoch 209: 0.06829620401064555\n",
            "Epoch 210/250, Batch 1/11, Loss: 0.025619380176067352\n",
            "Epoch 210/250, Batch 11/11, Loss: 0.03291737288236618\n",
            "Validation Loss after Epoch 210: 0.06902557238936424\n",
            "Epoch 211/250, Batch 1/11, Loss: 0.02076294645667076\n",
            "Epoch 211/250, Batch 11/11, Loss: 0.02769436314702034\n",
            "Validation Loss after Epoch 211: 0.0635659247636795\n",
            "Epoch 212/250, Batch 1/11, Loss: 0.022042596712708473\n",
            "Epoch 212/250, Batch 11/11, Loss: 0.028820045292377472\n",
            "Validation Loss after Epoch 212: 0.0644106554488341\n",
            "Epoch 213/250, Batch 1/11, Loss: 0.0224139466881752\n",
            "Epoch 213/250, Batch 11/11, Loss: 0.024800967425107956\n",
            "Validation Loss after Epoch 213: 0.06660481666525205\n",
            "Epoch 214/250, Batch 1/11, Loss: 0.02573363482952118\n",
            "Epoch 214/250, Batch 11/11, Loss: 0.02897147461771965\n",
            "Validation Loss after Epoch 214: 0.06388956184188525\n",
            "Epoch 215/250, Batch 1/11, Loss: 0.023989340290427208\n",
            "Epoch 215/250, Batch 11/11, Loss: 0.027334874495863914\n",
            "Validation Loss after Epoch 215: 0.0684806654850642\n",
            "Epoch 216/250, Batch 1/11, Loss: 0.021198712289333344\n",
            "Epoch 216/250, Batch 11/11, Loss: 0.027210434898734093\n",
            "Validation Loss after Epoch 216: 0.0637851282954216\n",
            "Epoch 217/250, Batch 1/11, Loss: 0.01984567753970623\n",
            "Epoch 217/250, Batch 11/11, Loss: 0.026962703093886375\n",
            "Validation Loss after Epoch 217: 0.06816618392864864\n",
            "Epoch 218/250, Batch 1/11, Loss: 0.024342594668269157\n",
            "Epoch 218/250, Batch 11/11, Loss: 0.027659405022859573\n",
            "Validation Loss after Epoch 218: 0.06757059941689174\n",
            "Epoch 219/250, Batch 1/11, Loss: 0.019613496959209442\n",
            "Epoch 219/250, Batch 11/11, Loss: 0.03359220549464226\n",
            "Validation Loss after Epoch 219: 0.062121196339527764\n",
            "Epoch 220/250, Batch 1/11, Loss: 0.019224710762500763\n",
            "Epoch 220/250, Batch 11/11, Loss: 0.03247670829296112\n",
            "Validation Loss after Epoch 220: 0.06570605685313542\n",
            "Epoch 221/250, Batch 1/11, Loss: 0.02260977402329445\n",
            "Epoch 221/250, Batch 11/11, Loss: 0.028851239010691643\n",
            "Validation Loss after Epoch 221: 0.0655413568019867\n",
            "Epoch 222/250, Batch 1/11, Loss: 0.02029033936560154\n",
            "Epoch 222/250, Batch 11/11, Loss: 0.030399970710277557\n",
            "Validation Loss after Epoch 222: 0.06381427124142647\n",
            "Epoch 223/250, Batch 1/11, Loss: 0.02368616685271263\n",
            "Epoch 223/250, Batch 11/11, Loss: 0.03203609213232994\n",
            "Validation Loss after Epoch 223: 0.06601167966922124\n",
            "Epoch 224/250, Batch 1/11, Loss: 0.023816276341676712\n",
            "Epoch 224/250, Batch 11/11, Loss: 0.025340504944324493\n",
            "Validation Loss after Epoch 224: 0.06906579931577046\n",
            "Epoch 225/250, Batch 1/11, Loss: 0.0239755529910326\n",
            "Epoch 225/250, Batch 11/11, Loss: 0.03204689547419548\n",
            "Validation Loss after Epoch 225: 0.06660556172331174\n",
            "Epoch 226/250, Batch 1/11, Loss: 0.020425530150532722\n",
            "Epoch 226/250, Batch 11/11, Loss: 0.028622083365917206\n",
            "Validation Loss after Epoch 226: 0.06538712233304977\n",
            "Epoch 227/250, Batch 1/11, Loss: 0.026359351351857185\n",
            "Epoch 227/250, Batch 11/11, Loss: 0.02287069894373417\n",
            "Validation Loss after Epoch 227: 0.06468156104286511\n",
            "Epoch 228/250, Batch 1/11, Loss: 0.02151418849825859\n",
            "Epoch 228/250, Batch 11/11, Loss: 0.028825076296925545\n",
            "Validation Loss after Epoch 228: 0.06270267814397812\n",
            "Epoch 229/250, Batch 1/11, Loss: 0.021081969141960144\n",
            "Epoch 229/250, Batch 11/11, Loss: 0.030638189986348152\n",
            "Validation Loss after Epoch 229: 0.06017674754063288\n",
            "Epoch 230/250, Batch 1/11, Loss: 0.023664193227887154\n",
            "Epoch 230/250, Batch 11/11, Loss: 0.026026075705885887\n",
            "Validation Loss after Epoch 230: 0.06416301801800728\n",
            "Epoch 231/250, Batch 1/11, Loss: 0.020268667489290237\n",
            "Epoch 231/250, Batch 11/11, Loss: 0.035098008811473846\n",
            "Validation Loss after Epoch 231: 0.06314802169799805\n",
            "Epoch 232/250, Batch 1/11, Loss: 0.025275325402617455\n",
            "Epoch 232/250, Batch 11/11, Loss: 0.0321815200150013\n",
            "Validation Loss after Epoch 232: 0.06917857006192207\n",
            "Epoch 233/250, Batch 1/11, Loss: 0.023981789126992226\n",
            "Epoch 233/250, Batch 11/11, Loss: 0.02772848680615425\n",
            "Validation Loss after Epoch 233: 0.06718680014212926\n",
            "Epoch 234/250, Batch 1/11, Loss: 0.018705805763602257\n",
            "Epoch 234/250, Batch 11/11, Loss: 0.02380148507654667\n",
            "Validation Loss after Epoch 234: 0.06934741139411926\n",
            "Epoch 235/250, Batch 1/11, Loss: 0.022456137463450432\n",
            "Epoch 235/250, Batch 11/11, Loss: 0.03333290293812752\n",
            "Validation Loss after Epoch 235: 0.06614478180805843\n",
            "Epoch 236/250, Batch 1/11, Loss: 0.021783022210001945\n",
            "Epoch 236/250, Batch 11/11, Loss: 0.023221246898174286\n",
            "Validation Loss after Epoch 236: 0.06793257345755895\n",
            "Epoch 237/250, Batch 1/11, Loss: 0.020383263006806374\n",
            "Epoch 237/250, Batch 11/11, Loss: 0.031694669276475906\n",
            "Validation Loss after Epoch 237: 0.06481392309069633\n",
            "Epoch 238/250, Batch 1/11, Loss: 0.02537655644118786\n",
            "Epoch 238/250, Batch 11/11, Loss: 0.029131731018424034\n",
            "Validation Loss after Epoch 238: 0.0671560491124789\n",
            "Epoch 239/250, Batch 1/11, Loss: 0.0247836634516716\n",
            "Epoch 239/250, Batch 11/11, Loss: 0.03048761747777462\n",
            "Validation Loss after Epoch 239: 0.06752648825446765\n",
            "Epoch 240/250, Batch 1/11, Loss: 0.021813642233610153\n",
            "Epoch 240/250, Batch 11/11, Loss: 0.027900557965040207\n",
            "Validation Loss after Epoch 240: 0.06387636189659436\n",
            "Epoch 241/250, Batch 1/11, Loss: 0.021915551275014877\n",
            "Epoch 241/250, Batch 11/11, Loss: 0.028816523030400276\n",
            "Validation Loss after Epoch 241: 0.0656384639441967\n",
            "Epoch 242/250, Batch 1/11, Loss: 0.021125078201293945\n",
            "Epoch 242/250, Batch 11/11, Loss: 0.024199390783905983\n",
            "Validation Loss after Epoch 242: 0.06353434796134631\n",
            "Epoch 243/250, Batch 1/11, Loss: 0.02200152352452278\n",
            "Epoch 243/250, Batch 11/11, Loss: 0.02899017557501793\n",
            "Validation Loss after Epoch 243: 0.05922315642237663\n",
            "Epoch 244/250, Batch 1/11, Loss: 0.01863347925245762\n",
            "Epoch 244/250, Batch 11/11, Loss: 0.027576757594943047\n",
            "Validation Loss after Epoch 244: 0.06039476891358694\n",
            "Epoch 245/250, Batch 1/11, Loss: 0.025762934237718582\n",
            "Epoch 245/250, Batch 11/11, Loss: 0.02780291624367237\n",
            "Validation Loss after Epoch 245: 0.06198863064249357\n",
            "Epoch 246/250, Batch 1/11, Loss: 0.023327253758907318\n",
            "Epoch 246/250, Batch 11/11, Loss: 0.02361529879271984\n",
            "Validation Loss after Epoch 246: 0.06530851870775223\n",
            "Epoch 247/250, Batch 1/11, Loss: 0.025140313431620598\n",
            "Epoch 247/250, Batch 11/11, Loss: 0.03060155361890793\n",
            "Validation Loss after Epoch 247: 0.06439387549956639\n",
            "Epoch 248/250, Batch 1/11, Loss: 0.018151508644223213\n",
            "Epoch 248/250, Batch 11/11, Loss: 0.031123744323849678\n",
            "Validation Loss after Epoch 248: 0.06866383304198583\n",
            "Epoch 249/250, Batch 1/11, Loss: 0.015816474333405495\n",
            "Epoch 249/250, Batch 11/11, Loss: 0.022551292553544044\n",
            "Validation Loss after Epoch 249: 0.06639677782853444\n",
            "Epoch 250/250, Batch 1/11, Loss: 0.02076348289847374\n",
            "Epoch 250/250, Batch 11/11, Loss: 0.026973458006978035\n",
            "Validation Loss after Epoch 250: 0.0684466262658437\n",
            "Subset size 500 - Test Loss: 0.0732, Test Accuracy: 97.35%, Average Dice Score: 0.9737\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Define your dataset paths\n",
        "data_dir = '/content/gdrive/MyDrive/training_dataset/data_resize16/'\n",
        "label_dir = '/content/gdrive/MyDrive/training_dataset/label_resize16/'\n",
        "\n",
        "# Get image paths and create the full dataset\n",
        "image_paths, label_paths = get_image_paths(data_dir, label_dir)\n",
        "dataset = CustomDataset(image_paths=image_paths, label_paths=label_paths)\n",
        "\n",
        "# Define subset sizes including the full dataset size\n",
        "subset_size = [100, 250, len(dataset)]  # Add the full dataset size\n",
        "\n",
        "# Create subsets\n",
        "dataset_subsets = create_subsets(dataset, subset_size)\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "# Training configurations\n",
        "learning_rate = 0.001\n",
        "num_epochs = 250  # Adjust as needed\n",
        "\n",
        "# Loop over subsets and train the model\n",
        "for size, subset in dataset_subsets.items():\n",
        "    print(f\"\\nTraining on subset size: {size}\")\n",
        "\n",
        "\n",
        "    # Split the subset into training, validation, and test datasets\n",
        "    train_size = int(0.70 * len(subset))\n",
        "    val_size = int(0.15 * len(subset))\n",
        "    test_size = len(subset) - train_size - val_size\n",
        "    train_dataset, val_dataset, test_dataset = random_split(subset, [train_size, val_size, test_size])\n",
        "\n",
        "    # DataLoader setup\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Model, loss function, and optimizer setup\n",
        "    model = UNet(n_class=3).to(device)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            labels = labels.squeeze(1).long()\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item()}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                labels = labels.squeeze(1).long()\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "            val_loss /= len(val_loader)\n",
        "            print(f\"Validation Loss after Epoch {epoch+1}: {val_loss}\")\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        dice_scores = []\n",
        "\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(probabilities, 1)\n",
        "            labels = labels.squeeze(1).long()\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            total += labels.numel()\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            dice_score = dice_coefficient(predicted, labels, num_classes=3)\n",
        "            dice_scores.append(dice_score)\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "        test_accuracy = 100 * correct / total\n",
        "        average_dice_score = sum(dice_scores) / len(dice_scores)\n",
        "\n",
        "        print(f\"Subset size {size} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Average Dice Score: {average_dice_score:.4f}\")\n",
        "\n",
        "        # Select a few images (e.g., first 4)\n",
        "        selected_images = images[:4].cpu()\n",
        "        actual_labels = labels[:4].cpu()\n",
        "        predicted_labels = predicted[:4].cpu()\n",
        "\n",
        "        # Call the function\n",
        "        plot_predictions(selected_images, actual_labels, predicted_labels)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}